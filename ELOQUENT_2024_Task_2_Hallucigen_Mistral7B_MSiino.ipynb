{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marco-siino/eloquent2024/blob/main/ELOQUENT_2024_Task_2_Hallucigen_Mistral7B_MSiino.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ubSCVmmlBKX"
      },
      "source": [
        "Installing dependencies. You might need to tweak the CMAKE_ARGS for the `llama-cpp-python` pip package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKL68Itp9Bm-",
        "outputId": "be8bd5a4-cdda-47ec-e891-624842306363"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python>=0.1.79\n",
            "  Downloading llama_cpp_python-0.2.74.tar.gz (49.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 MB\u001b[0m \u001b[31m191.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting typing-extensions>=4.5.0 (from llama-cpp-python>=0.1.79)\n",
            "  Downloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
            "Collecting numpy>=1.20.0 (from llama-cpp-python>=0.1.79)\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m264.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python>=0.1.79)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m130.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jinja2>=2.11.3 (from llama-cpp-python>=0.1.79)\n",
            "  Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m164.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting MarkupSafe>=2.0 (from jinja2>=2.11.3->llama-cpp-python>=0.1.79)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.74-cp310-cp310-linux_x86_64.whl size=61855856 sha256=7c49d3d4a20bb7c1096a5c9eadb816a051471fba20f94b86656614e195efb37e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-v4scoyns/wheels/21/1f/6d/ee16805ea65ed00d89776a8ce82f3631340a43fc41edabed97\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: typing-extensions, numpy, MarkupSafe, diskcache, jinja2, llama-cpp-python\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.11.0\n",
            "    Uninstalling typing_extensions-4.11.0:\n",
            "      Successfully uninstalled typing_extensions-4.11.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 2.1.5\n",
            "    Uninstalling MarkupSafe-2.1.5:\n",
            "      Successfully uninstalled MarkupSafe-2.1.5\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.4\n",
            "    Uninstalling Jinja2-3.1.4:\n",
            "      Successfully uninstalled Jinja2-3.1.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.2.1+cu121 requires nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-nccl-cu12==2.19.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-2.1.5 diskcache-5.6.3 jinja2-3.1.4 llama-cpp-python-0.2.74 numpy-1.26.4 typing-extensions-4.11.0\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.11.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.2.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Collecting huggingface-hub>=0.21.2 (from datasets)\n",
            "  Downloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "Successfully installed datasets-2.19.1 dill-0.3.8 huggingface-hub-0.23.0 multiprocess-0.70.16 xxhash-3.4.1\n",
            "Collecting deep-translator\n",
            "  Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (2.31.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2024.2.2)\n",
            "Installing collected packages: deep-translator\n",
            "Successfully installed deep-translator-1.11.4\n"
          ]
        }
      ],
      "source": [
        "# GPU llama-cpp-python; Starting from version llama-cpp-python==0.1.79, it supports GGUF\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on \" pip install 'llama-cpp-python>=0.1.79' --force-reinstall --upgrade --no-cache-dir\n",
        "# For download the models\n",
        "!pip install huggingface_hub\n",
        "!pip install datasets\n",
        "!pip install -U deep-translator\n",
        "\n",
        "import datasets\n",
        "from datasets import load_dataset\n",
        "from deep_translator import GoogleTranslator\n",
        "import json\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "import tqdm.notebook as tqdm\n",
        "\n",
        "# Seed to shuffle the json training set.\n",
        "seed_value = 42\n",
        "random.seed(seed_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2ns57iDlBKa"
      },
      "source": [
        "Downloading an instruction-finetuned Mistral model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uDMqQmBfAhYO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2647583556a2419d83b97a1d4e57555b",
            "c466da3506e7425ca7f5874d08460a99",
            "d3b2d3fd3ca24c3996a07f678229db32",
            "dee9385e8bcd47f7af8e4b84b04e5a43",
            "05bc6238f46b4cab82a21be889550a84",
            "1417532ecdd24e4f99754c4dad4af4d8",
            "bdcc141c7bd544ee8420423779449ea3",
            "fba8a0aed5654779ac9d8646ff25b1a7",
            "94dd097903c3479f94a51b6e938f7a3d",
            "b88d2665c5694762a7d8f096ea0785d9",
            "090c755b65054c7ca0f8d9bd059c21b1"
          ]
        },
        "outputId": "bcc17345-e45f-4988-971b-7c83e9fd3563"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "mistral-7b-instruct-v0.2.Q6_K.gguf:   0%|          | 0.00/5.94G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2647583556a2419d83b97a1d4e57555b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.cache/huggingface/hub/models--TheBloke--Mistral-7B-Instruct-v0.2-GGUF/snapshots/3a6fbf4a41a1d52e415a4958cde6856d34b2db93/mistral-7b-instruct-v0.2.Q6_K.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
            "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q6_K:  226 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 1000000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q6_K\n",
            "llm_load_print_meta: model params     = 7.24 B\n",
            "llm_load_print_meta: model size       = 5.53 GiB (6.56 BPW) \n",
            "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.30 MiB\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloaded 32/33 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =  5666.09 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  5461.00 MiB\n",
            "...................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 8192\n",
            "llama_new_context_with_model: n_batch    = 800\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 1000000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   560.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =    24.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 4\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '18'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Guessed chat format: mistral-instruct\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "model_name_or_path = \"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\"\n",
        "model_basename = \"mistral-7b-instruct-v0.2.Q6_K.gguf\"\n",
        "\n",
        "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n",
        "\n",
        "# This config has been tested on an RTX 3080 (VRAM of 16GB).\n",
        "# you might need to tweak with respect to your hardware.\n",
        "from llama_cpp import Llama\n",
        "lcpp_llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_threads=4, #16, # CPU cores\n",
        "    n_batch=800, #8000, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
        "    n_gpu_layers=32, # Change this value based on your model and your GPU VRAM pool.\n",
        "    n_ctx=8192, # Context window\n",
        "    logits_all=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the dataset for the three subtasks."
      ],
      "metadata": {
        "id": "jeXgLOpd4ztp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load the trial data for both English and Swedish\n",
        "trial_ds = load_dataset(\"Eloquent/HalluciGen-PG\", name=\"trial\")\n",
        "\n",
        "#load the trial data only for Swedish\n",
        "trial_ds_sv = load_dataset(\"Eloquent/HalluciGen-PG\", name=\"trial\", split=\"trial_swedish\")\n",
        "\n",
        "#load the test data for the detection step in both English and Swedish\n",
        "test_ds = load_dataset(\"Eloquent/HalluciGen-PG\", name=\"test_detection\")"
      ],
      "metadata": {
        "id": "rjFefzHKeKQk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds['test_detection_swedish'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0J_zqEDg4di",
        "outputId": "54e1b799-e5b3-401a-9511-7e1d9dfb57ff"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 0,\n",
              " 'source': 'Kvinnor kommer att möta högre bilförsäkringspremier.',\n",
              " 'hyp1': 'Det betyder att kvinnor kan förvänta sig att betala högre priser för sina fordonstilläggsförsäkringar.',\n",
              " 'hyp2': 'Kvinnor kommer att få högre premier för bilförsäkring.'}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trial_ds['trial_english'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVNUDVeFhp-g",
        "outputId": "59d25f3c-630a-461c-d3eb-7353ec0c9d46"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 0,\n",
              " 'source': 'The population has declined in some 210 of the 280 municipalities in Sweden, mainly in inland central and northern Sweden.',\n",
              " 'type': 'antonym',\n",
              " 'hyp1': \"In the majority of Sweden's 280 municipalities, the population has gone up.\",\n",
              " 'hyp2': \"In the majority of Sweden's 280 municipalities, the population has gone down.\",\n",
              " 'label': 'hyp1'}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the dataset object to a list\n",
        "trial_en_list = list(trial_ds['trial_english'])\n",
        "trial_sv_list = list(trial_ds['trial_swedish'])\n",
        "\n",
        "# Shuffle the list\n",
        "random.shuffle(trial_en_list)\n",
        "random.shuffle(trial_sv_list)\n",
        "\n"
      ],
      "metadata": {
        "id": "wpopYrrziQIq"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trial_sv_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_9tDXP8jHQQ",
        "outputId": "c9ccc8bc-e05d-4abf-a202-2bc37c6a9205"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'id': 2,\n",
              "  'source': 'Län med befolkningsminskning kommer att vara Vermillion, Posey och Madison.',\n",
              "  'type': 'named entity',\n",
              "  'hyp1': 'Vermillion, Posey och Madison är län som kommer att uppleva minskande befolkning.',\n",
              "  'hyp2': 'Vermillion, Posey och Marion är län som kommer att uppleva minskande befolkning.',\n",
              "  'label': 'hyp2'},\n",
              " {'id': 12,\n",
              "  'source': 'Israels Peres uppmanar parterna att återgå till fredssamtalen.',\n",
              "  'type': 'addition',\n",
              "  'hyp1': 'Peres i Israel ger en uppmaning att återgå till fredssamtalen.',\n",
              "  'hyp2': 'Israels president Shimon Peres uppmanade på måndagen världssamfundet att återuppta fredsförhandlingarna med palestinierna och betonade vikten av en tvåstatslösning.',\n",
              "  'label': 'hyp2'},\n",
              " {'id': 11,\n",
              "  'source': 'Grekisk högerextrem ledare fängslad i väntan på rättegång.',\n",
              "  'type': 'addition',\n",
              "  'hyp1': '\\xa0I väntan på rättegång har en grekisk högerextrem ledare fängslats.',\n",
              "  'hyp2': 'En grekisk högerextremistisk ledare har gripits och kommer att ställas inför rätta, anklagad för att ha organiserat våldsamma protester under de senaste åren. Ledaren, som går under namnet Nikos Michaloliakos, är en tidigare medlem av det grekiska nationalistpartiet Gyllene gryning. Han misstänks också för att ha varit inblandad i mordet på en anti-rasism-.',\n",
              "  'label': 'hyp2'},\n",
              " {'id': 7,\n",
              "  'source': 'Nordkorea carnar utlänningar i söder och ger råd om evakuering.',\n",
              "  'type': 'negation',\n",
              "  'hyp1': 'Nordkorea varnar inte utländska medborgare i södra delarna av landet och ger råd om evakuering.',\n",
              "  'hyp2': 'Nordkorea varnar utländska medborgare i södra delarna av landet och ger råd om evakuering.',\n",
              "  'label': 'hyp1'},\n",
              " {'id': 18,\n",
              "  'source': 'Vi har träffats, men du minns inte mig. Jag arbetade för ett företag som du anlitade för att få ditt minne raderat.',\n",
              "  'type': 'pronoun',\n",
              "  'hyp1': 'Vi har träffats tidigare, men du minns mig inte. Du var anställd av ett företag som jag anlitade för att radera mitt minne.',\n",
              "  'hyp2': 'Vi har träffats tidigare, men du minns mig inte. Jag var anställd av ett företag som du anlitade för att radera ditt minne.',\n",
              "  'label': 'hyp1'},\n",
              " {'id': 3,\n",
              "  'source': 'Google presenterar en prototyp för en självkörande bil.',\n",
              "  'type': 'addition',\n",
              "  'hyp1': 'En prototyp för en självkörande bil presentera av Google.',\n",
              "  'hyp2': 'Enligt ett blogginlägg från Google har företaget utvecklat en prototyp av en självkörande bil som kan köra helt på egen hand. Bilen, som kallas \"Waymo One\", har testats i Nevada-öknen och har visat att den kan navigera i komplexa trafikmiljöer utan att behöva någon mänsklig övervakning.',\n",
              "  'label': 'hyp2'},\n",
              " {'id': 9,\n",
              "  'source': 'Irans kärnvapenförhandlingar går in på tredje dagen.',\n",
              "  'type': 'number',\n",
              "  'hyp1': 'Diskussioner om irans kärnvapenprogram har nått sin tredje dag.',\n",
              "  'hyp2': 'Diskussioner om irans kärnvapenprogram har nått sin fjärde dag.',\n",
              "  'label': 'hyp2'},\n",
              " {'id': 0,\n",
              "  'source': 'Men intäkterna från mjukvarulicenser, ett mått som finansanalytiker följer noga, minskade med 21 procent till 107,6 miljoner dollar.',\n",
              "  'type': 'number',\n",
              "  'hyp1': 'Intäkter från programvarulicenser, en metrik som noggrant övervakas av finansiella analytiker, minskade med 21 procent till ett belopp av 107,6 miljoner dollar.',\n",
              "  'hyp2': 'Intäkter från programvarulicenser, en metrik som noggrant övervakas av finansiella analytiker, minskade med 42 procent till ett belopp av 107,6 miljoner dollar.',\n",
              "  'label': 'hyp2'},\n",
              " {'id': 17,\n",
              "  'source': ' Den iranska regeringen sa: det iranska kärnkraftsprogrammet förblir fredligt.',\n",
              "  'type': 'tense',\n",
              "  'hyp1': 'Regeringen i Iran förklarade att nationens kärnenergiprogram kommer att förbli fredligt.',\n",
              "  'hyp2': 'Regeringen i Iran förklarade att nationens kärnenergiprogram brukade vara fredligt.',\n",
              "  'label': 'hyp2'},\n",
              " {'id': 15,\n",
              "  'source': 'Efter att demonstranter rusat upp på scenen och två gånger brutit strömmen till mikrofonen, avslutade Hedges talet tidigt.',\n",
              "  'type': 'number',\n",
              "  'hyp1': 'När demonstranter stormade scenen och två gånger stängde av mikrofonens ström, avslutade Hedges sitt tal i förtid.',\n",
              "  'hyp2': 'När demonstranter stormade scenen och fem gånger stängde av mikrofonens ström, avslutade Hedges sitt tal i förtid.',\n",
              "  'label': 'hyp2'},\n",
              " {'id': 5,\n",
              "  'source': 'Spaniens prinsessa vittnar i historisk bedrägeriundersökning.',\n",
              "  'type': 'gender',\n",
              "  'hyp1': 'Spanska prinsessan blir en del av en utredning om historisk bedrägeri.',\n",
              "  'hyp2': 'Spanska prinsen blir en del av en utredning om historisk bedrägeri.',\n",
              "  'label': 'hyp2'},\n",
              " {'id': 13,\n",
              "  'source': 'Federal Trade Commission (FTC) bad kongressen idag om ytterligare behörighet för att bekämpa oönskad Internet-spam, som nu står för upp till hälften av all e-posttrafik. ',\n",
              "  'type': 'named entity',\n",
              "  'hyp1': 'Federal Trade Commission (FTC) har idag begärt ytterligare befogenhet från underhuset för att bekämpa oönskat internet-spam, som nu står för upp till hälften av all e-posttrafik.',\n",
              "  'hyp2': 'Federal Trade Commission (FTC) har idag begärt ytterligare befogenhet från kongressen för att bekämpa oönskat internet-spam, som nu står för upp till hälften av all e-posttrafik.',\n",
              "  'label': 'hyp1'},\n",
              " {'id': 16,\n",
              "  'source': 'Förare i spansk tågolycka frågas ut av domaren.',\n",
              "  'type': 'addition',\n",
              "  'hyp1': 'Enligt rapporterna från El mundo, en domare vid domstolen för Andalusien har påbörjat en utredning efter att ha mottagit en begäran om information från de spanska myndigheterna angående kraschen av ett höghastighetståg mellan Madrid och Sevilla förra veckan. Enligt uppgift var 77 personer som var ombord på tåget när det spårade ur. I ett uttalande på sin hemsida, sade domstolen:',\n",
              "  'hyp2': 'Förare i spansk tågolycka utfrågas av domare.',\n",
              "  'label': 'hyp1'},\n",
              " {'id': 14,\n",
              "  'source': 'Utfrågningen ägde rum en dag efter att Pentagon för första gången pekade ut en officer, Dallager, för att han inte tog upp skandalen.',\n",
              "  'type': 'named entity',\n",
              "  'hyp1': 'Förhöret ägde rum en dag efter att FBI offentligt identifierade en officer, Dallager, för första gången och hävdade att han inte hade åtgärdat skandalen.',\n",
              "  'hyp2': 'Förhöret ägde rum en dag efter att Pentagon offentligt identifierade en officer, Dallager, för första gången och hävdade att han inte hade åtgärdat skandalen.',\n",
              "  'label': 'hyp1'},\n",
              " {'id': 1,\n",
              "  'source': 'Hongkong-universitet samarbetar med universitet, företag och statliga sektorer i Kina för att samordna utbildningsprogram och forskningscentra för att främja högteknologisk forskning, kommersialisering och tekniköverföring.',\n",
              "  'type': 'natural',\n",
              "  'hyp1': 'University of Hong Kong samarbetar med olika kinesiska universitet, företag och regeringsdepartement för att samordna utbildningsprogram och forskningscentra med syftet att främja avancerad teknologisk forskning, kommersialisering och tekniköverföring.',\n",
              "  'hyp2': 'University of Hong Kong samarbetar med olika kinesiska universitet, företag och statliga sektorer för att samordna utbildningsprogram och forskningscentra med syftet att främja avancerad teknologisk forskning, kommersialisering och tekniköverföring.',\n",
              "  'label': 'hyp1'},\n",
              " {'id': 19,\n",
              "  'source': 'Deras ledare, Abu Bakr al-Azdi, överlämnade sig själv i juni; hans ställföreträdare dödades i en skjutning nyligen med saudiska styrkor.',\n",
              "  'type': 'date',\n",
              "  'hyp1': 'Abu Bakr al-Azdi, gruppledaren, hade nyligen överlämnat sig själv den 16 Juni 2015, medan hans biträdande nyligen dödades i ett möte med saudiska styrkor.',\n",
              "  'hyp2': 'Abu Bakr al-Azdi, gruppledaren, hade nyligen överlämnat sig själv i juni, medan hans biträdande nyligen dödades i ett möte med saudiska styrkor.',\n",
              "  'label': 'hyp1'},\n",
              " {'id': 6,\n",
              "  'source': 'Mannen använder en slägga för att bryta betongblocket som finns på den andre mannen.',\n",
              "  'type': 'antonym',\n",
              "  'hyp1': 'Mannen använder en slägga för att krossa betongblocket som ligger ovanpå den andra mannen.',\n",
              "  'hyp2': 'Mannen använder en slägga för att krossa betongblocket som ligger under den andra mannen.',\n",
              "  'label': 'hyp2'},\n",
              " {'id': 4,\n",
              "  'source': 'Lagförslaget säger att en kvinna som genomgår en sådan abort inte kunde åtalas.',\n",
              "  'type': 'negation',\n",
              "  'hyp1': 'Förslaget stadgar att en kvinna som genomgår en sådan abort kan åtalas.',\n",
              "  'hyp2': 'Förslaget stadgar att en kvinna som genomgår en sådan abort inte kan åtalas.',\n",
              "  'label': 'hyp1'},\n",
              " {'id': 8,\n",
              "  'source': 'Beväpnad man bland 7 döda efter lägenhetsskjutning i Florida.',\n",
              "  'type': 'addition',\n",
              "  'hyp1': 'En beväpnad man var bland de sju döda efter en lägenhetsskjutning i Florida.',\n",
              "  'hyp2': 'En man med ett skjutvapen har dödat minst sju personer och skadat flera andra efter att ha öppnat eld i en lägenhet i Fort Lauderdale, Florida, tidigt på morgonen lokal tid. Enligt lokala myndigheter var offren för skjutningen hemmahörande i olika delar av landet. Den misstänkta skytten är gripen. Det är för närvarande oklart vad som motiverade dådet.',\n",
              "  'label': 'hyp2'},\n",
              " {'id': 10,\n",
              "  'source': 'AstraZeneca betalar 4,1 miljarder dollar för att köpa ut Bristol-Myers Squibb ur diabetesalliansen.',\n",
              "  'type': 'natural',\n",
              "  'hyp1': 'AstraZeneca har kommit överens om att överföra 4,1 miljarder dollar till Bristol-Myers Squibb i utbyte mot kontroll över deras samarbete inom diabetesvården.',\n",
              "  'hyp2': 'AstraZeneca har kommit överens om att överföra 4,1 miljarder dollar till Bristol-Myers Squibb för att köpa ut företaget ur diabetesalliansen.',\n",
              "  'label': 'hyp1'}]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create few-shot samples from training set."
      ],
      "metadata": {
        "id": "fHPX3E0PCJ9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " prompt_context = \"\"\"[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "zafRW5vU6y1n"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_few_shot_samples(train_list,nr_samples,language):\n",
        "\n",
        "  nr_few_shot_samples = nr_samples\n",
        "\n",
        "  few_shot_samples = ''\n",
        "\n",
        "  for i in range(0,nr_few_shot_samples):\n",
        "    if language=='sv':\n",
        "      src = GoogleTranslator(source='sv', target='en').translate(train_list[i]['source'])\n",
        "      hyp1 = GoogleTranslator(source='sv', target='en').translate(train_list[i]['hyp1'])\n",
        "      hyp2 = GoogleTranslator(source='sv', target='en').translate(train_list[i]['hyp2'])\n",
        "    else:\n",
        "      src = train_list[i]['source']\n",
        "      hyp1 = train_list[i]['hyp1']\n",
        "      hyp2 = train_list[i]['hyp2']\n",
        "\n",
        "    label = train_list[i]['label']\n",
        "\n",
        "    few_shot_samples += '<s>'+prompt_context+\"src: \"+src+\"\\nhyp1: \"+hyp1 + '\\nhyp2: '+hyp2+'\\nlabel: [/INST] \\n ' + label + \"\\n</s> \\n\\n \"\n",
        "\n",
        "  return few_shot_samples"
      ],
      "metadata": {
        "id": "99ag8gav51mC"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_samples_en = create_few_shot_samples(trial_en_list,16,'en')"
      ],
      "metadata": {
        "id": "o8IaDH-7GmrN"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_samples_sv = create_few_shot_samples(trial_sv_list,20,'sv')"
      ],
      "metadata": {
        "id": "pW64JRxsr0os"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(few_shot_samples_en)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WHgxqxAxcu1",
        "outputId": "9d9f345a-03f4-4222-d2ac-bd3264d41c1a"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: This state of affairs has not changed in more than 100 years, but hopefully at some stage - and perhaps soon - change will come.\n",
            "hyp1: There has been no change in the status quo in over 100 years, but there is hope that change will soon come. \n",
            "hyp2: The state of affairs is1-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-6556\n",
            "label: [/INST] \n",
            " hyp2\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The draft agenda as drawn up by the Conference of Presidents pursuant to Rule 95 of the Rules of Procedure has been distributed.\n",
            "hyp1: The Conference of Presidents hasn't distributed the draft agenda.\n",
            "hyp2: The Conference of Presidents has distributed the draft agenda.\n",
            "label: [/INST] \n",
            " hyp1\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: There would be a dual ceiling: either the application of a 20 % rate of taxation on interest payments or the supply of information.\n",
            "hyp1: It would be possible to apply a 20 % rate of taxation on interest payments or the supply of information\n",
            "hyp2: It would be possible to apply a 20 % rate of taxation on interest payments.\n",
            "label: [/INST] \n",
            " hyp2\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: In addition to these losses, there were also significant losses in terms of infrastructures, totalling approximately EUR 15 million.\n",
            "hyp1: There were losses in the amount of approximately 15 million dollars.\n",
            "hyp2: There were infrastructure-related losses in the amount of approximately 15 million euros.\n",
            "label: [/INST] \n",
            " hyp1\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: I am always grateful for comments and suggestions from the floor, because I believe I need to take everyone's views into account to be an effective President.\n",
            "hyp1: I think I need to listen to everyone's views in order to be an ineffective President.\n",
            "hyp2: I think I need to listen to everyone's views in order to be an effective President.\n",
            "label: [/INST] \n",
            " hyp1\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: We need quite specific legislative proposals, including proposals based on Articles 13 and 137 of the Treaty of Amsterdam.\n",
            "hyp1: Legislative proposals based on the Treaty of Amsterdam are needed.\n",
            "hyp2: Legislative proposals based solely on the Treaty of Amsterdam are needed.\n",
            "label: [/INST] \n",
            " hyp2\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: In 1998, 1 700 000 net jobs were created in Europe, and although I admit that the employment situation is far from ideal, it has improved.\n",
            "hyp1: In 1700 there were 1 998 000 net jobs created in Europe.\n",
            "hyp2: In 1998 there were 1 700 000 net jobs created in Europe.\n",
            "label: [/INST] \n",
            " hyp1\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: We have done so: on 5 February we published an extremely detailed press release dealing with the questions you have raised.\n",
            "hyp1: We published a press release that dealt with the questions we raised.\n",
            "hyp2: We published a press release that dealt with the questions you raised.\n",
            "label: [/INST] \n",
            " hyp1\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Madam President, I am speaking on behalf of our colleague, Mr Francis Decourrière, who drafted one of the motions for a resolution.\n",
            "hyp1: One of the motions for a resolution was drafted by Mr Francis Decourrière.\n",
            "hyp2: One of the motions for a resolution was drafted by Mrs Francis Decourrière.\n",
            "label: [/INST] \n",
            " hyp2\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Furthermore, we are currently thinking of organising economic discussions in Brussels, in the form of study days, in which this House would be involved.\n",
            "hyp1: Studying days in which this House would be involved is one of the things we are thinking of doing.\n",
            "hyp2: Studying days and months in which this House would be involved is one of the things we are thinking of doing.\n",
            "label: [/INST] \n",
            " hyp2\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The fact is that a key omission from the proposals on agricultural policy in Agenda 2000 is a chapter on renewable energy.\n",
            "hyp1: Agenda 2030 does not include a chapter on renewable energy.\n",
            "hyp2: Agenda 2000 does not include a chapter on renewable energy.\n",
            "label: [/INST] \n",
            " hyp1\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: This is a very important issue, because we have no other way of cleaning up our beaches, especially those on the North Sea and Baltic coasts.\n",
            "hyp1: There is no other way to clean up the beaches on the North Sea and Baltic coast.\n",
            "hyp2: There is no other way to clean up the beaches on the North Sea and  the Bothnian Bay coastline.\n",
            "label: [/INST] \n",
            " hyp2\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The population has declined in some 210 of the 280 municipalities in Sweden, mainly in inland central and northern Sweden.\n",
            "hyp1: In the majority of Sweden's 280 municipalities, the population has gone up.\n",
            "hyp2: In the majority of Sweden's 280 municipalities, the population has gone down.\n",
            "label: [/INST] \n",
            " hyp1\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Mr President, I did prepare a speech but I have left it aside because many of my points have already been excellently made by previous speakers.\n",
            "hyp1: Many of the points they were going to make in their speech were already made by previous speakers, so they left it aside.\n",
            "hyp2: Many of the points I was going to make in my speech were already made by previous speakers, so I left it aside.\n",
            "label: [/INST] \n",
            " hyp1\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The European Commission proposes that this information should enter into force within a period of three years from 1 July 1998.\n",
            "hyp1: The EU wants this information to enter into force in three years.\n",
            "hyp2: The EU wants this information to enter into force in thirty years.\n",
            "label: [/INST] \n",
            " hyp2\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Amendment No 1 in the French version deletes illegal immigration and Amendment No 4 omits the expression 'police authorities'.\n",
            "hyp1: The French version excludes the expression'police authorities'.\n",
            "hyp2: The French version excludes the expression 'police authorities' from Amendment No 4.\n",
            "label: [/INST] \n",
            " hyp1\n",
            "</s> \n",
            "\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(few_shot_samples_sv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3ye1cEtsOIw",
        "outputId": "d30df782-181f-4b87-8a53-99acd6941058"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Counties with population declines will be Vermillion, Posey and Madison.\n",
            "hyp1: Vermillion, Posey and Madison are counties that will experience declining populations.\n",
            "hyp2: Vermillion, Posey and Marion are counties that will experience declining populations.\n",
            "label: [/INST] \n",
            " hyp2\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Israel's Peres calls on the parties to return to peace talks.\n",
            "hyp1: Peres in Israel calls for return to peace talks.\n",
            "hyp2: Israeli President Shimon Peres on Monday called on the world community to resume peace talks with the Palestinians, stressing the importance of a two-state solution.\n",
            "label: [/INST] \n",
            " hyp2\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Greek far-right leader jailed awaiting trial.\n",
            "hyp1: Pending trial, a Greek far-right leader has been imprisoned.\n",
            "hyp2: A Greek far-right leader has been arrested and will stand trial, accused of organizing violent protests in recent years. The leader, who goes by the name Nikos Michaloliakos, is a former member of the Greek nationalist party Golden Dawn. He is also suspected of having been involved in the murder of an anti-racism-.\n",
            "label: [/INST] \n",
            " hyp2\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: North Korea carves out foreigners in the South and advises evacuation.\n",
            "hyp1: North Korea does not warn foreign nationals in the southern parts of the country and advise evacuation.\n",
            "hyp2: North Korea warns foreign nationals in southern parts of the country and advises evacuation.\n",
            "label: [/INST] \n",
            " hyp1\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: We have met, but you don't remember me. I worked for a company you hired to get your memory erased.\n",
            "hyp1: We've met before, but you don't remember me. You were employed by a company I hired to erase my memory.\n",
            "hyp2: We've met before, but you don't remember me. I was employed by a company you hired to erase your memory.\n",
            "label: [/INST] \n",
            " hyp1\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Google presents a prototype for a self-driving car.\n",
            "hyp1: A prototype for a self-driving car presented by Google.\n",
            "hyp2: According to a blog post from Google, the company has developed a prototype of a self-driving car that can drive completely on its own. The car, called \"Waymo One\", has been tested in the Nevada desert and has shown that it can navigate complex traffic environments without the need for any human supervision.\n",
            "label: [/INST] \n",
            " hyp2\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Iran's nuclear negotiations enter third day.\n",
            "hyp1: Discussions about Iran's nuclear weapons program have reached their third day.\n",
            "hyp2: Discussions about Iran's nuclear weapons program have reached their fourth day.\n",
            "label: [/INST] \n",
            " hyp2\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: But revenue from software licenses, a metric watched closely by financial analysts, fell 21 percent to $107.6 million.\n",
            "hyp1: Revenue from software licenses, a metric closely watched by financial analysts, fell 21 percent to $107.6 million.\n",
            "hyp2: Revenue from software licenses, a metric closely watched by financial analysts, fell 42 percent to $107.6 million.\n",
            "label: [/INST] \n",
            " hyp2\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The Iranian government said: Iran's nuclear program remains peaceful.\n",
            "hyp1: The government of Iran declared that the nation's nuclear energy program will remain peaceful.\n",
            "hyp2: The government of Iran declared that the nation's nuclear energy program used to be peaceful.\n",
            "label: [/INST] \n",
            " hyp2\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: After protesters rushed the stage and twice cut power to the microphone, Hedges ended the speech early.\n",
            "hyp1: As protesters stormed the stage and twice cut off the power to the microphone, Hedges ended his speech prematurely.\n",
            "hyp2: As protesters stormed the stage and five times cut off the power to the microphone, Hedges ended his speech prematurely.\n",
            "label: [/INST] \n",
            " hyp2\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Spain's princess testifies in historic fraud inquiry.\n",
            "hyp1: The Spanish princess becomes part of an investigation into historical fraud.\n",
            "hyp2: The Spanish prince becomes part of an investigation into historical fraud.\n",
            "label: [/INST] \n",
            " hyp2\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The Federal Trade Commission (FTC) today asked Congress for additional authority to combat unwanted Internet spam, which now accounts for up to half of all email traffic.\n",
            "hyp1: The Federal Trade Commission (FTC) today requested additional authority from the House of Commons to combat unwanted Internet spam, which now accounts for up to half of all email traffic.\n",
            "hyp2: The Federal Trade Commission (FTC) today requested additional authority from Congress to combat unwanted Internet spam, which now accounts for up to half of all email traffic.\n",
            "label: [/INST] \n",
            " hyp1\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Driver in Spanish train accident questioned by judge.\n",
            "hyp1: According to reports from El mundo, a judge at the Court of Andalusia has begun an investigation after receiving a request for information from the Spanish authorities regarding the crash of a high-speed train between Madrid and Seville last week. According to reports, 77 people were on board the train when it derailed. In a statement on its website, the court said:\n",
            "hyp2: Driver in Spanish train accident questioned by judge.\n",
            "label: [/INST] \n",
            " hyp1\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The hearing took place a day after the Pentagon for the first time singled out an officer, Dallager, for failing to address the scandal.\n",
            "hyp1: The hearing took place a day after the FBI publicly identified an officer, Dallager, for the first time and claimed he had failed to fix the scandal.\n",
            "hyp2: The hearing came a day after the Pentagon publicly identified an officer, Dallager, for the first time and claimed he had failed to address the scandal.\n",
            "label: [/INST] \n",
            " hyp1\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Hong Kong University collaborates with universities, enterprises and government sectors in China to coordinate educational programs and research centers to promote high-tech research, commercialization and technology transfer.\n",
            "hyp1: The University of Hong Kong collaborates with various Chinese universities, companies and government departments to coordinate educational programs and research centers with the aim of promoting advanced technological research, commercialization and technology transfer.\n",
            "hyp2: The University of Hong Kong collaborates with various Chinese universities, enterprises and government sectors to coordinate educational programs and research centers with the aim of promoting advanced technological research, commercialization and technology transfer.\n",
            "label: [/INST] \n",
            " hyp1\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Their leader, Abu Bakr al-Azdi, surrendered himself in June; his deputy was killed in a recent shootout with Saudi forces.\n",
            "hyp1: Abu Bakr al-Azdi, the group's leader, had recently surrendered himself on June 16, 2015, while his deputy was recently killed in an encounter with Saudi forces.\n",
            "hyp2: Abu Bakr al-Azdi, the group's leader, had recently surrendered himself in June, while his deputy was recently killed in an encounter with Saudi forces.\n",
            "label: [/INST] \n",
            " hyp1\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The man uses a sledgehammer to break the concrete block that is on the other man.\n",
            "hyp1: The man uses a sledgehammer to break the concrete block that is on top of the other man.\n",
            "hyp2: The man uses a sledgehammer to break the concrete block that is under the other man.\n",
            "label: [/INST] \n",
            " hyp2\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The bill states that a woman undergoing such an abortion could not be prosecuted.\n",
            "hyp1: The proposal stipulates that a woman who undergoes such an abortion can be prosecuted.\n",
            "hyp2: The proposal stipulates that a woman who undergoes such an abortion cannot be prosecuted.\n",
            "label: [/INST] \n",
            " hyp1\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Gunman among 7 dead after apartment shooting in Florida.\n",
            "hyp1: A gunman was among the seven dead after a Florida apartment shooting.\n",
            "hyp2: A gunman has killed at least seven people and injured several others after opening fire at an apartment in Fort Lauderdale, Florida, early this morning local time. According to local authorities, the victims of the shooting were from different parts of the country. The suspected shooter has been arrested. It is currently unclear what motivated the act.\n",
            "label: [/INST] \n",
            " hyp2\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: AstraZeneca pays $4.1 billion to buy Bristol-Myers Squibb out of diabetes alliance.\n",
            "hyp1: AstraZeneca has agreed to transfer $4.1 billion to Bristol-Myers Squibb in exchange for control of their diabetes care collaboration.\n",
            "hyp2: AstraZeneca has agreed to transfer $4.1 billion to Bristol-Myers Squibb to buy the company out of the diabetes alliance.\n",
            "label: [/INST] \n",
            " hyp1\n",
            "</s> \n",
            "\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run! (English test set)"
      ],
      "metadata": {
        "id": "r6Cqo0JqotN9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKo1-X5OvT4b",
        "outputId": "240fa395-6168-4b84-e895-c76ffe56d1e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.89 ms /    16 runs   (    0.49 ms per token,  2028.65 tokens per second)\n",
            "llama_print_timings: prompt eval time =   71482.99 ms /    85 tokens (  840.98 ms per token,     1.19 tokens per second)\n",
            "llama_print_timings:        eval time =    1574.25 ms /    15 runs   (  104.95 ms per token,     9.53 tokens per second)\n",
            "llama_print_timings:       total time =    2264.06 ms /   100 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: It has enabled us to support and encourage an exchange of experiences and to pursue activities to raise the level of competence throughout Europe.\n",
            "hyp1: You can support and encourage an exchange of experiences to raise the level of competence in Europe.\n",
            "hyp2: We can support and encourage an exchange of experiences to raise the level of competence in Europe.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.60 ms /    16 runs   (    0.48 ms per token,  2104.43 tokens per second)\n",
            "llama_print_timings: prompt eval time =     305.73 ms /    95 tokens (    3.22 ms per token,   310.73 tokens per second)\n",
            "llama_print_timings:        eval time =    1645.82 ms /    15 runs   (  109.72 ms per token,     9.11 tokens per second)\n",
            "llama_print_timings:       total time =    2282.81 ms /   110 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Therefore, I am calling for an increase in the premiums for all varieties of leaf tobacco for the 1999, 2000 and 2001 harvests.\n",
            "hyp1: I want to see a decrease in the premiums for all varieties of leaf tobacco.\n",
            "hyp2: I want to see an increase in the premiums for all varieties of leaf tobacco.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.00 ms /    16 runs   (    0.56 ms per token,  1777.38 tokens per second)\n",
            "llama_print_timings: prompt eval time =     304.41 ms /   105 tokens (    2.90 ms per token,   344.93 tokens per second)\n",
            "llama_print_timings:        eval time =    1756.98 ms /    15 runs   (  117.13 ms per token,     8.54 tokens per second)\n",
            "llama_print_timings:       total time =    2457.46 ms /   120 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: In other words, a person may be prevented from coming near their victim when there is reason to fear that violent acts will be carried out again.\n",
            "hyp1: When there is reason to fear that a violent act will be carried out again, a person may not be allowed to come near their victim.\n",
            "hyp2: When there is reason to fear that a violent act will be carried out again, a person may not be allowed to come near her victim.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =      11.37 ms /    16 runs   (    0.71 ms per token,  1407.58 tokens per second)\n",
            "llama_print_timings: prompt eval time =     284.73 ms /    68 tokens (    4.19 ms per token,   238.82 tokens per second)\n",
            "llama_print_timings:        eval time =    1882.61 ms /    15 runs   (  125.51 ms per token,     7.97 tokens per second)\n",
            "llama_print_timings:       total time =    2594.71 ms /    83 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: None of my 34 amendments were adopted and my arguments against the ridiculous administrative burden proposed were not heeded.\n",
            "hyp1: Our arguments against the ridiculous administrative burden were not heard.\n",
            "hyp2: My arguments against the ridiculous administrative burden were not heard.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.52 ms /    16 runs   (    0.53 ms per token,  1878.15 tokens per second)\n",
            "llama_print_timings: prompt eval time =     288.32 ms /    89 tokens (    3.24 ms per token,   308.68 tokens per second)\n",
            "llama_print_timings:        eval time =    1695.82 ms /    15 runs   (  113.05 ms per token,     8.85 tokens per second)\n",
            "llama_print_timings:       total time =    2305.82 ms /   104 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The final key feature of the report is its orientation. It aims to protest against certain drifts that are becoming evident, and I will mention two instances.\n",
            "hyp1: The final main feature of the report is its orientation protesting against drifts, and I will provide two instances of that.\n",
            "hyp2: The final features of the report are its orientation and two instances.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.66 ms /    16 runs   (    0.60 ms per token,  1656.14 tokens per second)\n",
            "llama_print_timings: prompt eval time =     293.50 ms /   106 tokens (    2.77 ms per token,   361.16 tokens per second)\n",
            "llama_print_timings:        eval time =    1571.48 ms /    15 runs   (  104.77 ms per token,     9.55 tokens per second)\n",
            "llama_print_timings:       total time =    2257.44 ms /   121 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: It is high time an appropriate level of social protection was created at European level for those whose work does not fit into the usual pattern.\n",
            "hyp1: It's time for an appropriate level of social protection for those whose work doesn't fit into the usual pattern.\n",
            "hyp2: It's time for an appropriate level of social protection for those whose work doesn't fit into the usual pattern, such as firefighters or security officers. \n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.11 ms /    16 runs   (    0.57 ms per token,  1756.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =     311.66 ms /    94 tokens (    3.32 ms per token,   301.61 tokens per second)\n",
            "llama_print_timings:        eval time =    1562.39 ms /    15 runs   (  104.16 ms per token,     9.60 tokens per second)\n",
            "llama_print_timings:       total time =    2187.09 ms /   109 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: These sums, however, have still not produced the desired effect: the substantial fall in the average earnings of the Palestinians is proof of this.\n",
            "hyp1: The fall in average earnings of Egyptians is proof that these sums have not produced the desired effect.\n",
            "hyp2: The fall in average earnings of Palestinians is proof that these sums have not produced the desired effect.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =      11.58 ms /    16 runs   (    0.72 ms per token,  1381.57 tokens per second)\n",
            "llama_print_timings: prompt eval time =     326.22 ms /    93 tokens (    3.51 ms per token,   285.09 tokens per second)\n",
            "llama_print_timings:        eval time =    1778.61 ms /    15 runs   (  118.57 ms per token,     8.43 tokens per second)\n",
            "llama_print_timings:       total time =    2728.40 ms /   108 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Mr President, the approach adopted by the rapporteur to the Commission's 1999 annual economic report is comprehensive and also sensible.\n",
            "hyp1: The approach taken by the rapporteur to the 1997 annual economic report is comprehensive and sensible.\n",
            "hyp2: The approach taken by the rapporteur to the 1999 annual economic report is comprehensive and sensible.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.39 ms /    16 runs   (    0.52 ms per token,  1907.71 tokens per second)\n",
            "llama_print_timings: prompt eval time =     278.73 ms /    75 tokens (    3.72 ms per token,   269.07 tokens per second)\n",
            "llama_print_timings:        eval time =    1533.45 ms /    15 runs   (  102.23 ms per token,     9.78 tokens per second)\n",
            "llama_print_timings:       total time =    2076.46 ms /    90 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The Berlin summit, which will focus solely on Agenda 2000, is of particular importance for the future of the European Union.\n",
            "hyp1: The future of the European Union is important to the Berlin summit.\n",
            "hyp2: The Berlin summit is important to the future of the European Union.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.77 ms /    16 runs   (    0.55 ms per token,  1825.44 tokens per second)\n",
            "llama_print_timings: prompt eval time =     290.76 ms /   106 tokens (    2.74 ms per token,   364.56 tokens per second)\n",
            "llama_print_timings:        eval time =    1628.39 ms /    15 runs   (  108.56 ms per token,     9.21 tokens per second)\n",
            "llama_print_timings:       total time =    2298.88 ms /   121 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: As a result, I cannot promise the chairman of the Committee on Budgets that there will be 100 % funding for the financial programme.\n",
            "hyp1: I can't promise that there will be 100 percent funding for the financial program.\n",
            "hyp2: Because of this, it's not possible for me to promise the chairman of the Committee on Budgets that there would be 210 % funding for the financial programme.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.68 ms /    16 runs   (    0.54 ms per token,  1843.96 tokens per second)\n",
            "llama_print_timings: prompt eval time =     290.73 ms /   111 tokens (    2.62 ms per token,   381.79 tokens per second)\n",
            "llama_print_timings:        eval time =    1659.05 ms /    15 runs   (  110.60 ms per token,     9.04 tokens per second)\n",
            "llama_print_timings:       total time =    2317.62 ms /   126 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: We voted for Amendments Nos 22 to 25, even though they do not take adequate measures to prevent these problems from happening.\n",
            "hyp1: Amendments Nos 22 to 25 didn't take enough measures to prevent problems from happening, so we voted for them.\n",
            "hyp2: Amendments Nos 22 to 25 didn't take enough measures to prevent problems from happening, yet we voted for them.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.98 ms /    16 runs   (    0.50 ms per token,  2005.01 tokens per second)\n",
            "llama_print_timings: prompt eval time =     332.94 ms /   101 tokens (    3.30 ms per token,   303.36 tokens per second)\n",
            "llama_print_timings:        eval time =    1648.07 ms /    15 runs   (  109.87 ms per token,     9.10 tokens per second)\n",
            "llama_print_timings:       total time =    2579.74 ms /   116 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Secondly, we say that the type approval rules as from 2005 must provide in precise terms that new cars must be recycling-friendly.\n",
            "hyp1: The type approval rules from 2005 must give precise terms for new cars to be recycling-friendly.\n",
            "hyp2: The type approval rules as of 2005 must give precise terms for new cars to be recycling-friendly.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.50 ms /    16 runs   (    0.47 ms per token,  2132.76 tokens per second)\n",
            "llama_print_timings: prompt eval time =     282.28 ms /    91 tokens (    3.10 ms per token,   322.37 tokens per second)\n",
            "llama_print_timings:        eval time =    1546.07 ms /    15 runs   (  103.07 ms per token,     9.70 tokens per second)\n",
            "llama_print_timings:       total time =    2155.78 ms /   106 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: B4-0188/99 by Mrs d'Ancona, on behalf of the PSE Group, on the death penalty against Greg Summers - Texas, USA; Leonard Peltier\n",
            "hyp1: The death penalty was imposed on Gregory Winters in Texas, USA.\n",
            "hyp2: The death penalty was imposed on Greg Summers in Texas, USA.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.84 ms /    16 runs   (    0.55 ms per token,  1809.14 tokens per second)\n",
            "llama_print_timings: prompt eval time =     302.19 ms /   116 tokens (    2.61 ms per token,   383.86 tokens per second)\n",
            "llama_print_timings:        eval time =    1656.36 ms /    15 runs   (  110.42 ms per token,     9.06 tokens per second)\n",
            "llama_print_timings:       total time =    2364.10 ms /   131 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: We struggle with water on a daily basis in the Netherlands - in the polders, the delta where the Meuse, the Rhine and the Scheldt flow into the sea.\n",
            "hyp1: In the Netherlands, we struggle with water on a daily basis because of the Meuse, Rhine, Scheldt, Noord, Voer and Dieze\n",
            "hyp2: In the Netherlands, we struggle with water on a daily basis because of the Meuse, Rhine and Scheldt.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.67 ms /    16 runs   (    0.54 ms per token,  1845.44 tokens per second)\n",
            "llama_print_timings: prompt eval time =     272.03 ms /    76 tokens (    3.58 ms per token,   279.38 tokens per second)\n",
            "llama_print_timings:        eval time =    1881.85 ms /    15 runs   (  125.46 ms per token,     7.97 tokens per second)\n",
            "llama_print_timings:       total time =    2442.03 ms /    91 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: It may lower costs by 10 % and is, of course, only applicable over the shorter road legs of the combined transport journey.\n",
            "hyp1: It is only applicable to the short road legs of the combined transport journey.\n",
            "hyp2: It is applicable to the road legs of the combined transport journey.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.12 ms /    16 runs   (    0.51 ms per token,  1971.41 tokens per second)\n",
            "llama_print_timings: prompt eval time =     282.01 ms /    78 tokens (    3.62 ms per token,   276.58 tokens per second)\n",
            "llama_print_timings:        eval time =    1576.98 ms /    15 runs   (  105.13 ms per token,     9.51 tokens per second)\n",
            "llama_print_timings:       total time =    2130.59 ms /    93 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Nothing has been done in the 27 years since then, while we have liberalised sea transport, air transport and road transport.\n",
            "hyp1: Since then, we have restricted air transport, road transport and sea transport.\n",
            "hyp2: Since then, we have liberalised air transport, road transport and sea transport.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.77 ms /    16 runs   (    0.49 ms per token,  2059.73 tokens per second)\n",
            "llama_print_timings: prompt eval time =     270.07 ms /    80 tokens (    3.38 ms per token,   296.22 tokens per second)\n",
            "llama_print_timings:        eval time =    1574.95 ms /    15 runs   (  105.00 ms per token,     9.52 tokens per second)\n",
            "llama_print_timings:       total time =    2123.34 ms /    95 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: This concerns the applicant countries which are still in the process of membership negotiations, but which in the meantime are being discriminated against.\n",
            "hyp1: This is about the countries that are still in membership negotiations and are also being discriminated against.\n",
            "hyp2: The countries that are still in membership negotiations are being discriminated against.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.00 ms /    16 runs   (    0.50 ms per token,  2000.75 tokens per second)\n",
            "llama_print_timings: prompt eval time =     290.54 ms /   105 tokens (    2.77 ms per token,   361.40 tokens per second)\n",
            "llama_print_timings:        eval time =    1560.41 ms /    15 runs   (  104.03 ms per token,     9.61 tokens per second)\n",
            "llama_print_timings:       total time =    2194.69 ms /   120 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: I would be grateful if you could tell me whether or not Rambouillet was a European Union initiative or a Franco-British initiative, as I did not quite understand this point.\n",
            "hyp1: I need to know if Rambouillet is a European Union initiative or a Franco-British initiative.\n",
            "hyp2: We need to know if Rambouillet is a European Union initiative or a Franco-British initiative.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.21 ms /    16 runs   (    0.58 ms per token,  1736.30 tokens per second)\n",
            "llama_print_timings: prompt eval time =     287.75 ms /    96 tokens (    3.00 ms per token,   333.62 tokens per second)\n",
            "llama_print_timings:        eval time =    1915.96 ms /    15 runs   (  127.73 ms per token,     7.83 tokens per second)\n",
            "llama_print_timings:       total time =    2564.98 ms /   111 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Madam President, I referred earlier to inherited inertia, and this leads me to the third group of amendments, to which Mr Fabre-Aubrespy referred.\n",
            "hyp1: The third group of amendments was referred to by Ms Fabre- Aubrespy.\n",
            "hyp2: The third group of amendments was referred to by Mr Fabre- Aubrespy.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.29 ms /    16 runs   (    0.46 ms per token,  2194.49 tokens per second)\n",
            "llama_print_timings: prompt eval time =     299.20 ms /    74 tokens (    4.04 ms per token,   247.32 tokens per second)\n",
            "llama_print_timings:        eval time =    1468.04 ms /    15 runs   (   97.87 ms per token,    10.22 tokens per second)\n",
            "llama_print_timings:       total time =    2018.31 ms /    89 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Other elements have contributed to the proposal to conclude this partnership agreement, especially the very worrying regional context in Central Asia.\n",
            "hyp1: There are other elements that contributed to the proposal to conclude the partnership agreement.\n",
            "hyp2: There are no other elements that contributed to the proposal to conclude the partnership agreement.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.44 ms /    16 runs   (    0.53 ms per token,  1896.41 tokens per second)\n",
            "llama_print_timings: prompt eval time =     415.11 ms /   150 tokens (    2.77 ms per token,   361.35 tokens per second)\n",
            "llama_print_timings:        eval time =    1603.35 ms /    15 runs   (  106.89 ms per token,     9.36 tokens per second)\n",
            "llama_print_timings:       total time =    2486.90 ms /   165 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: A way must also be found to tax all kinds of cross-border capital movements which are carried out for profit, in order to put a brake, albeit in a limited way, on the increase in the volume of parasitic, speculative capital.\n",
            "hyp1: To put a brake on the increase in the volume of speculative capital, which is often regulated under the Tobin Tax, a way must be found to tax cross-border capital movements carried out for profit.\n",
            "hyp2: To put a brake on the increase in the volume of speculative capital, a way must be found to tax cross-border capital movements carried out for profit.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.75 ms /    16 runs   (    0.48 ms per token,  2065.58 tokens per second)\n",
            "llama_print_timings: prompt eval time =     275.41 ms /    86 tokens (    3.20 ms per token,   312.27 tokens per second)\n",
            "llama_print_timings:        eval time =    1519.99 ms /    15 runs   (  101.33 ms per token,     9.87 tokens per second)\n",
            "llama_print_timings:       total time =    2088.52 ms /   101 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: An important report was also put together under the leadership of Susan Waddington extending the debate to cover the issue of the trade in women.\n",
            "hyp1: Susan Waddington extended the debate on the trade in women to include an important report.\n",
            "hyp2: Susan Waddington extended the debate on the trade in women by putting together a critical report. \n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =      12.61 ms /    16 runs   (    0.79 ms per token,  1268.73 tokens per second)\n",
            "llama_print_timings: prompt eval time =     268.51 ms /    69 tokens (    3.89 ms per token,   256.97 tokens per second)\n",
            "llama_print_timings:        eval time =    1903.24 ms /    15 runs   (  126.88 ms per token,     7.88 tokens per second)\n",
            "llama_print_timings:       total time =    2590.83 ms /    84 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Over recent years, these have largely been behind the development of a significant number of economic policies, which is good for employment.\n",
            "hyp1: A lot of economic policies are good for employment. \n",
            "hyp2: A lot of economic policies are good for employment because of these.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.73 ms /    16 runs   (    0.48 ms per token,  2070.93 tokens per second)\n",
            "llama_print_timings: prompt eval time =     309.04 ms /    85 tokens (    3.64 ms per token,   275.04 tokens per second)\n",
            "llama_print_timings:        eval time =    1582.41 ms /    15 runs   (  105.49 ms per token,     9.48 tokens per second)\n",
            "llama_print_timings:       total time =    2201.57 ms /   100 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The presidency is very much of this view, as economic and political stability in Jordan is a crucial factor in peace-keeping in the Middle East.\n",
            "hyp1: Peace-keeping in the Middle East depends on economic and political stability in the north of Jordan.\n",
            "hyp2: Peace-keeping in the Middle East depends on economic and political stability in Jordan.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.83 ms /    16 runs   (    0.49 ms per token,  2044.47 tokens per second)\n",
            "llama_print_timings: prompt eval time =     287.86 ms /   105 tokens (    2.74 ms per token,   364.77 tokens per second)\n",
            "llama_print_timings:        eval time =    1590.37 ms /    15 runs   (  106.02 ms per token,     9.43 tokens per second)\n",
            "llama_print_timings:       total time =    2241.43 ms /   120 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: In addition, the Commission is unable to accept Amendments Nos 27 and 28, because they go beyond the scope of this programme.\n",
            "hyp1: The Commission can't accept Amendments 27 and 28 because they go past the scope of the programme.\n",
            "hyp2: The Commission can't accept Amendments 27 and 28 even though they go past the scope of the programme.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       6.94 ms /    14 runs   (    0.50 ms per token,  2017.29 tokens per second)\n",
            "llama_print_timings: prompt eval time =     287.69 ms /    95 tokens (    3.03 ms per token,   330.21 tokens per second)\n",
            "llama_print_timings:        eval time =    1350.98 ms /    13 runs   (  103.92 ms per token,     9.62 tokens per second)\n",
            "llama_print_timings:       total time =    1976.52 ms /   108 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: On the contrary, things came to a head in the crisis and we now know for certain that this Augean stable at any rate is going to be cleaned out.\n",
            "hyp1: The crisis came to a head and we now know that the Hercules stable will be cleaned out.\n",
            "hyp2: The crisis came to a head and we now know that the Augean stable will be cleaned out.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =      10.48 ms /    16 runs   (    0.66 ms per token,  1526.43 tokens per second)\n",
            "llama_print_timings: prompt eval time =     280.73 ms /    85 tokens (    3.30 ms per token,   302.78 tokens per second)\n",
            "llama_print_timings:        eval time =    1878.32 ms /    15 runs   (  125.22 ms per token,     7.99 tokens per second)\n",
            "llama_print_timings:       total time =    2645.18 ms /   100 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: As you will remember, on 30 October 1997, the Commission presented a Community initiative for the European Capital of Culture.\n",
            "hyp1: The European Capital of Culture community initiative was presented by the Commission in 1997.\n",
            "hyp2: The European Capital of Culture was presented by the Commission in 1997.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.93 ms /    16 runs   (    0.50 ms per token,  2018.42 tokens per second)\n",
            "llama_print_timings: prompt eval time =     287.19 ms /    83 tokens (    3.46 ms per token,   289.00 tokens per second)\n",
            "llama_print_timings:        eval time =    1611.34 ms /    15 runs   (  107.42 ms per token,     9.31 tokens per second)\n",
            "llama_print_timings:       total time =    2250.75 ms /    98 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: A second point on the timing: this Committee of Experts will make its report available - if I recall correctly - on 15 March.\n",
            "hyp1: If I remember correctly, the report will be made available on March 15.\n",
            "hyp2: If I remember correctly, the report will be made available on March 17.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.83 ms /    16 runs   (    0.49 ms per token,  2044.21 tokens per second)\n",
            "llama_print_timings: prompt eval time =     281.50 ms /    85 tokens (    3.31 ms per token,   301.96 tokens per second)\n",
            "llama_print_timings:        eval time =    1569.03 ms /    15 runs   (  104.60 ms per token,     9.56 tokens per second)\n",
            "llama_print_timings:       total time =    2136.14 ms /   100 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: There are four points in relation to which the Committee on Agriculture and Rural Development has made changes which we believe will be adopted.\n",
            "hyp1: The Committee on Agriculture and Rural Development made changes that we think will be adopted.\n",
            "hyp2: The Committee on Agriculture and Rural Development did not make changes that we think will be adopted.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.85 ms /    16 runs   (    0.49 ms per token,  2037.44 tokens per second)\n",
            "llama_print_timings: prompt eval time =     270.05 ms /    73 tokens (    3.70 ms per token,   270.32 tokens per second)\n",
            "llama_print_timings:        eval time =    1526.21 ms /    15 runs   (  101.75 ms per token,     9.83 tokens per second)\n",
            "llama_print_timings:       total time =    2056.80 ms /    88 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: In the Nordic countries there are excellent examples of this, and for that reason I recommend that Amendment No 1 be rejected.\n",
            "hyp1: There are excellent examples of this in the Nordic countries.\n",
            "hyp2: There are excellent examples of this Amendment in the Nordic countries.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.82 ms /    16 runs   (    0.61 ms per token,  1629.99 tokens per second)\n",
            "llama_print_timings: prompt eval time =     268.74 ms /    71 tokens (    3.79 ms per token,   264.19 tokens per second)\n",
            "llama_print_timings:        eval time =    1868.38 ms /    15 runs   (  124.56 ms per token,     8.03 tokens per second)\n",
            "llama_print_timings:       total time =    2579.56 ms /    86 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: It is we politicians who are most concerned, because we understand the link between energy consumption and CO2 emissions.\n",
            "hyp1: Politicians understand the link between CO2 emissions and energy consumption.\n",
            "hyp2: Politicians don't understand the link between CO2 emissions and energy consumption.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.05 ms /    16 runs   (    0.50 ms per token,  1988.32 tokens per second)\n",
            "llama_print_timings: prompt eval time =     298.61 ms /    80 tokens (    3.73 ms per token,   267.91 tokens per second)\n",
            "llama_print_timings:        eval time =    1679.07 ms /    15 runs   (  111.94 ms per token,     8.93 tokens per second)\n",
            "llama_print_timings:       total time =    2257.16 ms /    95 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Having pressed the Commission to present this proposal, the Socialist Group - to which I have the privilege of belonging - now supports it enthusiastically.\n",
            "hyp1: The Communist Group now supports the proposal after pressing the Commission to present it.\n",
            "hyp2: The Socialist Group now supports the proposal after pressing the Commission to present it.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.25 ms /    16 runs   (    0.52 ms per token,  1939.86 tokens per second)\n",
            "llama_print_timings: prompt eval time =     293.39 ms /    77 tokens (    3.81 ms per token,   262.45 tokens per second)\n",
            "llama_print_timings:        eval time =    1566.62 ms /    15 runs   (  104.44 ms per token,     9.57 tokens per second)\n",
            "llama_print_timings:       total time =    2121.15 ms /    92 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: For example, unemployment is very high in Réunion and in the West Indies. Therefore, we should not devote our efforts to integrating unlimited numbers of immigrants.\n",
            "hyp1: The West Indies and Réunion have low unemployment.\n",
            "hyp2: The West Indies and Réunion have high unemployment.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.57 ms /    16 runs   (    0.54 ms per token,  1866.54 tokens per second)\n",
            "llama_print_timings: prompt eval time =     277.22 ms /    81 tokens (    3.42 ms per token,   292.19 tokens per second)\n",
            "llama_print_timings:        eval time =    1744.02 ms /    15 runs   (  116.27 ms per token,     8.60 tokens per second)\n",
            "llama_print_timings:       total time =    2324.43 ms /    96 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: A uniform phased reduction of 3 % will primarily penalise the sectors of production which receive the most aid, such as tobacco growing.\n",
            "hyp1: Tobacco growing will benefit by a 3 % phased reduction in aid.\n",
            "hyp2: Tobacco growing will be affected by a 3 % phased reduction in aid.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.20 ms /    16 runs   (    0.57 ms per token,  1739.32 tokens per second)\n",
            "llama_print_timings: prompt eval time =     310.49 ms /    95 tokens (    3.27 ms per token,   305.97 tokens per second)\n",
            "llama_print_timings:        eval time =    2192.00 ms /    15 runs   (  146.13 ms per token,     6.84 tokens per second)\n",
            "llama_print_timings:       total time =    3053.85 ms /   110 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The situation in India is appalling as 50 million women are missing because they are eliminated right from the stage of conception.\n",
            "hyp1: 50 million women are missing in India due to the fact that they are eliminated before they are conceived.\n",
            "hyp2: 50 million women are missing in India due to the fact that they are eliminated directly after they have been conceived.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.45 ms /    16 runs   (    0.59 ms per token,  1692.41 tokens per second)\n",
            "llama_print_timings: prompt eval time =     286.05 ms /    82 tokens (    3.49 ms per token,   286.67 tokens per second)\n",
            "llama_print_timings:        eval time =    1655.85 ms /    15 runs   (  110.39 ms per token,     9.06 tokens per second)\n",
            "llama_print_timings:       total time =    2230.55 ms /    97 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Nevertheless, we want to limit the Council regulations to general provisions and to cover the remaining provisions in implementing regulations.\n",
            "hyp1: We want the regulations to only cover general provisions and the rest of the regulations.\n",
            "hyp2: We want the Council's regulations to only cover general provisions while the rest of the provisions should be covered by implementing regulations\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.95 ms /    16 runs   (    0.50 ms per token,  2011.57 tokens per second)\n",
            "llama_print_timings: prompt eval time =     289.31 ms /    92 tokens (    3.14 ms per token,   318.00 tokens per second)\n",
            "llama_print_timings:        eval time =    1621.86 ms /    15 runs   (  108.12 ms per token,     9.25 tokens per second)\n",
            "llama_print_timings:       total time =    2242.85 ms /   107 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: I hope that the Euro-Mediterranean Conference to held in Stuttgart between 4 and 6 April will enable us to make progress in that direction.\n",
            "hyp1: I hope the Euro-Mediterranean Conference will help us make progress in that direction.\n",
            "hyp2: I hope the Euro-Mediterranean Conference will help you make progress in that direction.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.77 ms /    16 runs   (    0.55 ms per token,  1825.03 tokens per second)\n",
            "llama_print_timings: prompt eval time =     279.66 ms /    88 tokens (    3.18 ms per token,   314.67 tokens per second)\n",
            "llama_print_timings:        eval time =    1847.70 ms /    15 runs   (  123.18 ms per token,     8.12 tokens per second)\n",
            "llama_print_timings:       total time =    2509.04 ms /   103 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: In 1996, the Commission submitted its proposal to have Turkey included in the Socrates, Youth for Europe and Leonardo programmes.\n",
            "hyp1: Turkey was included in the Commission's proposal in 1996.\n",
            "hyp2: Turkey was included in the Commission's proposal for three different programmes in 1996.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.97 ms /    16 runs   (    0.50 ms per token,  2008.28 tokens per second)\n",
            "llama_print_timings: prompt eval time =     321.58 ms /   106 tokens (    3.03 ms per token,   329.62 tokens per second)\n",
            "llama_print_timings:        eval time =    1515.97 ms /    15 runs   (  101.06 ms per token,     9.89 tokens per second)\n",
            "llama_print_timings:       total time =    2179.53 ms /   121 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: We cannot accept Amendments Nos 19, 21 and 51, which are aimed at regulating the import and export of genetically modified organisms.\n",
            "hyp1: Amendments Nos 21 and 23 are aimed at regulating genetically modified organisms.\n",
            "hyp2: Amendments Nos 19 and 21 are aimed at regulating genetically modified organisms.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.95 ms /    16 runs   (    0.50 ms per token,  2013.09 tokens per second)\n",
            "llama_print_timings: prompt eval time =     298.56 ms /    96 tokens (    3.11 ms per token,   321.54 tokens per second)\n",
            "llama_print_timings:        eval time =    2005.15 ms /    15 runs   (  133.68 ms per token,     7.48 tokens per second)\n",
            "llama_print_timings:       total time =    2635.53 ms /   111 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: I would also like to draw attention to Amendments Nos 27, 29 and 32 on public inquiries or consultation in connection with trial releases.\n",
            "hyp1: Amendments Nos 27 and 29 are about consultations related to trial releases.\n",
            "hyp2: Amendments Nos 27 and 29 are related to trial releases.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.50 ms /    16 runs   (    0.53 ms per token,  1882.57 tokens per second)\n",
            "llama_print_timings: prompt eval time =     291.76 ms /    82 tokens (    3.56 ms per token,   281.06 tokens per second)\n",
            "llama_print_timings:        eval time =    1641.84 ms /    15 runs   (  109.46 ms per token,     9.14 tokens per second)\n",
            "llama_print_timings:       total time =    2228.71 ms /    97 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: A true strategic partnership should be established with this great country of a billion inhabitants, compared with 1.2 billion in China.\n",
            "hyp1: China has 1.2 billion people, while this great country has a billion inhabitants.\n",
            "hyp2: China has 1.2 billion people, while this great country has a million inhabitants.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.55 ms /    16 runs   (    0.53 ms per token,  1870.25 tokens per second)\n",
            "llama_print_timings: prompt eval time =     310.43 ms /   105 tokens (    2.96 ms per token,   338.25 tokens per second)\n",
            "llama_print_timings:        eval time =    1753.43 ms /    15 runs   (  116.90 ms per token,     8.55 tokens per second)\n",
            "llama_print_timings:       total time =    2666.34 ms /   120 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: This is a positive development, and I can of course endorse the outcome which appears in the form of Compromise Amendments Nos 189 to 201.\n",
            "hyp1: The outcome appears in the form of Compromise Amendments Nos 189 to 201.\n",
            "hyp2: The outcome appears in the form of Compromise Amendments Nos 190 to 202.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.27 ms /    16 runs   (    0.52 ms per token,  1934.94 tokens per second)\n",
            "llama_print_timings: prompt eval time =     286.78 ms /    87 tokens (    3.30 ms per token,   303.37 tokens per second)\n",
            "llama_print_timings:        eval time =    1950.22 ms /    15 runs   (  130.01 ms per token,     7.69 tokens per second)\n",
            "llama_print_timings:       total time =    2555.99 ms /   102 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: For instance, in the summer it would perhaps be possible to set up surveillance operations to detect the flimsy craft used to make the crossing.\n",
            "hyp1: It would be possible to detect the flimsy craft that makes the crossing in the summer.\n",
            "hyp2: It would be possible to overlook the flimsy craft that makes the crossing in the summer.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =      12.50 ms /    16 runs   (    0.78 ms per token,  1280.20 tokens per second)\n",
            "llama_print_timings: prompt eval time =     301.15 ms /    83 tokens (    3.63 ms per token,   275.61 tokens per second)\n",
            "llama_print_timings:        eval time =    1592.26 ms /    15 runs   (  106.15 ms per token,     9.42 tokens per second)\n",
            "llama_print_timings:       total time =    2187.32 ms /    98 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The increase in the number of staff employed must be strictly controlled, and this is a point on which I disagree with the Committee of Wise Men.\n",
            "hyp1: I agree with the idea of controlling the increase in the number of staff.\n",
            "hyp2: I don't agree with the idea of controlling the increase in the number of staff.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.90 ms /    16 runs   (    0.56 ms per token,  1797.95 tokens per second)\n",
            "llama_print_timings: prompt eval time =     270.21 ms /    74 tokens (    3.65 ms per token,   273.87 tokens per second)\n",
            "llama_print_timings:        eval time =    1837.35 ms /    15 runs   (  122.49 ms per token,     8.16 tokens per second)\n",
            "llama_print_timings:       total time =    2382.90 ms /    89 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The public must be aware of the collective responsibility borne by the whole Commission, but also of every individual Commissioner's accountability.\n",
            "hyp1: Every individual accountability must be known by the Commissioners.\n",
            "hyp2: Every individual Commissioner's accountability must be known by the public.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.96 ms /    16 runs   (    0.50 ms per token,  2010.81 tokens per second)\n",
            "llama_print_timings: prompt eval time =     304.32 ms /    86 tokens (    3.54 ms per token,   282.60 tokens per second)\n",
            "llama_print_timings:        eval time =    1608.15 ms /    15 runs   (  107.21 ms per token,     9.33 tokens per second)\n",
            "llama_print_timings:       total time =    2216.67 ms /   101 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: We need to ensure that investigations at the external borders of the EU are effective and to investigate the main routes used for illegal activities.\n",
            "hyp1: We need to make sure that the investigations at the EU's external borders are effective.\n",
            "hyp2: We need to make sure that the investigations at the EU's external borders are not effective.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.81 ms /    16 runs   (    0.49 ms per token,  2049.18 tokens per second)\n",
            "llama_print_timings: prompt eval time =     304.73 ms /   112 tokens (    2.72 ms per token,   367.54 tokens per second)\n",
            "llama_print_timings:        eval time =    1580.85 ms /    15 runs   (  105.39 ms per token,     9.49 tokens per second)\n",
            "llama_print_timings:       total time =    2265.13 ms /   127 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: For the same reason, we cannot accept Amendments Nos 5, 6, 7, 11, 16 and 20 or the second part of Amendments Nos 9 and 15.\n",
            "hyp1: The second part of Amendments 9 and 15 cannot be accepted for the same reason.\n",
            "hyp2: The fifth part of Amendments 9 and 15 cannot be accepted for the same reason.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.97 ms /    16 runs   (    0.50 ms per token,  2008.54 tokens per second)\n",
            "llama_print_timings: prompt eval time =     290.94 ms /   101 tokens (    2.88 ms per token,   347.15 tokens per second)\n",
            "llama_print_timings:        eval time =    1617.47 ms /    15 runs   (  107.83 ms per token,     9.27 tokens per second)\n",
            "llama_print_timings:       total time =    2234.29 ms /   116 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Commissioner, the Malta forum will take place in March, and it is a great pity that the European Parliament will not be allowed to participate in it.\n",
            "hyp1: It is sad that the European Parliament won't be able to participate in the Malta forum in March.\n",
            "hyp2: It is sad that the European Parliament won't be able to participate in the Malta forum in March 2004.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =      16.70 ms /    16 runs   (    1.04 ms per token,   958.20 tokens per second)\n",
            "llama_print_timings: prompt eval time =     338.43 ms /    92 tokens (    3.68 ms per token,   271.84 tokens per second)\n",
            "llama_print_timings:        eval time =    2142.41 ms /    15 runs   (  142.83 ms per token,     7.00 tokens per second)\n",
            "llama_print_timings:       total time =    3348.90 ms /   107 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: And I should like to warn this House that we must be careful when voting on Amendment No 98 so that we are not inconsistent.\n",
            "hyp1: I would like to warn the House that all of us need to be careful when voting on the amendment.\n",
            "hyp2: I would like to warn the House that they need to be careful when voting on the amendment.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.24 ms /    16 runs   (    0.52 ms per token,  1941.28 tokens per second)\n",
            "llama_print_timings: prompt eval time =     314.59 ms /   102 tokens (    3.08 ms per token,   324.23 tokens per second)\n",
            "llama_print_timings:        eval time =    1652.76 ms /    15 runs   (  110.18 ms per token,     9.08 tokens per second)\n",
            "llama_print_timings:       total time =    2321.70 ms /   117 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: In addition, there will be no benefit to exchequers as the taxable interest payments will be outside the 15 Member States.\n",
            "hyp1: There will be no benefit to exchequers as interest payments outside of the 15 Member States will be taxed.\n",
            "hyp2: There will be no benefit to exchequers as only interest payments outside of the 15 Member States can be taxed.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.26 ms /    16 runs   (    0.52 ms per token,  1936.58 tokens per second)\n",
            "llama_print_timings: prompt eval time =     291.25 ms /    85 tokens (    3.43 ms per token,   291.84 tokens per second)\n",
            "llama_print_timings:        eval time =    1578.19 ms /    15 runs   (  105.21 ms per token,     9.50 tokens per second)\n",
            "llama_print_timings:       total time =    2167.28 ms /   100 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Mr Florenz said, in his nice way, that we would be taking a softer line and the directive would only apply from the year 2020.\n",
            "hyp1: The directive would not apply from the year 2020.\n",
            "hyp2: The directive would only apply from the year 2020.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.78 ms /    16 runs   (    0.49 ms per token,  2055.76 tokens per second)\n",
            "llama_print_timings: prompt eval time =     272.62 ms /    79 tokens (    3.45 ms per token,   289.78 tokens per second)\n",
            "llama_print_timings:        eval time =    1549.32 ms /    15 runs   (  103.29 ms per token,     9.68 tokens per second)\n",
            "llama_print_timings:       total time =    2097.58 ms /    94 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Establishing a new framework directive is certainly going to take some doing: the way it was dealt with at first reading was extremely confusing.\n",
            "hyp1: The way in which the framework directive was dealt with was very clear.\n",
            "hyp2: The way in which the framework directive was dealt with was very confusing.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.74 ms /    16 runs   (    0.55 ms per token,  1830.66 tokens per second)\n",
            "llama_print_timings: prompt eval time =     308.36 ms /    90 tokens (    3.43 ms per token,   291.87 tokens per second)\n",
            "llama_print_timings:        eval time =    1875.98 ms /    15 runs   (  125.07 ms per token,     8.00 tokens per second)\n",
            "llama_print_timings:       total time =    2720.76 ms /   105 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: However, there is one issue, outlined in paragraph 11, which clearly detracts from the report's otherwise good intentions.\n",
            "hyp1: Paragraph 11 contains an issue that detracts from the good intentions of the report.\n",
            "hyp2: Paragraph 11.4 contains an issue that detracts from the good intentions of the report.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =      13.92 ms /    16 runs   (    0.87 ms per token,  1149.34 tokens per second)\n",
            "llama_print_timings: prompt eval time =     311.01 ms /    98 tokens (    3.17 ms per token,   315.10 tokens per second)\n",
            "llama_print_timings:        eval time =    1860.74 ms /    15 runs   (  124.05 ms per token,     8.06 tokens per second)\n",
            "llama_print_timings:       total time =    2737.75 ms /   113 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: In Kyoto, the European Union committed itself to reducing greenhouse gas emissions by 8 % compared to 1990 levels before 2012.\n",
            "hyp1: The European Union pledged in Kyoto to reduce greenhouse gas emissions by 8 percent.\n",
            "hyp2: The European Union pledged in Kyoto to reduce greenhouse gas emissions by 8 metric tons of CO2.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.60 ms /    16 runs   (    0.48 ms per token,  2104.16 tokens per second)\n",
            "llama_print_timings: prompt eval time =     305.09 ms /   104 tokens (    2.93 ms per token,   340.88 tokens per second)\n",
            "llama_print_timings:        eval time =    1567.51 ms /    15 runs   (  104.50 ms per token,     9.57 tokens per second)\n",
            "llama_print_timings:       total time =    2211.68 ms /   119 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: It is not only a pressing problem for the Member States in the Mediterranean region, but also concerns, to a large extent, the entire European Union.\n",
            "hyp1: It's a big problem for the Member States in the Mediterranean region and it's also a big problem for the European Union.\n",
            "hyp2: It's a big problem for the United States in the Mediterranean region and it's also a big problem for the European Union.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.33 ms /    16 runs   (    0.58 ms per token,  1715.27 tokens per second)\n",
            "llama_print_timings: prompt eval time =     295.87 ms /   103 tokens (    2.87 ms per token,   348.13 tokens per second)\n",
            "llama_print_timings:        eval time =    1721.33 ms /    15 runs   (  114.76 ms per token,     8.71 tokens per second)\n",
            "llama_print_timings:       total time =    2369.31 ms /   118 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: It taxes the interest on the savings of the small investor and the small saver and eases the tax burden on companies and large conglomerates.\n",
            "hyp1: It lowers the tax burden on companies and large conglomerates by taxing the interest on savings of small investers and savers.\n",
            "hyp2: It lowers the tax burden on companies and large conglomerates by taxing the interest on savings.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.99 ms /    16 runs   (    0.50 ms per token,  2003.00 tokens per second)\n",
            "llama_print_timings: prompt eval time =     288.26 ms /    78 tokens (    3.70 ms per token,   270.59 tokens per second)\n",
            "llama_print_timings:        eval time =    1575.37 ms /    15 runs   (  105.02 ms per token,     9.52 tokens per second)\n",
            "llama_print_timings:       total time =    2121.36 ms /    93 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Amendment No 3 refers to the putting in place of a Community plan of action which, however, has already been established.\n",
            "hyp1: The Community plan of action has already been put in place successfully and with only a few delays.\n",
            "hyp2: The Community plan of action has already been put in place.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.59 ms /    16 runs   (    0.47 ms per token,  2108.31 tokens per second)\n",
            "llama_print_timings: prompt eval time =     285.47 ms /    87 tokens (    3.28 ms per token,   304.77 tokens per second)\n",
            "llama_print_timings:        eval time =    1643.59 ms /    15 runs   (  109.57 ms per token,     9.13 tokens per second)\n",
            "llama_print_timings:       total time =    2212.07 ms /   102 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Berlin was the scene of his rise to fame and it was to Berlin that he returned in 1945 to promote dialogue with the German people.\n",
            "hyp1: In 1945 she came back to Berlin to promote dialogue with the Germans.\n",
            "hyp2: In 1945 he came back to Berlin to promote dialogue with the Germans.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.27 ms /    16 runs   (    0.45 ms per token,  2199.62 tokens per second)\n",
            "llama_print_timings: prompt eval time =     282.94 ms /    84 tokens (    3.37 ms per token,   296.88 tokens per second)\n",
            "llama_print_timings:        eval time =    1582.69 ms /    15 runs   (  105.51 ms per token,     9.48 tokens per second)\n",
            "llama_print_timings:       total time =    2151.62 ms /    99 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: However, I am opposed to the proposal contained in paragraph 8 of the resolution, because it will lead to increased supranationality.\n",
            "hyp1: Paragraph 8 of the resolution will lead to decreased supranationality.\n",
            "hyp2: Paragraph 8 of the resolution will lead to increased supranationality.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.02 ms /    16 runs   (    0.56 ms per token,  1774.43 tokens per second)\n",
            "llama_print_timings: prompt eval time =     288.09 ms /    97 tokens (    2.97 ms per token,   336.71 tokens per second)\n",
            "llama_print_timings:        eval time =    1778.25 ms /    15 runs   (  118.55 ms per token,     8.44 tokens per second)\n",
            "llama_print_timings:       total time =    2405.31 ms /   112 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: According to what I have heard, the ISO will already have a label ready for the summer of 1999, in other words in a few months' time.\n",
            "hyp1: The MSZT will have a label ready in a few months, according to what I've heard.\n",
            "hyp2: The ISO will have a label ready in a few months, according to what I've heard.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.50 ms /    16 runs   (    0.47 ms per token,  2131.91 tokens per second)\n",
            "llama_print_timings: prompt eval time =     294.05 ms /    74 tokens (    3.97 ms per token,   251.66 tokens per second)\n",
            "llama_print_timings:        eval time =    1601.09 ms /    15 runs   (  106.74 ms per token,     9.37 tokens per second)\n",
            "llama_print_timings:       total time =    2153.29 ms /    89 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Nevertheless, there are two particular points that cannot go unmentioned. The first relates to road transport and is the issue of working time.\n",
            "hyp1: The particular issue of working time within road transport cannot be ignored.\n",
            "hyp2: The issues of working time and road transport cannot be ignored.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.65 ms /    16 runs   (    0.48 ms per token,  2091.78 tokens per second)\n",
            "llama_print_timings: prompt eval time =     277.27 ms /    88 tokens (    3.15 ms per token,   317.38 tokens per second)\n",
            "llama_print_timings:        eval time =    1557.07 ms /    15 runs   (  103.80 ms per token,     9.63 tokens per second)\n",
            "llama_print_timings:       total time =    2135.09 ms /   103 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: In May 1998, the Single Market Council adopted provisions regarding the development of a single market in medicinal products.\n",
            "hyp1: The Development of a Single Market in Medicinal Products was adopted by the Single Market Council in 1989.\n",
            "hyp2: The Development of a Single Market in Medicinal Products was adopted by the Single Market Council.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.99 ms /    16 runs   (    0.50 ms per token,  2002.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =     283.19 ms /    86 tokens (    3.29 ms per token,   303.68 tokens per second)\n",
            "llama_print_timings:        eval time =    1505.25 ms /    15 runs   (  100.35 ms per token,     9.97 tokens per second)\n",
            "llama_print_timings:       total time =    2093.05 ms /   101 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: We should remember that the nuclear power station at Chernobyl was, until April 1986, said to have presented 'few risks'.\n",
            "hyp1: The nuclear power station at Chernobyl was said to have presented'few risks'.\n",
            "hyp2: The nuclear power station at Chernobyl was said to have presented 'no risks'.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.33 ms /    16 runs   (    0.58 ms per token,  1715.08 tokens per second)\n",
            "llama_print_timings: prompt eval time =     283.10 ms /    90 tokens (    3.15 ms per token,   317.91 tokens per second)\n",
            "llama_print_timings:        eval time =    1870.88 ms /    15 runs   (  124.73 ms per token,     8.02 tokens per second)\n",
            "llama_print_timings:       total time =    2494.16 ms /   105 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The report we asked for from the committee of experts is part of the consequences of our withholding the discharge for 1996.\n",
            "hyp1: The committee of experts gave us a report on the consequences of withholding the discharge.\n",
            "hyp2: The committee of experts gave us a report as part of the consequences of withholding the discharge.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.40 ms /    16 runs   (    0.53 ms per token,  1904.31 tokens per second)\n",
            "llama_print_timings: prompt eval time =     308.66 ms /    87 tokens (    3.55 ms per token,   281.87 tokens per second)\n",
            "llama_print_timings:        eval time =    1562.69 ms /    15 runs   (  104.18 ms per token,     9.60 tokens per second)\n",
            "llama_print_timings:       total time =    2155.20 ms /   102 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The irregularities which have been identified must never happen again, and this means that both the methods and the approach need to be changed.\n",
            "hyp1: The approach and methods need to be changed because they need to never happen again.\n",
            "hyp2: It's imperative to never repeat these irregularities, which requires a change in the methods and approach. \n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.65 ms /    16 runs   (    0.48 ms per token,  2091.23 tokens per second)\n",
            "llama_print_timings: prompt eval time =     290.48 ms /    98 tokens (    2.96 ms per token,   337.37 tokens per second)\n",
            "llama_print_timings:        eval time =    1544.74 ms /    15 runs   (  102.98 ms per token,     9.71 tokens per second)\n",
            "llama_print_timings:       total time =    2146.25 ms /   113 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: I am referring more particularly here to Amendments Nos 4 and 33, which provide for the directive to be monitored by the Commission.\n",
            "hyp1: Amendments Nos 4 and 33 give the Commission no power to monitor the directive.\n",
            "hyp2: Amendments Nos 4 and 33 give the Commission the power to monitor the directive.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.14 ms /    16 runs   (    0.51 ms per token,  1964.88 tokens per second)\n",
            "llama_print_timings: prompt eval time =     271.78 ms /    82 tokens (    3.31 ms per token,   301.71 tokens per second)\n",
            "llama_print_timings:        eval time =    1561.09 ms /    15 runs   (  104.07 ms per token,     9.61 tokens per second)\n",
            "llama_print_timings:       total time =    2100.85 ms /    97 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: That is why it is also important that the innovation dimension should become a more integral part of interregional and cross-border cooperation.\n",
            "hyp1: It is important that innovation is included in interregional and cross border cooperation.\n",
            "hyp2: It is important that innovation becomes a more essential part of interregional and cross border cooperation.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.14 ms /    16 runs   (    0.57 ms per token,  1750.55 tokens per second)\n",
            "llama_print_timings: prompt eval time =     272.50 ms /    78 tokens (    3.49 ms per token,   286.23 tokens per second)\n",
            "llama_print_timings:        eval time =    1795.65 ms /    15 runs   (  119.71 ms per token,     8.35 tokens per second)\n",
            "llama_print_timings:       total time =    2390.43 ms /    93 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Environmental considerations must feature prominently in this process, particularly sustainable development, as it is one of the distinguishing features of the European Union.\n",
            "hyp1: Sustainable development is a distinguishing feature of the European Union.\n",
            "hyp2: One of the distinguishing features of the European Union is the environment.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.78 ms /    16 runs   (    0.49 ms per token,  2057.61 tokens per second)\n",
            "llama_print_timings: prompt eval time =     295.50 ms /    98 tokens (    3.02 ms per token,   331.64 tokens per second)\n",
            "llama_print_timings:        eval time =    1509.82 ms /    15 runs   (  100.65 ms per token,     9.93 tokens per second)\n",
            "llama_print_timings:       total time =    2124.42 ms /   113 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Mrs Haug's report is an important addition to the Agenda 2000 debate, and to the ongoing preparation process for the next phase of the programme.\n",
            "hyp1: An important addition to the Agenda 2000 debate is Mx Haug's report.\n",
            "hyp2: An important addition to the Agenda 2000 debate is Mrs Haug's report.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.19 ms /    15 runs   (    0.48 ms per token,  2085.94 tokens per second)\n",
            "llama_print_timings: prompt eval time =     301.88 ms /   109 tokens (    2.77 ms per token,   361.08 tokens per second)\n",
            "llama_print_timings:        eval time =    1430.72 ms /    14 runs   (  102.19 ms per token,     9.79 tokens per second)\n",
            "llama_print_timings:       total time =    2094.46 ms /   123 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Fortunately, there has been a cease-fire since 1994 which is opening the way for negotiation, and the Minsk Group is taking charge of this.\n",
            "hyp1: There has been an open-fire since 1994 and the Minsk Group is in charge of this.\n",
            "hyp2: There has been a cease-fire since 1994 and the Minsk Group is in charge of this.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.11 ms /    16 runs   (    0.51 ms per token,  1972.39 tokens per second)\n",
            "llama_print_timings: prompt eval time =     288.16 ms /   110 tokens (    2.62 ms per token,   381.74 tokens per second)\n",
            "llama_print_timings:        eval time =    1669.69 ms /    15 runs   (  111.31 ms per token,     8.98 tokens per second)\n",
            "llama_print_timings:       total time =    2317.93 ms /   125 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Women make up 80 % of the people who carry out secretarial and office work, while men occupy 87 % of the managerial positions.\n",
            "hyp1: Eighty percent of the people who work in the office are women, while 87 percent are men.\n",
            "hyp2: Eighty percent of the people who work in the secretarial positions at the office are women, while 87 percent of those with managerial positions are men.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.72 ms /    16 runs   (    0.61 ms per token,  1645.92 tokens per second)\n",
            "llama_print_timings: prompt eval time =     293.22 ms /    93 tokens (    3.15 ms per token,   317.17 tokens per second)\n",
            "llama_print_timings:        eval time =    1830.86 ms /    15 runs   (  122.06 ms per token,     8.19 tokens per second)\n",
            "llama_print_timings:       total time =    2561.72 ms /   108 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: In a similar manner, I could accept the proposed Amendment No 11, which would require the same separation but in a shorter time period.\n",
            "hyp1: The proposed Amendment No 11 requires the same separation, but in a shorter period of time.\n",
            "hyp2: The proposed Amendment No 11 previously required the same separation, but in a shorter period of time.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.00 ms /    16 runs   (    0.56 ms per token,  1777.19 tokens per second)\n",
            "llama_print_timings: prompt eval time =     301.02 ms /    85 tokens (    3.54 ms per token,   282.37 tokens per second)\n",
            "llama_print_timings:        eval time =    1535.31 ms /    15 runs   (  102.35 ms per token,     9.77 tokens per second)\n",
            "llama_print_timings:       total time =    2112.87 ms /   100 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The outcome will hinge on how the new decision-making procedures work out in practice. We cannot therefore support paragraphs 30 and 31.\n",
            "hyp1: Paragraphs 300 and 31 are not supported by us.\n",
            "hyp2: Paragraphs 30 and 31 are not supported by us.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.99 ms /    16 runs   (    0.50 ms per token,  2001.75 tokens per second)\n",
            "llama_print_timings: prompt eval time =     280.78 ms /    84 tokens (    3.34 ms per token,   299.16 tokens per second)\n",
            "llama_print_timings:        eval time =    1754.23 ms /    15 runs   (  116.95 ms per token,     8.55 tokens per second)\n",
            "llama_print_timings:       total time =    2344.60 ms /    99 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: But the US, in threatening a trade boycott, is overstepping the bounds of that clear position and the bounds of the transatlantic partnership.\n",
            "hyp1: The US is overstepping its bounds by threatening to boycott the oil trade.\n",
            "hyp2: The US is overstepping its bounds by threatening to boycott trade.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.24 ms /    16 runs   (    0.52 ms per token,  1941.28 tokens per second)\n",
            "llama_print_timings: prompt eval time =     388.60 ms /   161 tokens (    2.41 ms per token,   414.31 tokens per second)\n",
            "llama_print_timings:        eval time =    1997.17 ms /    15 runs   (  133.14 ms per token,     7.51 tokens per second)\n",
            "llama_print_timings:       total time =    2906.03 ms /   176 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: In short, therefore, the Commission accepts Amendment Nos 1, 5, 6, 7, 8, 9 and 11, whilst rejecting Amendments Nos 2, 3, 4, 10 and 12.\n",
            "hyp1: The Commission accepts Amendments 1 to 9 and 11 and rejects Amendments 3, 4, 10 and 12.\n",
            "hyp2: The Commission accepts Amendments 1, 5, 6, 7, 8, 9 and 11 and rejects Amendments 3, 4, 10 and 12.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.60 ms /    16 runs   (    0.54 ms per token,  1861.55 tokens per second)\n",
            "llama_print_timings: prompt eval time =     313.23 ms /    98 tokens (    3.20 ms per token,   312.87 tokens per second)\n",
            "llama_print_timings:        eval time =    1644.42 ms /    15 runs   (  109.63 ms per token,     9.12 tokens per second)\n",
            "llama_print_timings:       total time =    2458.01 ms /   113 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: While the Kalanke ruling was a setback, the European Court of Justice's decision in the 1997 Marschall judgment was encouraging.\n",
            "hyp1: The European Court of Justice's 1997 Marschall judgement was an encouraging one.\n",
            "hyp2: The European Court of Justice's 1997 Marschall judgement was not an encouraging one.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.75 ms /    16 runs   (    0.48 ms per token,  2065.58 tokens per second)\n",
            "llama_print_timings: prompt eval time =     291.12 ms /    89 tokens (    3.27 ms per token,   305.72 tokens per second)\n",
            "llama_print_timings:        eval time =    1539.18 ms /    15 runs   (  102.61 ms per token,     9.75 tokens per second)\n",
            "llama_print_timings:       total time =    2126.05 ms /   104 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: People would think we were mad if we said tomorrow that drivers were allowed to take their cars on the road without an insurance certificate.\n",
            "hyp1: If we said drivers could take their cars on the road without insurance, people would not think we were crazy.\n",
            "hyp2: If we said drivers could take their cars on the road without insurance, people would think we were crazy.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.24 ms /    16 runs   (    0.51 ms per token,  1942.45 tokens per second)\n",
            "llama_print_timings: prompt eval time =     273.96 ms /    79 tokens (    3.47 ms per token,   288.36 tokens per second)\n",
            "llama_print_timings:        eval time =    1646.57 ms /    15 runs   (  109.77 ms per token,     9.11 tokens per second)\n",
            "llama_print_timings:       total time =    2194.73 ms /    94 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: As we have already said, the adoption of such a stance is incompatible with the aims of the Treaty, because agricultural expenditure is compulsory.\n",
            "hyp1: The aims of the Treaty are compatible with the stance adopted.\n",
            "hyp2: The aims of the Treaty are incompatible with the stance adopted.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.35 ms /    14 runs   (    0.60 ms per token,  1676.04 tokens per second)\n",
            "llama_print_timings: prompt eval time =     305.52 ms /    79 tokens (    3.87 ms per token,   258.58 tokens per second)\n",
            "llama_print_timings:        eval time =    1634.56 ms /    13 runs   (  125.74 ms per token,     7.95 tokens per second)\n",
            "llama_print_timings:       total time =    2393.43 ms /    92 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Of the 279 local authorities that exist in Sweden, 211 - all of them in the interior of the country - have lost part of their population.\n",
            "hyp1: The majority of the national authorities in Sweden have lost population.\n",
            "hyp2: The majority of the local authorities in Sweden have lost population.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.77 ms /    16 runs   (    0.49 ms per token,  2059.20 tokens per second)\n",
            "llama_print_timings: prompt eval time =     327.75 ms /   126 tokens (    2.60 ms per token,   384.45 tokens per second)\n",
            "llama_print_timings:        eval time =    1518.01 ms /    15 runs   (  101.20 ms per token,     9.88 tokens per second)\n",
            "llama_print_timings:       total time =    2261.68 ms /   141 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: This is also the reason why we cannot accept Amendment No 25 or Amendments Nos 46 and 54, but we can endorse Amendment No 43.\n",
            "hyp1: I can endorse Amendment No 43 but I can't accept Amendments No 25 and Nos 46 and 54.\n",
            "hyp2: We can endorse Amendment No 43 but we can't accept Amendments No 25 and Nos 46 and 54.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.88 ms /    16 runs   (    0.49 ms per token,  2030.46 tokens per second)\n",
            "llama_print_timings: prompt eval time =     291.62 ms /    95 tokens (    3.07 ms per token,   325.76 tokens per second)\n",
            "llama_print_timings:        eval time =    1548.85 ms /    15 runs   (  103.26 ms per token,     9.68 tokens per second)\n",
            "llama_print_timings:       total time =    2145.21 ms /   110 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Madam President, in yesterday's International Herald Tribune, the following appeared in the section 'News from 50 years ago'.\n",
            "hyp1: The section \"news from 50 years ago\" appeared in yesterday'sInternational Herald Tribune.\n",
            "hyp2: This appeared in section \"news from 50 years ago\" in yesterday's International Herald Tribune.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.48 ms /    16 runs   (    0.47 ms per token,  2138.47 tokens per second)\n",
            "llama_print_timings: prompt eval time =     287.68 ms /    88 tokens (    3.27 ms per token,   305.89 tokens per second)\n",
            "llama_print_timings:        eval time =    1574.10 ms /    15 runs   (  104.94 ms per token,     9.53 tokens per second)\n",
            "llama_print_timings:       total time =    2151.72 ms /   103 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The current status quo of a 12-mile limit for the area is the best possible compromise for the whole of the French fishing sector.\n",
            "hyp1: The current 12 kilometer limit is the best compromise for the entire French fishing sector.\n",
            "hyp2: The current 12-mile limit is the best compromise for the entire French fishing sector.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.86 ms /    16 runs   (    0.55 ms per token,  1806.28 tokens per second)\n",
            "llama_print_timings: prompt eval time =     279.75 ms /    84 tokens (    3.33 ms per token,   300.27 tokens per second)\n",
            "llama_print_timings:        eval time =    1792.78 ms /    15 runs   (  119.52 ms per token,     8.37 tokens per second)\n",
            "llama_print_timings:       total time =    2405.19 ms /    99 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: It has come to my knowledge that we have 300 freelance interpreters whom the Commission has not paid properly since October.\n",
            "hyp1: 300 interpreters have been adequetly paid  by the Commission. \n",
            "hyp2: 300 interpreters have not been paid properly by the Commission.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.91 ms /    16 runs   (    0.49 ms per token,  2021.73 tokens per second)\n",
            "llama_print_timings: prompt eval time =     305.61 ms /    94 tokens (    3.25 ms per token,   307.58 tokens per second)\n",
            "llama_print_timings:        eval time =    1645.65 ms /    15 runs   (  109.71 ms per token,     9.11 tokens per second)\n",
            "llama_print_timings:       total time =    2252.17 ms /   109 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: For example, the conclave of foreign ministers in Luxembourg on 21 February made substantial progress on a number of issues.\n",
            "hyp1: Significant progress was made on a number of issues during the foreign minister's conclave in Luxembourg City.\n",
            "hyp2: Significant progress was made on a number of issues during the foreign minister's conclave in Luxembourg.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.13 ms /    16 runs   (    0.51 ms per token,  1968.75 tokens per second)\n",
            "llama_print_timings: prompt eval time =     291.85 ms /    86 tokens (    3.39 ms per token,   294.67 tokens per second)\n",
            "llama_print_timings:        eval time =    1496.46 ms /    15 runs   (   99.76 ms per token,    10.02 tokens per second)\n",
            "llama_print_timings:       total time =    2072.54 ms /   101 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Cigarettes account for the other 95 %, but as far as they are concerned, this report only deals with technical adjustments to the taxation regime.\n",
            "hyp1: The report only deals with technical adjustments to the taxation regime and excludes cigarettes.\n",
            "hyp2: The report only deals with technical adjustments to the taxation regime.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.47 ms /    16 runs   (    0.59 ms per token,  1689.90 tokens per second)\n",
            "llama_print_timings: prompt eval time =     290.07 ms /    80 tokens (    3.63 ms per token,   275.79 tokens per second)\n",
            "llama_print_timings:        eval time =    1863.22 ms /    15 runs   (  124.21 ms per token,     8.05 tokens per second)\n",
            "llama_print_timings:       total time =    2618.25 ms /    95 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: But at that time the agricultural proportion of the budget, accounting for over 70 %, was considerably higher than it is today.\n",
            "hyp1: More than 70 dollars from the budget was devoted to agriculture at that time.\n",
            "hyp2: More than 70 percent of the budget was devoted to agriculture at that time.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.28 ms /    16 runs   (    0.58 ms per token,  1725.07 tokens per second)\n",
            "llama_print_timings: prompt eval time =     312.65 ms /    96 tokens (    3.26 ms per token,   307.05 tokens per second)\n",
            "llama_print_timings:        eval time =    1865.46 ms /    15 runs   (  124.36 ms per token,     8.04 tokens per second)\n",
            "llama_print_timings:       total time =    2707.00 ms /   111 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: In two days' time, we shall have the debate and the decisions of the European Council on Agenda 2000 and the financial perspective.\n",
            "hyp1: The European Council on Agenda 2000 and financial perspective will be debated in twenty two days.\n",
            "hyp2: The European Council on Agenda 2000 and financial perspective will be debated in two days.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.85 ms /    16 runs   (    0.49 ms per token,  2039.00 tokens per second)\n",
            "llama_print_timings: prompt eval time =     304.40 ms /    86 tokens (    3.54 ms per token,   282.52 tokens per second)\n",
            "llama_print_timings:        eval time =    1586.90 ms /    15 runs   (  105.79 ms per token,     9.45 tokens per second)\n",
            "llama_print_timings:       total time =    2182.87 ms /   101 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The Member States are, in other words, being asked to harmonise their economic policies - which is one of the reasons why I voted against this report.\n",
            "hyp1: I voted against the report because Member States are being asked to change their economic policies.\n",
            "hyp2: I voted against the report because Member States are being asked to reconcile their economic policies.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.72 ms /    16 runs   (    0.48 ms per token,  2072.27 tokens per second)\n",
            "llama_print_timings: prompt eval time =     275.36 ms /    80 tokens (    3.44 ms per token,   290.53 tokens per second)\n",
            "llama_print_timings:        eval time =    1566.69 ms /    15 runs   (  104.45 ms per token,     9.57 tokens per second)\n",
            "llama_print_timings:       total time =    2124.55 ms /    95 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: We have also considered this, and we think that these vehicles should only be exempt from Articles 4 and 7 of the proposal.\n",
            "hyp1: We think the vehicles should be exempt articles number 4 and 7 from the proposal.\n",
            "hyp2: We think the vehicles should be exempt from the proposal.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.39 ms /    16 runs   (    0.46 ms per token,  2164.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =     279.88 ms /    96 tokens (    2.92 ms per token,   343.00 tokens per second)\n",
            "llama_print_timings:        eval time =    1579.39 ms /    15 runs   (  105.29 ms per token,     9.50 tokens per second)\n",
            "llama_print_timings:       total time =    2172.66 ms /   111 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: How is rail transport supposed to compete with road transport if large containers cannot be brought to the trains and we are therefore forced to accept the 44 tonne limit?\n",
            "hyp1: If rail transport can't be brought to the trains, how can large containers compete with road transportation?\n",
            "hyp2: If large containers can't be brought to the trains, how can rail transport compete with road transportation?\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.03 ms /    16 runs   (    0.56 ms per token,  1771.68 tokens per second)\n",
            "llama_print_timings: prompt eval time =     301.34 ms /    98 tokens (    3.07 ms per token,   325.22 tokens per second)\n",
            "llama_print_timings:        eval time =    1776.46 ms /    15 runs   (  118.43 ms per token,     8.44 tokens per second)\n",
            "llama_print_timings:       total time =    2614.93 ms /   113 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: For the latter, the initial birth of several operators is now giving way to the reconcentration of the sector in the hands of a single company.\n",
            "hyp1: The establishment of a number of operators is giving way to the reconcentration of the sector in the hands of one company.\n",
            "hyp2: Several operators have given way to the reconcentration of the sector in the hands of one company.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.51 ms /    16 runs   (    0.47 ms per token,  2129.36 tokens per second)\n",
            "llama_print_timings: prompt eval time =     285.28 ms /    68 tokens (    4.20 ms per token,   238.36 tokens per second)\n",
            "llama_print_timings:        eval time =    1512.27 ms /    15 runs   (  100.82 ms per token,     9.92 tokens per second)\n",
            "llama_print_timings:       total time =    2023.44 ms /    83 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: At the same time, and I always thought this was correct, we have quite simply deleted Chapter 8 from the Convention entirely.\n",
            "hyp1: I have completely removed Chapter 8 from the Convention.\n",
            "hyp2: We have completely removed Chapter 8 from the Convention.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.72 ms /    16 runs   (    0.48 ms per token,  2072.81 tokens per second)\n",
            "llama_print_timings: prompt eval time =     296.34 ms /    85 tokens (    3.49 ms per token,   286.83 tokens per second)\n",
            "llama_print_timings:        eval time =    1570.06 ms /    15 runs   (  104.67 ms per token,     9.55 tokens per second)\n",
            "llama_print_timings:       total time =    2140.20 ms /   100 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: It is true that even greater efforts could be made in this direction, as my honourable colleague said, and indeed we are demanding progress here.\n",
            "hyp1: I agree with my colleague and fellow lawyers that more efforts could be made in this direction.\n",
            "hyp2: I agree with my colleague that more efforts could be made in this direction.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.11 ms /    16 runs   (    0.51 ms per token,  1974.09 tokens per second)\n",
            "llama_print_timings: prompt eval time =     280.05 ms /    83 tokens (    3.37 ms per token,   296.37 tokens per second)\n",
            "llama_print_timings:        eval time =    1581.67 ms /    15 runs   (  105.44 ms per token,     9.48 tokens per second)\n",
            "llama_print_timings:       total time =    2129.92 ms /    98 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: For that reason, on 17 December 1998 the Council adopted a comprehensive programme to supply agricultural produce to Russia.\n",
            "hyp1: The Council adopted a programme to supply agricultural produce to Russia in 1917.\n",
            "hyp2: The Council adopted a programme to supply agricultural produce to Russia in 1998.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =      12.41 ms /    16 runs   (    0.78 ms per token,  1289.18 tokens per second)\n",
            "llama_print_timings: prompt eval time =     313.80 ms /    93 tokens (    3.37 ms per token,   296.36 tokens per second)\n",
            "llama_print_timings:        eval time =    1759.77 ms /    15 runs   (  117.32 ms per token,     8.52 tokens per second)\n",
            "llama_print_timings:       total time =    2592.66 ms /   108 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Frontier regions where the inhabitants do not understand their neighbour's language cannot fully experience the single market at local level.\n",
            "hyp1: The single market can't be fully experienced in frontier regions where the inhabitants understand their neighbours' languages.\n",
            "hyp2: The single market can't be fully experienced in frontier regions where the inhabitants don't understand their neighbours' languages.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.71 ms /    16 runs   (    0.48 ms per token,  2076.30 tokens per second)\n",
            "llama_print_timings: prompt eval time =     282.42 ms /    87 tokens (    3.25 ms per token,   308.05 tokens per second)\n",
            "llama_print_timings:        eval time =    1615.91 ms /    15 runs   (  107.73 ms per token,     9.28 tokens per second)\n",
            "llama_print_timings:       total time =    2176.82 ms /   102 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: We welcome the adoption by the Council of Ministers of general guidelines on the issue of the death penalty in June 1998.\n",
            "hyp1: The Council of Ministers adopted general guidelines regarding the death penalty in late 1998.\n",
            "hyp2: The Council of Ministers adopted general guidelines regarding the death penalty in 1998.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.09 ms /    16 runs   (    0.51 ms per token,  1977.75 tokens per second)\n",
            "llama_print_timings: prompt eval time =     295.39 ms /    91 tokens (    3.25 ms per token,   308.07 tokens per second)\n",
            "llama_print_timings:        eval time =    1645.64 ms /    15 runs   (  109.71 ms per token,     9.12 tokens per second)\n",
            "llama_print_timings:       total time =    2233.87 ms /   106 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: In addition, television campaigns designed to dissuade would-be immigrants from making the crossing could be broadcast on television in northern Morocco.\n",
            "hyp1: Television campaigns designed to discourage would-be immigrants from crossing the border could be broadcasted on television.\n",
            "hyp2: Television campaigns designed to persuade would-be immigrants from crossing the border could be broadcasted on television.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.33 ms /    16 runs   (    0.52 ms per token,  1920.54 tokens per second)\n",
            "llama_print_timings: prompt eval time =     294.73 ms /   100 tokens (    2.95 ms per token,   339.30 tokens per second)\n",
            "llama_print_timings:        eval time =    1641.22 ms /    15 runs   (  109.41 ms per token,     9.14 tokens per second)\n",
            "llama_print_timings:       total time =    2263.95 ms /   115 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Thirdly, the Commission agrees with the preoccupation of the rapporteur as far as the famous RAL, reste à liquider , the backlog, is concerned.\n",
            "hyp1: The Commission agrees with the rapporteur's preoccupation with the famous RAL, reste liquider.\n",
            "hyp2: The Commission agrees with the rapporteur's preoccupation with the famous RAL, reste à liquider.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.06 ms /    16 runs   (    0.57 ms per token,  1766.39 tokens per second)\n",
            "llama_print_timings: prompt eval time =     292.69 ms /    76 tokens (    3.85 ms per token,   259.66 tokens per second)\n",
            "llama_print_timings:        eval time =    1768.70 ms /    15 runs   (  117.91 ms per token,     8.48 tokens per second)\n",
            "llama_print_timings:       total time =    2463.27 ms /    91 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: 1.Negotiations between the governments on the reform of agricultural policy are still in progress and will be resumed today.\n",
            "hyp1: The negotiations regarding the reform of agricultural policy have been finalized by the governments.\n",
            "hyp2: The reform of agricultural policy is still being negotiated by the governments.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.35 ms /    16 runs   (    0.46 ms per token,  2178.06 tokens per second)\n",
            "llama_print_timings: prompt eval time =     295.34 ms /    81 tokens (    3.65 ms per token,   274.26 tokens per second)\n",
            "llama_print_timings:        eval time =    1513.81 ms /    15 runs   (  100.92 ms per token,     9.91 tokens per second)\n",
            "llama_print_timings:       total time =    2067.47 ms /    96 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: It also recommends strengthening the initiative-taking and political impetus of the Commission, whose political accountability would also be improved in that way.\n",
            "hyp1: The Commission's political accountability would not be improved in this way.\n",
            "hyp2: The Commission's political accountability would be improved in this way.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.84 ms /    16 runs   (    0.49 ms per token,  2041.60 tokens per second)\n",
            "llama_print_timings: prompt eval time =     287.69 ms /    96 tokens (    3.00 ms per token,   333.69 tokens per second)\n",
            "llama_print_timings:        eval time =    1530.29 ms /    15 runs   (  102.02 ms per token,     9.80 tokens per second)\n",
            "llama_print_timings:       total time =    2130.10 ms /   111 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Another declaration to mention in this respect is Declaration No 39 annexed to the Treaty of Amsterdam on the quality of drafting.\n",
            "hyp1: Declaration No 39 was annexed to the Treaty of Amsterdam to discuss drafting quality.\n",
            "hyp2: Declaration No 39 wasn't annexed to the Treaty of Amsterdam to discuss drafting quality.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.72 ms /    16 runs   (    0.48 ms per token,  2072.81 tokens per second)\n",
            "llama_print_timings: prompt eval time =     282.93 ms /    87 tokens (    3.25 ms per token,   307.50 tokens per second)\n",
            "llama_print_timings:        eval time =    1517.97 ms /    15 runs   (  101.20 ms per token,     9.88 tokens per second)\n",
            "llama_print_timings:       total time =    2089.92 ms /   102 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: It is extremely important that the forthcoming European Council meeting in Berlin should cut the Gordian knot on Agenda 2000.\n",
            "hyp1: Agenda 2000 should be fully handled during the European Council meeting in Berlin.\n",
            "hyp2: Agenda 2000 should be cut at the European Council meeting in Berlin.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.19 ms /    16 runs   (    0.57 ms per token,  1741.97 tokens per second)\n",
            "llama_print_timings: prompt eval time =     329.56 ms /   110 tokens (    3.00 ms per token,   333.78 tokens per second)\n",
            "llama_print_timings:        eval time =    1565.09 ms /    15 runs   (  104.34 ms per token,     9.58 tokens per second)\n",
            "llama_print_timings:       total time =    2450.51 ms /   125 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: As a result of this investment, which I fully support, the suburban rail network will be increased in capacity in terms of over 60 %.\n",
            "hyp1: The investment will increase the suburban rail network's capacity by more than 60 percent.\n",
            "hyp2: The investment will increase the suburban rail network's capacity by more than 60 percentm which is an investment I support given the latest numbers provided by the Suburban Rail Network Committee.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.55 ms /    16 runs   (    0.47 ms per token,  2120.33 tokens per second)\n",
            "llama_print_timings: prompt eval time =     301.04 ms /    97 tokens (    3.10 ms per token,   322.22 tokens per second)\n",
            "llama_print_timings:        eval time =    1495.62 ms /    15 runs   (   99.71 ms per token,    10.03 tokens per second)\n",
            "llama_print_timings:       total time =    2106.04 ms /   112 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: That is precisely why we cannot give way to the unjustified US demand, even though and in fact just because they are threatening lunatic sanctions.\n",
            "hyp1: That's why we can't give in to the US demand even though they're threatening sanctions.\n",
            "hyp2: That's why they can't give in to the US demand even though we're threatening sanctions.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.00 ms /    16 runs   (    0.50 ms per token,  2000.00 tokens per second)\n",
            "llama_print_timings: prompt eval time =     269.60 ms /    76 tokens (    3.55 ms per token,   281.90 tokens per second)\n",
            "llama_print_timings:        eval time =    1556.51 ms /    15 runs   (  103.77 ms per token,     9.64 tokens per second)\n",
            "llama_print_timings:       total time =    2072.04 ms /    91 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: All aspects of the question need to be brought together; perhaps a thorough investigation of the whole practice of aquaculture is now called for.\n",
            "hyp1: A thorough investigation of all aspects of the question is needed.\n",
            "hyp2: A thorough investigation of all aspects of aquaculture may we warranted. \n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.09 ms /    16 runs   (    0.51 ms per token,  1978.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =     279.69 ms /    82 tokens (    3.41 ms per token,   293.18 tokens per second)\n",
            "llama_print_timings:        eval time =    1649.05 ms /    15 runs   (  109.94 ms per token,     9.10 tokens per second)\n",
            "llama_print_timings:       total time =    2215.15 ms /    97 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Mr President, the dossier on European cities of culture which has been open since October 1997 will be closed, I hope, tomorrow.\n",
            "hyp1: I hope that the European cities of culture will be closed tomorrow.\n",
            "hyp2: I hope that the dossier on European cities of culture will be closed tomorrow.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.82 ms /    15 runs   (    0.52 ms per token,  1917.42 tokens per second)\n",
            "llama_print_timings: prompt eval time =     311.18 ms /    84 tokens (    3.70 ms per token,   269.94 tokens per second)\n",
            "llama_print_timings:        eval time =    1490.21 ms /    14 runs   (  106.44 ms per token,     9.39 tokens per second)\n",
            "llama_print_timings:       total time =    2211.91 ms /    98 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Premiums are to be increased by 10 %, with a corresponding reduction in special subsidies allocated through producer organisations.\n",
            "hyp1: Premiums will decrease by 10 % and special subsidies will be increased.\n",
            "hyp2: Premiums will increase by 10 % and special subsidies will be reduced.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.52 ms /    16 runs   (    0.47 ms per token,  2127.09 tokens per second)\n",
            "llama_print_timings: prompt eval time =     282.71 ms /    84 tokens (    3.37 ms per token,   297.12 tokens per second)\n",
            "llama_print_timings:        eval time =    1517.92 ms /    15 runs   (  101.19 ms per token,     9.88 tokens per second)\n",
            "llama_print_timings:       total time =    2070.66 ms /    99 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: At the end of 1997, the Commission put forward a controversial proposal for the harmonisation of copyright law in the Community.\n",
            "hyp1: The proposal for copyright harmonization was put forward at the end of 1997.\n",
            "hyp2: The harmonisation proposal was put forward at the end of 1997.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.08 ms /    16 runs   (    0.50 ms per token,  1980.93 tokens per second)\n",
            "llama_print_timings: prompt eval time =     285.68 ms /    99 tokens (    2.89 ms per token,   346.55 tokens per second)\n",
            "llama_print_timings:        eval time =    1524.93 ms /    15 runs   (  101.66 ms per token,     9.84 tokens per second)\n",
            "llama_print_timings:       total time =    2122.28 ms /   114 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: I had proposed to you that the Euro-11 be recognised in the Treaty, but my understanding was that Mr Spiers did not agree.\n",
            "hyp1: I wanted the Euro-11 to be recognised in the Treaty but Mr. Spiers didn't agree with me.\n",
            "hyp2: I wanted the Euro-11 to be recognised in the Treaty and Mr. Spiers agreed with me.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.23 ms /    16 runs   (    0.51 ms per token,  1944.34 tokens per second)\n",
            "llama_print_timings: prompt eval time =     272.11 ms /    77 tokens (    3.53 ms per token,   282.97 tokens per second)\n",
            "llama_print_timings:        eval time =    1671.21 ms /    15 runs   (  111.41 ms per token,     8.98 tokens per second)\n",
            "llama_print_timings:       total time =    2209.40 ms /    92 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: As a result, we would like to be convinced that these cooperation agreements are not going to contribute to such extremely dangerous research.\n",
            "hyp1: She want to be sure that these agreements won't contribute to dangerous research.\n",
            "hyp2: We want to be sure that these agreements won't contribute to dangerous research.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.63 ms /    16 runs   (    0.48 ms per token,  2095.89 tokens per second)\n",
            "llama_print_timings: prompt eval time =     307.53 ms /    84 tokens (    3.66 ms per token,   273.15 tokens per second)\n",
            "llama_print_timings:        eval time =    1531.61 ms /    15 runs   (  102.11 ms per token,     9.79 tokens per second)\n",
            "llama_print_timings:       total time =    2117.22 ms /    99 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: An important factor here is that the pay and other working conditions of those employed in navigation should be in line with those of other occupations.\n",
            "hyp1: The pay and working conditions of those employed in navigation should be better than those of other jobs.\n",
            "hyp2: The pay and working conditions of those employed in navigation should match those of other jobs.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.89 ms /    16 runs   (    0.49 ms per token,  2027.88 tokens per second)\n",
            "llama_print_timings: prompt eval time =     292.70 ms /   106 tokens (    2.76 ms per token,   362.15 tokens per second)\n",
            "llama_print_timings:        eval time =    1588.10 ms /    15 runs   (  105.87 ms per token,     9.45 tokens per second)\n",
            "llama_print_timings:       total time =    2206.91 ms /   121 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Indeed, without the chairmanship skills of the Deputy Prime Minister, John Prescott, the Conference would have produced more hot air than it prevented!\n",
            "hyp1: The Conference wouldn't have produced as much hot air if it weren't for the chairmanship skills of the deputy prime minister.\n",
            "hyp2: The Conference would have made more hot air than it prevented if it weren't for the chairmanship skills of the deputy prime minister!\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.71 ms /    16 runs   (    0.48 ms per token,  2074.15 tokens per second)\n",
            "llama_print_timings: prompt eval time =     275.48 ms /    87 tokens (    3.17 ms per token,   315.81 tokens per second)\n",
            "llama_print_timings:        eval time =    1655.50 ms /    15 runs   (  110.37 ms per token,     9.06 tokens per second)\n",
            "llama_print_timings:       total time =    2207.42 ms /   102 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: I want to say very clearly that the Commission is no longer be prepared to accept ever-increasing tasks without receiving the means to execute them.\n",
            "hyp1: The Commission is not prepared to accept ever increasing tasks  while getting the means to execute them.\n",
            "hyp2: The Commission is not prepared to accept ever increasing tasks without getting the means to execute them.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.47 ms /    16 runs   (    0.59 ms per token,  1689.55 tokens per second)\n",
            "llama_print_timings: prompt eval time =     292.37 ms /   102 tokens (    2.87 ms per token,   348.87 tokens per second)\n",
            "llama_print_timings:        eval time =    1798.50 ms /    15 runs   (  119.90 ms per token,     8.34 tokens per second)\n",
            "llama_print_timings:       total time =    2432.27 ms /   117 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The enlargement of the EU compels us to pay greater attention to these questions, a point which is also emphasised in the committee's proposals.\n",
            "hyp1: The committee's proposals emphasize that the EU's enlargement makes us pay more attention to the questions.\n",
            "hyp2: The committee's proposals emphasize that the EEAS's enlargement makes us pay more attention to the questions.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.61 ms /    16 runs   (    0.54 ms per token,  1859.38 tokens per second)\n",
            "llama_print_timings: prompt eval time =     306.61 ms /    84 tokens (    3.65 ms per token,   273.96 tokens per second)\n",
            "llama_print_timings:        eval time =    1510.24 ms /    15 runs   (  100.68 ms per token,     9.93 tokens per second)\n",
            "llama_print_timings:       total time =    2083.39 ms /    99 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: I am referring to the current vote but in relation to a vote that will take place later, namely the vote on Amendment No 98.\n",
            "hyp1: The vote on Amendment No 98 will take place before the current vote.\n",
            "hyp2: The vote on Amendment No 98 will take place after the current vote.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.61 ms /    16 runs   (    0.48 ms per token,  2103.05 tokens per second)\n",
            "llama_print_timings: prompt eval time =     283.45 ms /    86 tokens (    3.30 ms per token,   303.40 tokens per second)\n",
            "llama_print_timings:        eval time =    1470.38 ms /    15 runs   (   98.03 ms per token,    10.20 tokens per second)\n",
            "llama_print_timings:       total time =    2022.94 ms /   101 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: When I became a Member of the European Parliament in 1994, I was struck by the lack of knowledge and the total absence of understanding of the overseas territories.\n",
            "hyp1: I was struck by the abundance of knowledge when I joined the European Parliament.\n",
            "hyp2: I was struck by the lack of knowledge when I joined the European Parliament.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.76 ms /    16 runs   (    0.49 ms per token,  2061.06 tokens per second)\n",
            "llama_print_timings: prompt eval time =     278.75 ms /    84 tokens (    3.32 ms per token,   301.34 tokens per second)\n",
            "llama_print_timings:        eval time =    1505.52 ms /    15 runs   (  100.37 ms per token,     9.96 tokens per second)\n",
            "llama_print_timings:       total time =    2051.10 ms /    99 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Mr President, let me express my thanks for the preceding remarks and extend a special welcome to my honourable colleague the Transport Commissioner.\n",
            "hyp1: Mr President, I would like to extend a warm welcome to the Transport Commissioner.\n",
            "hyp2: Madam President, I would like to extend a warm welcome to the Transport Commissioner.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.40 ms /    16 runs   (    0.59 ms per token,  1701.40 tokens per second)\n",
            "llama_print_timings: prompt eval time =     297.01 ms /   108 tokens (    2.75 ms per token,   363.63 tokens per second)\n",
            "llama_print_timings:        eval time =    1858.07 ms /    15 runs   (  123.87 ms per token,     8.07 tokens per second)\n",
            "llama_print_timings:       total time =    2510.38 ms /   123 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: We can replace 16 out of 626 Members of the European Parliament, but we can never again as voters decide to bring in a new law.\n",
            "hyp1: As voters we cannot decide on a new law, but we can replace 16 out of 626 Members of the European Parliament.\n",
            "hyp2: As voters decide on a new law, we can replace 16 out of 626 Members of the European Parliament.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.44 ms /    16 runs   (    0.46 ms per token,  2151.41 tokens per second)\n",
            "llama_print_timings: prompt eval time =     292.96 ms /    88 tokens (    3.33 ms per token,   300.38 tokens per second)\n",
            "llama_print_timings:        eval time =    1681.97 ms /    15 runs   (  112.13 ms per token,     8.92 tokens per second)\n",
            "llama_print_timings:       total time =    2245.92 ms /   103 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The Council is making a mistake by leaving it entirely up to the United States to formulate proposals for a solution to the Middle East problem.\n",
            "hyp1: The United States should not be the only ones to come up with a solution to the Middle East problem\n",
            "hyp2: The United States needs to come up with a solution to the Middle East problem, not the Council.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        }
      ],
      "source": [
        "#replies_list = ['YES','NO']\n",
        "counter = 0\n",
        "\n",
        "# List to store the results.\n",
        "prediction_en = []\n",
        "\n",
        "for i in range(0,len(test_ds['test_detection_english'])):\n",
        "  src = test_ds['test_detection_english'][i]['source']\n",
        "  hyp1 = test_ds['test_detection_english'][i]['hyp1']\n",
        "  hyp2 = test_ds['test_detection_english'][i]['hyp2']\n",
        "  id = test_ds['test_detection_english'][i]['id']\n",
        "\n",
        "  current_sample = prompt_context+\"src: \"+src+\"\\nhyp1: \"+hyp1 + '\\nhyp2: '+hyp2+'\\nlabel: [/INST] \\n '\n",
        "  prompt = few_shot_samples_en+current_sample\n",
        "\n",
        "  #print(prompt)\n",
        "  #print(current_sample)\n",
        "\n",
        "  response = lcpp_llm(\n",
        "        prompt=prompt,\n",
        "        temperature= 0.2,\n",
        "        logprobs=1,\n",
        "        #max_tokens =1\n",
        "      )\n",
        "\n",
        "  #print(response)\n",
        "\n",
        "  answer = str(response[\"choices\"][0][\"text\"]).strip()\n",
        "  #print(answer)\n",
        "  answer = answer[:4]\n",
        "  #answer = answer.split()[0]\n",
        "  # Sometime output contains a '.' remove it!\n",
        "  #answer = answer.replace('.','')\n",
        "\n",
        "  # If the predicted word is not in emotion list just replace with neutral.\n",
        "  #if answer not in replies_list:\n",
        "\n",
        "  #current_sample += answer + \" \\n \"\n",
        "\n",
        "  print(\"GENERATED: \"+ current_sample+'\\n'+answer)\n",
        "\n",
        "  current_element = {\n",
        "        \"id\": id,\n",
        "        \"label\": answer,\n",
        "        \"explanation\": \"\"\n",
        "    }\n",
        "  prediction_en.append(current_element)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(prediction_en)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pqj9hh4JtGgm",
        "outputId": "11468b32-9582-4878-90b9-8298d42f4109"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'id': 0, 'label': 'hyp1', 'explanation': ''}, {'id': 1, 'label': 'hyp1', 'explanation': ''}, {'id': 2, 'label': 'hyp1', 'explanation': ''}, {'id': 3, 'label': 'hyp1', 'explanation': ''}, {'id': 4, 'label': 'hyp2', 'explanation': ''}, {'id': 5, 'label': 'hyp1', 'explanation': ''}, {'id': 6, 'label': 'hyp1', 'explanation': ''}, {'id': 7, 'label': 'hyp1', 'explanation': ''}, {'id': 8, 'label': 'hyp1', 'explanation': ''}, {'id': 9, 'label': 'hyp2', 'explanation': ''}, {'id': 10, 'label': 'hyp1', 'explanation': ''}, {'id': 11, 'label': 'hyp1', 'explanation': ''}, {'id': 12, 'label': 'hyp1', 'explanation': ''}, {'id': 13, 'label': 'hyp1', 'explanation': ''}, {'id': 14, 'label': 'hyp2', 'explanation': ''}, {'id': 15, 'label': 'hyp1', 'explanation': ''}, {'id': 16, 'label': 'hyp1', 'explanation': ''}, {'id': 17, 'label': 'hyp1', 'explanation': ''}, {'id': 18, 'label': 'hyp1', 'explanation': ''}, {'id': 19, 'label': 'hyp2', 'explanation': ''}, {'id': 20, 'label': 'hyp1', 'explanation': ''}, {'id': 21, 'label': 'hyp1', 'explanation': ''}, {'id': 22, 'label': 'hyp1', 'explanation': ''}, {'id': 23, 'label': 'hyp1', 'explanation': ''}, {'id': 24, 'label': 'hyp1', 'explanation': ''}, {'id': 25, 'label': 'hyp1', 'explanation': ''}, {'id': 26, 'label': 'hyp1', 'explanation': ''}, {'id': 27, 'label': 'hyp2', 'explanation': ''}, {'id': 28, 'label': 'hyp2', 'explanation': ''}, {'id': 29, 'label': 'hyp2', 'explanation': ''}, {'id': 30, 'label': 'hyp2', 'explanation': ''}, {'id': 31, 'label': 'hyp1', 'explanation': ''}, {'id': 32, 'label': 'hyp1', 'explanation': ''}, {'id': 33, 'label': 'hyp2', 'explanation': ''}, {'id': 34, 'label': 'hyp1', 'explanation': ''}, {'id': 35, 'label': 'hyp2', 'explanation': ''}, {'id': 36, 'label': 'hyp2', 'explanation': ''}, {'id': 37, 'label': 'hyp2', 'explanation': ''}, {'id': 38, 'label': 'hyp1', 'explanation': ''}, {'id': 39, 'label': 'hyp1', 'explanation': ''}, {'id': 40, 'label': 'hyp2', 'explanation': ''}, {'id': 41, 'label': 'hyp2', 'explanation': ''}, {'id': 42, 'label': 'hyp2', 'explanation': ''}, {'id': 43, 'label': 'hyp2', 'explanation': ''}, {'id': 44, 'label': 'hyp1', 'explanation': ''}, {'id': 45, 'label': 'hyp2', 'explanation': ''}, {'id': 46, 'label': 'hyp1', 'explanation': ''}, {'id': 47, 'label': 'hyp1', 'explanation': ''}, {'id': 48, 'label': 'hyp1', 'explanation': ''}, {'id': 49, 'label': 'hyp2', 'explanation': ''}, {'id': 50, 'label': 'hyp1', 'explanation': ''}, {'id': 51, 'label': 'hyp2', 'explanation': ''}, {'id': 52, 'label': 'hyp1', 'explanation': ''}, {'id': 53, 'label': 'hyp2', 'explanation': ''}, {'id': 54, 'label': 'hyp2', 'explanation': ''}, {'id': 55, 'label': 'hyp1', 'explanation': ''}, {'id': 56, 'label': 'hyp1', 'explanation': ''}, {'id': 57, 'label': 'hyp1', 'explanation': ''}, {'id': 58, 'label': 'hyp1', 'explanation': ''}, {'id': 59, 'label': 'hyp1', 'explanation': ''}, {'id': 60, 'label': 'hyp1', 'explanation': ''}, {'id': 61, 'label': 'hyp1', 'explanation': ''}, {'id': 62, 'label': 'hyp2', 'explanation': ''}, {'id': 63, 'label': 'hyp1', 'explanation': ''}, {'id': 64, 'label': 'hyp1', 'explanation': ''}, {'id': 65, 'label': 'hyp1', 'explanation': ''}, {'id': 66, 'label': 'hyp1', 'explanation': ''}, {'id': 67, 'label': 'hyp2', 'explanation': ''}, {'id': 68, 'label': 'hyp1', 'explanation': ''}, {'id': 69, 'label': 'hyp1', 'explanation': ''}, {'id': 70, 'label': 'hyp1', 'explanation': ''}, {'id': 71, 'label': 'hyp2', 'explanation': ''}, {'id': 72, 'label': 'hyp1', 'explanation': ''}, {'id': 73, 'label': 'hyp1', 'explanation': ''}, {'id': 74, 'label': 'hyp2', 'explanation': ''}, {'id': 75, 'label': 'hyp2', 'explanation': ''}, {'id': 76, 'label': 'hyp1', 'explanation': ''}, {'id': 77, 'label': 'hyp2', 'explanation': ''}, {'id': 78, 'label': 'hyp1', 'explanation': ''}, {'id': 79, 'label': 'hyp1', 'explanation': ''}, {'id': 80, 'label': 'hyp1', 'explanation': ''}, {'id': 81, 'label': 'hyp1', 'explanation': ''}, {'id': 82, 'label': 'hyp2', 'explanation': ''}, {'id': 83, 'label': 'hyp1', 'explanation': ''}, {'id': 84, 'label': 'hyp1', 'explanation': ''}, {'id': 85, 'label': 'hyp1', 'explanation': ''}, {'id': 86, 'label': 'hyp1', 'explanation': ''}, {'id': 87, 'label': 'hyp1', 'explanation': ''}, {'id': 88, 'label': 'hyp2', 'explanation': ''}, {'id': 89, 'label': 'hyp1', 'explanation': ''}, {'id': 90, 'label': 'hyp1', 'explanation': ''}, {'id': 91, 'label': 'hyp1', 'explanation': ''}, {'id': 92, 'label': 'hyp1', 'explanation': ''}, {'id': 93, 'label': 'hyp1', 'explanation': ''}, {'id': 94, 'label': 'hyp1', 'explanation': ''}, {'id': 95, 'label': 'hyp1', 'explanation': ''}, {'id': 96, 'label': 'hyp1', 'explanation': ''}, {'id': 97, 'label': 'hyp1', 'explanation': ''}, {'id': 98, 'label': 'hyp1', 'explanation': ''}, {'id': 99, 'label': 'hyp1', 'explanation': ''}, {'id': 100, 'label': 'hyp2', 'explanation': ''}, {'id': 101, 'label': 'hyp1', 'explanation': ''}, {'id': 102, 'label': 'hyp2', 'explanation': ''}, {'id': 103, 'label': 'hyp2', 'explanation': ''}, {'id': 104, 'label': 'hyp2', 'explanation': ''}, {'id': 105, 'label': 'hyp1', 'explanation': ''}, {'id': 106, 'label': 'hyp1', 'explanation': ''}, {'id': 107, 'label': 'hyp1', 'explanation': ''}, {'id': 108, 'label': 'hyp1', 'explanation': ''}, {'id': 109, 'label': 'hyp1', 'explanation': ''}, {'id': 110, 'label': 'hyp1', 'explanation': ''}, {'id': 111, 'label': 'hyp1', 'explanation': ''}, {'id': 112, 'label': 'hyp1', 'explanation': ''}, {'id': 113, 'label': 'hyp1', 'explanation': ''}, {'id': 114, 'label': 'hyp1', 'explanation': ''}, {'id': 115, 'label': 'hyp1', 'explanation': ''}, {'id': 116, 'label': 'hyp1', 'explanation': ''}, {'id': 117, 'label': 'hyp2', 'explanation': ''}, {'id': 118, 'label': 'hyp2', 'explanation': ''}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List to store the results.\n",
        "prediction_sv = []\n",
        "\n",
        "for i in range(0,len(test_ds['test_detection_swedish'])):\n",
        "  src = GoogleTranslator(source='sv', target='en').translate(test_ds['test_detection_swedish'][i]['source'])\n",
        "  hyp1 = GoogleTranslator(source='sv', target='en').translate(test_ds['test_detection_swedish'][i]['hyp1'])\n",
        "  hyp2 = GoogleTranslator(source='sv', target='en').translate(test_ds['test_detection_swedish'][i]['hyp2'])\n",
        "  id = test_ds['test_detection_swedish'][i]['id']\n",
        "\n",
        "  current_sample = prompt_context+\"src: \"+src+\"\\nhyp1: \"+hyp1 + '\\nhyp2: '+hyp2+'\\nlabel: [/INST] \\n '\n",
        "  prompt = few_shot_samples_sv+current_sample\n",
        "\n",
        "  #print(prompt)\n",
        "  #print(current_sample)\n",
        "\n",
        "  response = lcpp_llm(\n",
        "        prompt=prompt,\n",
        "        temperature= 0.2,\n",
        "        logprobs=1,\n",
        "        #max_tokens =1\n",
        "      )\n",
        "\n",
        "  #print(response)\n",
        "\n",
        "  answer = str(response[\"choices\"][0][\"text\"]).strip()\n",
        "  #print(answer)\n",
        "  answer = answer[:4]\n",
        "  #answer = answer.split()[0]\n",
        "  # Sometime output contains a '.' remove it!\n",
        "  #answer = answer.replace('.','')\n",
        "\n",
        "  # If the predicted word is not in emotion list just replace with neutral.\n",
        "  #if answer not in replies_list:\n",
        "\n",
        "  #current_sample += answer + \" \\n \"\n",
        "\n",
        "  print(\"GENERATED: \"+ current_sample+'\\n'+answer)\n",
        "\n",
        "  current_element = {\n",
        "        \"id\": id,\n",
        "        \"label\": answer,\n",
        "        \"explanation\": \"\"\n",
        "    }\n",
        "  prediction_sv.append(current_element)\n"
      ],
      "metadata": {
        "id": "4SfpJbGEa0R_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "425db1b3-8292-4078-d2a6-3b9b31d3332a"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.08 ms /    16 runs   (    0.50 ms per token,  1980.69 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4018.19 ms /  2593 tokens (    1.55 ms per token,   645.32 tokens per second)\n",
            "llama_print_timings:        eval time =    1519.75 ms /    15 runs   (  101.32 ms per token,     9.87 tokens per second)\n",
            "llama_print_timings:       total time =   13216.20 ms /  2608 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Women will face higher car insurance premiums.\n",
            "hyp1: This means women can expect to pay higher prices for their vehicle supplement insurance.\n",
            "hyp2: Women will receive higher car insurance premiums.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.67 ms /    16 runs   (    0.48 ms per token,  2086.59 tokens per second)\n",
            "llama_print_timings: prompt eval time =     323.04 ms /   104 tokens (    3.11 ms per token,   321.94 tokens per second)\n",
            "llama_print_timings:        eval time =    1551.31 ms /    15 runs   (  103.42 ms per token,     9.67 tokens per second)\n",
            "llama_print_timings:       total time =    2173.91 ms /   119 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Operating income was $1.45 billion, up from last year's earnings of $1.38 billion.\n",
            "hyp1: Revenue from the operation was $1.45 billion, down from the previous year's result of $1.38 billion.\n",
            "hyp2: Revenue from the operation was $1.45 billion, up from the previous year's result of $1.38 billion.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.93 ms /    16 runs   (    0.50 ms per token,  2017.15 tokens per second)\n",
            "llama_print_timings: prompt eval time =     280.02 ms /    60 tokens (    4.67 ms per token,   214.27 tokens per second)\n",
            "llama_print_timings:        eval time =    1587.35 ms /    15 runs   (  105.82 ms per token,     9.45 tokens per second)\n",
            "llama_print_timings:       total time =    2056.53 ms /    75 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Mandela back in hospital in 'serious but stable' condition.\n",
            "hyp1: Mandela does not return to hospital in serious, but stable, health.\n",
            "hyp2: Mandela returns to hospital in serious but stable condition.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.46 ms /    16 runs   (    0.53 ms per token,  1891.48 tokens per second)\n",
            "llama_print_timings: prompt eval time =     327.76 ms /   114 tokens (    2.88 ms per token,   347.81 tokens per second)\n",
            "llama_print_timings:        eval time =    1552.58 ms /    15 runs   (  103.51 ms per token,     9.66 tokens per second)\n",
            "llama_print_timings:       total time =    2235.44 ms /   129 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Egypt freezes Muslim Brotherhood assets.\n",
            "hyp1: The Egyptian government has frozen assets of the Muslim Brotherhood, an Islamist group banned in Egypt. The move comes after the military overthrew President Muhammad Mursi last year, accusing the group of plotting a coup. The government claims that the asset freeze is a way to prevent the Brotherhood from fleeing abroad and trying to overthrow the government again.\n",
            "hyp2: Egypt freezes the assets of the Muslim Brotherhood.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.01 ms /    16 runs   (    0.56 ms per token,  1776.40 tokens per second)\n",
            "llama_print_timings: prompt eval time =     401.02 ms /   156 tokens (    2.57 ms per token,   389.01 tokens per second)\n",
            "llama_print_timings:        eval time =    1532.42 ms /    15 runs   (  102.16 ms per token,     9.79 tokens per second)\n",
            "llama_print_timings:       total time =    2393.48 ms /   171 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The pill, which they call the \"polypill,\" would contain aspirin, a cholesterol-lowering drug, three blood pressure-lowering drugs at half the standard dose, and folic acid.\n",
            "hyp1: The medication commonly known as the \"poly pill\" is said to contain aspirin, a cholesterol-lowering medication, three blood pressure-lowering medications at half the standard dose, and folic acid.\n",
            "hyp2: The medication commonly known as the \"poly pill\" is said to contain aspirin, a cholesterol-lowering medication, 2 blood pressure-lowering medications at half the standard dose, and folic acid.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.69 ms /    16 runs   (    0.61 ms per token,  1650.51 tokens per second)\n",
            "llama_print_timings: prompt eval time =     297.78 ms /    67 tokens (    4.44 ms per token,   225.00 tokens per second)\n",
            "llama_print_timings:        eval time =    1790.50 ms /    15 runs   (  119.37 ms per token,     8.38 tokens per second)\n",
            "llama_print_timings:       total time =    2459.81 ms /    82 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Gunman kills 6 in shooting at Wisconsin Sikh temple.\n",
            "hyp1: A gunman kills six people in a shooting at a Sikh temple in Wisconsin.\n",
            "hyp2: Two gunmen kill four people in a shooting at a Sikh temple in Wisconsin.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.92 ms /    16 runs   (    0.50 ms per token,  2019.18 tokens per second)\n",
            "llama_print_timings: prompt eval time =     300.47 ms /    71 tokens (    4.23 ms per token,   236.30 tokens per second)\n",
            "llama_print_timings:        eval time =    1548.87 ms /    15 runs   (  103.26 ms per token,     9.68 tokens per second)\n",
            "llama_print_timings:       total time =    2084.31 ms /    86 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Man who set himself on fire on National Mall dies of injuries.\n",
            "hyp1: The person who set herself on fire on the National Mall died as a result of her injuries.\n",
            "hyp2: The person who set himself on fire on the National Mall died as a result of his injuries.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.77 ms /    16 runs   (    0.49 ms per token,  2060.00 tokens per second)\n",
            "llama_print_timings: prompt eval time =     298.16 ms /    65 tokens (    4.59 ms per token,   218.00 tokens per second)\n",
            "llama_print_timings:        eval time =    1513.75 ms /    15 runs   (  100.92 ms per token,     9.91 tokens per second)\n",
            "llama_print_timings:       total time =    2010.44 ms /    80 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Romney announces Paul Ryan as vice presidential running mate.\n",
            "hyp1: Romney announces that Paul Ryan is his chosen candidate for the post of Vice President.\n",
            "hyp2: Romney announces that Paul Ryan is her chosen candidate for vice president.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.29 ms /    16 runs   (    0.52 ms per token,  1930.73 tokens per second)\n",
            "llama_print_timings: prompt eval time =     312.44 ms /    89 tokens (    3.51 ms per token,   284.85 tokens per second)\n",
            "llama_print_timings:        eval time =    1579.07 ms /    15 runs   (  105.27 ms per token,     9.50 tokens per second)\n",
            "llama_print_timings:       total time =    2155.77 ms /   104 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Doctors say one or both boys may die, and some brain damage is possible if they survive.\n",
            "hyp1: The doctor states that one or both boys may die, and there is a risk of brain damage if they survive.\n",
            "hyp2: The doctor states that one or both boys are in danger of death, and there is a risk of brain damage if they survive.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.80 ms /    15 runs   (    0.52 ms per token,  1923.82 tokens per second)\n",
            "llama_print_timings: prompt eval time =     292.46 ms /    66 tokens (    4.43 ms per token,   225.67 tokens per second)\n",
            "llama_print_timings:        eval time =    1458.03 ms /    14 runs   (  104.15 ms per token,     9.60 tokens per second)\n",
            "llama_print_timings:       total time =    1965.58 ms /    80 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: North Korea says a US citizen should be put on trial.\n",
            "hyp1: According to North Korea, a citizen from the United States will be put on trial.\n",
            "hyp2: According to South Korea, a citizen from the United States will be put on trial.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.09 ms /    16 runs   (    0.57 ms per token,  1759.60 tokens per second)\n",
            "llama_print_timings: prompt eval time =     310.55 ms /   102 tokens (    3.04 ms per token,   328.45 tokens per second)\n",
            "llama_print_timings:        eval time =    1677.79 ms /    15 runs   (  111.85 ms per token,     8.94 tokens per second)\n",
            "llama_print_timings:       total time =    2286.97 ms /   117 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Dozens killed in airstrike on bakery in central Syria.\n",
            "hyp1: An airstrike on a bakery in central Syria killed dozens of people.\n",
            "hyp2: At least 20 people were killed and dozens injured when a Syrian warplane carried out an airstrike on a bakery in the city of Idlib, according to the opposition Syrian Observatory for Human Rights (SOHR).\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.75 ms /    16 runs   (    0.48 ms per token,  2063.45 tokens per second)\n",
            "llama_print_timings: prompt eval time =     292.70 ms /    76 tokens (    3.85 ms per token,   259.65 tokens per second)\n",
            "llama_print_timings:        eval time =    1672.03 ms /    15 runs   (  111.47 ms per token,     8.97 tokens per second)\n",
            "llama_print_timings:       total time =    2201.56 ms /    91 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: China's new premier rejects US hacking claims.\n",
            "hyp1: China's newly appointed premier dismisses US claims that China is responsible for cyber breaches.\n",
            "hyp2: The newly appointed Prime Minister of the United States dismisses claims by the United States that China is responsible for cyber breaches.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.98 ms /    16 runs   (    0.50 ms per token,  2004.01 tokens per second)\n",
            "llama_print_timings: prompt eval time =     287.67 ms /    70 tokens (    4.11 ms per token,   243.34 tokens per second)\n",
            "llama_print_timings:        eval time =    1543.67 ms /    15 runs   (  102.91 ms per token,     9.72 tokens per second)\n",
            "llama_print_timings:       total time =    2042.30 ms /    85 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: US top diplomat Kerry's wife rushed to hospital.\n",
            "hyp1: America's top diplomat Kerry has a wife who was rushed to hospital.\n",
            "hyp2: The top diplomat in the United States, Kerry's wife, rushed to the hospital.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.15 ms /    16 runs   (    0.51 ms per token,  1962.71 tokens per second)\n",
            "llama_print_timings: prompt eval time =     280.07 ms /    64 tokens (    4.38 ms per token,   228.51 tokens per second)\n",
            "llama_print_timings:        eval time =    1642.80 ms /    15 runs   (  109.52 ms per token,     9.13 tokens per second)\n",
            "llama_print_timings:       total time =    2134.19 ms /    79 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Andy Murray deserves the title of nobility, says David Cameron.\n",
            "hyp1: David Cameron believes Andy Murray is not worthy of a peerage, he says.\n",
            "hyp2: David Cameron thinks Andy Murray is worthy of a peerage, he says.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.91 ms /    16 runs   (    0.49 ms per token,  2022.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =     285.04 ms /    76 tokens (    3.75 ms per token,   266.63 tokens per second)\n",
            "llama_print_timings:        eval time =    1528.52 ms /    15 runs   (  101.90 ms per token,     9.81 tokens per second)\n",
            "llama_print_timings:       total time =    2053.30 ms /    91 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Dozens killed in airstrike on bakery in central Syria.\n",
            "hyp1: Dessertal people were killed in an airstrike on a bakery in the center of Syria.\n",
            "hyp2: Several people were killed in an airstrike on a bakery in central Syria.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.01 ms /    16 runs   (    0.56 ms per token,  1775.21 tokens per second)\n",
            "llama_print_timings: prompt eval time =     314.97 ms /   100 tokens (    3.15 ms per token,   317.49 tokens per second)\n",
            "llama_print_timings:        eval time =    1615.28 ms /    15 runs   (  107.69 ms per token,     9.29 tokens per second)\n",
            "llama_print_timings:       total time =    2225.37 ms /   115 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: US top diplomat Kerry's wife rushed to hospital.\n",
            "hyp1: John Kerry's wife, Teresa Heinz Kerry, was taken to hospital in New York on Saturday after suffering a heart attack. She underwent surgery and is in stable condition, according to a State Department statement.\n",
            "hyp2: The wife of the US's top diplomat, Kerry, was rushed to the hospital.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.32 ms /    16 runs   (    0.52 ms per token,  1922.15 tokens per second)\n",
            "llama_print_timings: prompt eval time =     293.08 ms /    84 tokens (    3.49 ms per token,   286.61 tokens per second)\n",
            "llama_print_timings:        eval time =    1636.80 ms /    15 runs   (  109.12 ms per token,     9.16 tokens per second)\n",
            "llama_print_timings:       total time =    2190.98 ms /    99 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Brazil drew 2-2 against England when the Maracana reopened.\n",
            "hyp1: In Brazil, the result reached 2-2 against England during the reopening of the Maracana Stadium.\n",
            "hyp2: Brazil reached the score 2-2 against England during the re-opening of the maracana stadium.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.65 ms /    16 runs   (    0.60 ms per token,  1657.86 tokens per second)\n",
            "llama_print_timings: prompt eval time =     315.04 ms /   118 tokens (    2.67 ms per token,   374.55 tokens per second)\n",
            "llama_print_timings:        eval time =    1619.88 ms /    15 runs   (  107.99 ms per token,     9.26 tokens per second)\n",
            "llama_print_timings:       total time =    2281.31 ms /   133 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Women will face higher car insurance premiums.\n",
            "hyp1: Yes, women are more likely to receive a traffic ticket than men. This is because they drive more often and for longer distances than men, making them targets for criminals who want to steal cars or drive off the road. In addition, female drivers tend to be younger and less experienced behind the wheel, which also increases the risk of accidents. As a result, insurance companies have to charge higher.\n",
            "hyp2: Car insurance will be more expensive for women.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.95 ms /    16 runs   (    0.50 ms per token,  2011.82 tokens per second)\n",
            "llama_print_timings: prompt eval time =     306.42 ms /    93 tokens (    3.29 ms per token,   303.51 tokens per second)\n",
            "llama_print_timings:        eval time =    1571.40 ms /    15 runs   (  104.76 ms per token,     9.55 tokens per second)\n",
            "llama_print_timings:       total time =    2147.65 ms /   108 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Yahoo agrees to buy Tumblr for $1.1 billion in cash.\n",
            "hyp1: It has become public knowledge that Yahoo refused to acquire Tumblr for a sum of $1.1 billion in cash.\n",
            "hyp2: It has become public knowledge that Yahoo has agreed to acquire Tumblr for a sum of $1.1 billion in cash.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.61 ms /    16 runs   (    0.54 ms per token,  1858.95 tokens per second)\n",
            "llama_print_timings: prompt eval time =     305.33 ms /    91 tokens (    3.36 ms per token,   298.04 tokens per second)\n",
            "llama_print_timings:        eval time =    1701.66 ms /    15 runs   (  113.44 ms per token,     8.81 tokens per second)\n",
            "llama_print_timings:       total time =    2282.24 ms /   106 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The US Senate confirms Janet Yellen as head of the US Federal Reserve.\n",
            "hyp1: The US Senate confirms Janet Yellen's appointment as head of the Federal Reserve, the country's central bank.\n",
            "hyp2: The US Senate confirms James Yellen's appointment as head of the Federal Reserve, the nation's central bank.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.54 ms /    16 runs   (    0.47 ms per token,  2120.89 tokens per second)\n",
            "llama_print_timings: prompt eval time =     291.59 ms /    68 tokens (    4.29 ms per token,   233.21 tokens per second)\n",
            "llama_print_timings:        eval time =    1590.67 ms /    15 runs   (  106.04 ms per token,     9.43 tokens per second)\n",
            "llama_print_timings:       total time =    2087.89 ms /    83 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Zimmerman pulled over for speeding in Texas, released with warning\n",
            "hyp1: In Texas, Zimmerman received a warning after she was stopped for speeding.\n",
            "hyp2: In Texas, Zimmerman received a warning after he was stopped for speeding.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.35 ms /    16 runs   (    0.52 ms per token,  1917.32 tokens per second)\n",
            "llama_print_timings: prompt eval time =     300.52 ms /    91 tokens (    3.30 ms per token,   302.81 tokens per second)\n",
            "llama_print_timings:        eval time =    1576.61 ms /    15 runs   (  105.11 ms per token,     9.51 tokens per second)\n",
            "llama_print_timings:       total time =    2136.13 ms /   106 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Ruffner, 45, does not yet have an attorney on the murder charge, authorities said.\n",
            "hyp1: Authorities said Ruffner, 45 years old, does not yet have an attorney on the murder charge.\n",
            "hyp2: Authorities said they have not yet appointed a lawyer for Ruffner, 45, who is charged with murder.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.19 ms /    16 runs   (    0.57 ms per token,  1741.02 tokens per second)\n",
            "llama_print_timings: prompt eval time =     307.67 ms /    86 tokens (    3.58 ms per token,   279.52 tokens per second)\n",
            "llama_print_timings:        eval time =    1639.85 ms /    15 runs   (  109.32 ms per token,     9.15 tokens per second)\n",
            "llama_print_timings:       total time =    2219.85 ms /   101 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: An elderly woman stands in a kitchen with two cats at her feet.\n",
            "hyp1: You will come up with new ideas! I was tired and felt limited, because that was my nature.\n",
            "hyp2: The elderly woman was standing in the kitchen, with her two cats at her feet. She was wearing a white blouse and apron and was cooking.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.55 ms /    16 runs   (    0.47 ms per token,  2118.64 tokens per second)\n",
            "llama_print_timings: prompt eval time =     283.00 ms /    57 tokens (    4.96 ms per token,   201.41 tokens per second)\n",
            "llama_print_timings:        eval time =    1568.10 ms /    15 runs   (  104.54 ms per token,     9.57 tokens per second)\n",
            "llama_print_timings:       total time =    2069.77 ms /    72 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The back of a stop sign with many stickers on it.\n",
            "hyp1: The back side of a road sign with many stickers.\n",
            "hyp2: The back side of a road sign filled with stickers.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.16 ms /    16 runs   (    0.51 ms per token,  1961.02 tokens per second)\n",
            "llama_print_timings: prompt eval time =     299.28 ms /    71 tokens (    4.22 ms per token,   237.24 tokens per second)\n",
            "llama_print_timings:        eval time =    1525.67 ms /    15 runs   (  101.71 ms per token,     9.83 tokens per second)\n",
            "llama_print_timings:       total time =    2041.29 ms /    86 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: \"The gloves are off,\" UNIFI official Rob O'Neill said.\n",
            "hyp1: According to UNIFI employee Rob O'Neill, the gloves refer to something already mentioned.\n",
            "hyp2: Rob O'Neill, official at UNIFI said the gloves are off.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.93 ms /    16 runs   (    0.56 ms per token,  1791.91 tokens per second)\n",
            "llama_print_timings: prompt eval time =     390.77 ms /   130 tokens (    3.01 ms per token,   332.67 tokens per second)\n",
            "llama_print_timings:        eval time =    1639.46 ms /    15 runs   (  109.30 ms per token,     9.15 tokens per second)\n",
            "llama_print_timings:       total time =    2401.18 ms /   145 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Syria's military police chief defected to the opposition\n",
            "hyp1: The Syrian Military Police has announced that its chief, Major General Maher Al-Assir, has defected to the opposition. This comes after months of protests and unrest in the country that have led to an escalation of violence between the government and protesters. Assir's defection is seen as a major setback for President Bashar Al-Assad and his regime, which has responded.\n",
            "hyp2: Syria's military police chief has defected to the opposition.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =      10.13 ms /    16 runs   (    0.63 ms per token,  1578.69 tokens per second)\n",
            "llama_print_timings: prompt eval time =     301.68 ms /    84 tokens (    3.59 ms per token,   278.44 tokens per second)\n",
            "llama_print_timings:        eval time =    1594.87 ms /    15 runs   (  106.32 ms per token,     9.41 tokens per second)\n",
            "llama_print_timings:       total time =    2301.11 ms /    99 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: General Electric reports a 9% profit drop for the third quarter.\n",
            "hyp1: The company's earnings fell 9% in the third quarter, reflecting a decline in demand for products and services as well as increased costs to deal with disruptions.\n",
            "hyp2: General Electric's profit fell 9% in the third quarter.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.15 ms /    16 runs   (    0.57 ms per token,  1748.83 tokens per second)\n",
            "llama_print_timings: prompt eval time =     399.85 ms /   136 tokens (    2.94 ms per token,   340.13 tokens per second)\n",
            "llama_print_timings:        eval time =    1527.35 ms /    15 runs   (  101.82 ms per token,     9.82 tokens per second)\n",
            "llama_print_timings:       total time =    2342.01 ms /   151 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: East Timor bans martial arts clubs due to killings.\n",
            "hyp1: On 19 November 2021, the East Timorese government passed a law banning all organizations associated with martial arts, including karate, taekwondo and other forms, from operating in the country. This decision was made after several deaths occurred within the country's martial arts scene, including the killing of a young man in Dili in May 2021.\n",
            "hyp2: In East Timor, martial arts clubs are banned as a result of murders.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.38 ms /    16 runs   (    0.46 ms per token,  2168.32 tokens per second)\n",
            "llama_print_timings: prompt eval time =     283.47 ms /    67 tokens (    4.23 ms per token,   236.36 tokens per second)\n",
            "llama_print_timings:        eval time =    1534.13 ms /    15 runs   (  102.28 ms per token,     9.78 tokens per second)\n",
            "llama_print_timings:       total time =    2033.05 ms /    82 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The statement did not specify how many warheads the missile can carry.\n",
            "hyp1: The statement did not specify the exact number of warheads the missile can carry.\n",
            "hyp2: The statement specified the exact number of warheads the missile can carry.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =      17.88 ms /    16 runs   (    1.12 ms per token,   894.75 tokens per second)\n",
            "llama_print_timings: prompt eval time =     336.02 ms /   128 tokens (    2.63 ms per token,   380.93 tokens per second)\n",
            "llama_print_timings:        eval time =    1982.12 ms /    15 runs   (  132.14 ms per token,     7.57 tokens per second)\n",
            "llama_print_timings:       total time =    3029.75 ms /   143 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: BlackBerry loses $965 million in the second quarter.\n",
            "hyp1: Overall, Blackberry's revenue fell 17% to $4.3 billion in the third quarter. The decrease in revenue was primarily due to a decrease in smartphone sales and reduced sales of enterprise products due to a decline in the business market. In addition, a weaker Canadian dollar led to a decrease in exports and an increase in imports, which contributed to\n",
            "hyp2: BlackBerry makes a loss of 965 million dollars in the second quarter.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.85 ms /    16 runs   (    0.55 ms per token,  1807.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =     324.28 ms /   110 tokens (    2.95 ms per token,   339.21 tokens per second)\n",
            "llama_print_timings:        eval time =    1552.25 ms /    15 runs   (  103.48 ms per token,     9.66 tokens per second)\n",
            "llama_print_timings:       total time =    2201.36 ms /   125 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: \"When you crossed the line, you violated that constitutional right,\" said Charles Weisselberg, a law professor at UC Berkeley.\n",
            "hyp1: Charles Weisselberg, a law professor at UC Berkeley, says, \"You violated constitutional rights when you crossed the line.\"\n",
            "hyp2: \"According to Charles Weisselberg, a law professor at UC Berkeley, you violated constitutional rights when you crossed the line,\" put another way.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.07 ms /    16 runs   (    0.57 ms per token,  1764.06 tokens per second)\n",
            "llama_print_timings: prompt eval time =     325.72 ms /   123 tokens (    2.65 ms per token,   377.62 tokens per second)\n",
            "llama_print_timings:        eval time =    1553.01 ms /    15 runs   (  103.53 ms per token,     9.66 tokens per second)\n",
            "llama_print_timings:       total time =    2227.89 ms /   138 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Snowden sees \"no chance\" for a fair trial in the United States.\n",
            "hyp1: Edward Snowden, the former National Security Agency (NSA) employee who leaked classified government documents to the press, says he has no faith that he will receive a fair trial in the United States. In a rare statement posted on his website on Monday, Snowden said he feared he would be the victim of \"politically motivated persecution\" if.\n",
            "hyp2: Snowden sees no prospect of a fair trial in the United States.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.54 ms /    16 runs   (    0.53 ms per token,  1873.54 tokens per second)\n",
            "llama_print_timings: prompt eval time =     285.98 ms /    67 tokens (    4.27 ms per token,   234.28 tokens per second)\n",
            "llama_print_timings:        eval time =    1724.40 ms /    15 runs   (  114.96 ms per token,     8.70 tokens per second)\n",
            "llama_print_timings:       total time =    2242.76 ms /    82 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: An elderly woman stands in a kitchen with two cats at her feet.\n",
            "hyp1: An elderly woman is standing in a kitchen and 3 cats are at her feet.\n",
            "hyp2: An elderly woman is standing in a kitchen and two cats are at her feet.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.04 ms /    16 runs   (    0.50 ms per token,  1989.55 tokens per second)\n",
            "llama_print_timings: prompt eval time =     281.54 ms /    62 tokens (    4.54 ms per token,   220.22 tokens per second)\n",
            "llama_print_timings:        eval time =    1633.71 ms /    15 runs   (  108.91 ms per token,     9.18 tokens per second)\n",
            "llama_print_timings:       total time =    2106.05 ms /    77 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Anthony Weiner slips to fourth place in new poll.\n",
            "hyp1: In a recent poll, Anthony Weiner has slipped back to fourth place.\n",
            "hyp2: In a recent poll, Andrew Weiner has slipped back to fourth place.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.72 ms /    16 runs   (    0.48 ms per token,  2071.73 tokens per second)\n",
            "llama_print_timings: prompt eval time =     288.20 ms /    65 tokens (    4.43 ms per token,   225.54 tokens per second)\n",
            "llama_print_timings:        eval time =    1649.42 ms /    15 runs   (  109.96 ms per token,     9.09 tokens per second)\n",
            "llama_print_timings:       total time =    2138.53 ms /    80 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The US believes the Syrian government used chemical weapons.\n",
            "hyp1: The African regime suspects that the Syrian government has used chemical warfare agents.\n",
            "hyp2: The US administration suspects that the Syrian government has used chemical warfare agents.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.25 ms /    16 runs   (    0.58 ms per token,  1729.36 tokens per second)\n",
            "llama_print_timings: prompt eval time =     385.17 ms /   129 tokens (    2.99 ms per token,   334.92 tokens per second)\n",
            "llama_print_timings:        eval time =    1579.32 ms /    15 runs   (  105.29 ms per token,     9.50 tokens per second)\n",
            "llama_print_timings:       total time =    2335.32 ms /   144 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Three feared dead after helicopter crashes into pub.\n",
            "hyp1: Three people are feared dead when a helicopter crashed into a pub.\n",
            "hyp2: A helicopter with three people on board has crashed into a restaurant in the city of Canton, Ohio. According to local authorities, three people are confirmed dead and several others are being treated in hospital with serious injuries. The accident occurred on a stormy afternoon when the helicopter was en route from the Cleveland Clinic to a local hospital to pick up patients. There is no information on the cause of the crash.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.67 ms /    16 runs   (    0.48 ms per token,  2086.59 tokens per second)\n",
            "llama_print_timings: prompt eval time =     307.55 ms /    84 tokens (    3.66 ms per token,   273.13 tokens per second)\n",
            "llama_print_timings:        eval time =    1525.69 ms /    15 runs   (  101.71 ms per token,     9.83 tokens per second)\n",
            "llama_print_timings:       total time =    2078.13 ms /    99 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Macau's gaming revenue reached a record $38 billion in 2012.\n",
            "hyp1: Revenues from gambling in Macau reached a record sum of SEK 38 billion in 2012.\n",
            "hyp2: Macau gaming revenue reached a record $38 billion in 2012.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.76 ms /    16 runs   (    0.48 ms per token,  2062.92 tokens per second)\n",
            "llama_print_timings: prompt eval time =     272.71 ms /    64 tokens (    4.26 ms per token,   234.68 tokens per second)\n",
            "llama_print_timings:        eval time =    1594.47 ms /    15 runs   (  106.30 ms per token,     9.41 tokens per second)\n",
            "llama_print_timings:       total time =    2062.50 ms /    79 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Helmand province is the world's largest opium-producing region.\n",
            "hyp1: Helmand is the largest opium growing region in the world.\n",
            "hyp2: Helmand used to be the largest opium growing region in the world.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.17 ms /    16 runs   (    0.51 ms per token,  1959.58 tokens per second)\n",
            "llama_print_timings: prompt eval time =     303.41 ms /    76 tokens (    3.99 ms per token,   250.48 tokens per second)\n",
            "llama_print_timings:        eval time =    1559.12 ms /    15 runs   (  103.94 ms per token,     9.62 tokens per second)\n",
            "llama_print_timings:       total time =    2096.62 ms /    91 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: At least 11 more cases in Indiana and three in Illinois are suspected.\n",
            "hyp1: In at least twelve cases in the state of Indiana and another three in the neighboring state of Illinois there is suspicion of\n",
            "hyp2: At least 11 more cases are suspected in Indiana and three in Illinois.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.91 ms /    16 runs   (    0.56 ms per token,  1796.14 tokens per second)\n",
            "llama_print_timings: prompt eval time =     413.08 ms /   144 tokens (    2.87 ms per token,   348.60 tokens per second)\n",
            "llama_print_timings:        eval time =    1593.75 ms /    15 runs   (  106.25 ms per token,     9.41 tokens per second)\n",
            "llama_print_timings:       total time =    2695.33 ms /   159 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The island nation reported 65 new cases on May 22, a one-day record, and 55 new cases on May 23, making Taiwan's epidemic the fastest growing in the world.\n",
            "hyp1: Taiwan is currently experiencing the fastest growing epidemic in the world, with a record 65 new cases reported on May 22 and another 55 new cases reported the following day.\n",
            "hyp2: Taiwan is currently experiencing the fastest growing epidemic in the world, with a record 65 new cases reported on May 22 and another 55 new cases reported the previous day.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.69 ms /    16 runs   (    0.48 ms per token,  2080.62 tokens per second)\n",
            "llama_print_timings: prompt eval time =     304.70 ms /    74 tokens (    4.12 ms per token,   242.86 tokens per second)\n",
            "llama_print_timings:        eval time =    1529.94 ms /    15 runs   (  102.00 ms per token,     9.80 tokens per second)\n",
            "llama_print_timings:       total time =    2061.03 ms /    89 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Wells' other series include NBC's ER and Third Watch.\n",
            "hyp1: One of Crichton's other television series is NBC's ER, and another is Third Watch.\n",
            "hyp2: One of Wells' other TV series is NBC's ER, and another is Third Watch.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.91 ms /    16 runs   (    0.56 ms per token,  1795.74 tokens per second)\n",
            "llama_print_timings: prompt eval time =     285.24 ms /    73 tokens (    3.91 ms per token,   255.93 tokens per second)\n",
            "llama_print_timings:        eval time =    1608.74 ms /    15 runs   (  107.25 ms per token,     9.32 tokens per second)\n",
            "llama_print_timings:       total time =    2112.10 ms /    88 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Romney announces Paul Ryan as vice presidential running mate.\n",
            "hyp1: Former Massachusetts governor Mitt Romney and his running mate, Wisconsin congressman Paul Ryan, today unveiled a new plan to rebuild America.\n",
            "hyp2: Romney announces Paul Ryan as his running mate.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.69 ms /    16 runs   (    0.61 ms per token,  1651.36 tokens per second)\n",
            "llama_print_timings: prompt eval time =     318.99 ms /   107 tokens (    2.98 ms per token,   335.43 tokens per second)\n",
            "llama_print_timings:        eval time =    1582.30 ms /    15 runs   (  105.49 ms per token,     9.48 tokens per second)\n",
            "llama_print_timings:       total time =    2406.62 ms /   122 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Boy Scouts postpones decision to admit homosexuals.\n",
            "hyp1: Scouting USA has not adopted an official stance on gay membership, and the issue is still debated among Scouts. As a result, individual scout groups or divisions may make decisions about whether or not to welcome LGBTQ members based on their own values ​​and beliefs.\n",
            "hyp2: Decision to allow homosexuals as members of the Boy Scouts postponed.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.00 ms /    16 runs   (    0.50 ms per token,  2001.00 tokens per second)\n",
            "llama_print_timings: prompt eval time =     288.82 ms /    78 tokens (    3.70 ms per token,   270.06 tokens per second)\n",
            "llama_print_timings:        eval time =    1613.69 ms /    15 runs   (  107.58 ms per token,     9.30 tokens per second)\n",
            "llama_print_timings:       total time =    2139.88 ms /    93 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Russia finds dead lawyer Magnitsky guilty in posthumous trial.\n",
            "hyp1: In a trial held after his death, the German authorities have determined that the lawyer Magnitsky was guilty.\n",
            "hyp2: In a trial held after his death, the Russian authorities have determined that the lawyer Magnitsky was guilty.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.62 ms /    16 runs   (    0.54 ms per token,  1855.07 tokens per second)\n",
            "llama_print_timings: prompt eval time =     318.27 ms /   105 tokens (    3.03 ms per token,   329.91 tokens per second)\n",
            "llama_print_timings:        eval time =    1555.34 ms /    15 runs   (  103.69 ms per token,     9.64 tokens per second)\n",
            "llama_print_timings:       total time =    2181.88 ms /   120 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: 2 dead, 2 injured in Nevada middle school shooting.\n",
            "hyp1: Two people were killed and two were injured in a shooting at a middle school in Nevada.\n",
            "hyp2: According to local news media reports, a shooting occurred at a school in Nevada today. Two people were killed and two were injured, one seriously. The shooter's identity is currently unknown. According to reports, the shooter took his own life after shooting himself.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =      10.27 ms /    16 runs   (    0.64 ms per token,  1558.39 tokens per second)\n",
            "llama_print_timings: prompt eval time =     312.46 ms /   103 tokens (    3.03 ms per token,   329.65 tokens per second)\n",
            "llama_print_timings:        eval time =    1661.70 ms /    15 runs   (  110.78 ms per token,     9.03 tokens per second)\n",
            "llama_print_timings:       total time =    2461.89 ms /   118 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Yahoo agrees to buy Tumblr for $1.1 billion in cash.\n",
            "hyp1: Yahoo has confirmed that it will buy blogging platform Tumblr for $1.1 billion in cash. This is a big step forward for Yahoo as it hopes to leverage its large database and user base to grow its digital business.\n",
            "hyp2: Yahoo buys Tumblr for $1.1 billion.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.48 ms /    16 runs   (    0.53 ms per token,  1886.57 tokens per second)\n",
            "llama_print_timings: prompt eval time =     309.33 ms /   107 tokens (    2.89 ms per token,   345.90 tokens per second)\n",
            "llama_print_timings:        eval time =    1569.71 ms /    15 runs   (  104.65 ms per token,     9.56 tokens per second)\n",
            "llama_print_timings:       total time =    2194.48 ms /   122 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Putin stated that the Russian government may withdraw from the treaty entirely if Western nations refuse to ratify the amended treaty.\n",
            "hyp1: Putin stated that the Russian government is fully prepared to completely withdraw from the agreement if Western countries refuse to approve the amended treaty.\n",
            "hyp2: Putin stated that the Russian government is fully prepared to completely withdraw from the agreement if Western countries do not refuse to approve the amended treaty.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.33 ms /    16 runs   (    0.58 ms per token,  1714.35 tokens per second)\n",
            "llama_print_timings: prompt eval time =     400.16 ms /   138 tokens (    2.90 ms per token,   344.86 tokens per second)\n",
            "llama_print_timings:        eval time =    1535.82 ms /    15 runs   (  102.39 ms per token,     9.77 tokens per second)\n",
            "llama_print_timings:       total time =    2337.31 ms /   153 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: \"And now this is what he wants to say,\" Alesha Badgley, director of social services for the Stone County Nursing and Rehabilitation Center, said this week.\n",
            "hyp1: \"This week, Alesha Badgley, the director of social services at Stone County Nursing and Rehabilitation Center, said this is what he wants to express,\" she said.\n",
            "hyp2: This week, Alesha Badgley, director of social services at Stone County Nursing and Rehabilitation Center, said: \"And now this is what he wants to say\".\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.77 ms /    16 runs   (    0.61 ms per token,  1638.34 tokens per second)\n",
            "llama_print_timings: prompt eval time =     296.31 ms /    72 tokens (    4.12 ms per token,   242.99 tokens per second)\n",
            "llama_print_timings:        eval time =    1825.16 ms /    15 runs   (  121.68 ms per token,     8.22 tokens per second)\n",
            "llama_print_timings:       total time =    2470.30 ms /    87 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Napolitano was elected for a second term as President of Italy.\n",
            "hyp1: Italy has elected Napolitano to a second term as the country's president.\n",
            "hyp2: Italy has not elected Napolitano for a second term as the country's president.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.04 ms /    16 runs   (    0.57 ms per token,  1769.91 tokens per second)\n",
            "llama_print_timings: prompt eval time =     418.15 ms /   139 tokens (    3.01 ms per token,   332.41 tokens per second)\n",
            "llama_print_timings:        eval time =    1567.17 ms /    15 runs   (  104.48 ms per token,     9.57 tokens per second)\n",
            "llama_print_timings:       total time =    2383.56 ms /   154 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Macau's gaming revenue reached a record $38 billion in 2012.\n",
            "hyp1: Gaming revenue in Macau reached a record $38 billion in 2012, according to data from international game technology (IGT), which provides gaming technology to many land-based and online casinos worldwide. This increase in revenue was partly due to more people starting to play online casino games via smartphones and tablets. In addition, Macao's has\n",
            "hyp2: Macau's gaming revenue reached a record high of $38 billion in 2012.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.97 ms /    16 runs   (    0.56 ms per token,  1783.52 tokens per second)\n",
            "llama_print_timings: prompt eval time =     418.05 ms /   132 tokens (    3.17 ms per token,   315.75 tokens per second)\n",
            "llama_print_timings:        eval time =    1659.09 ms /    15 runs   (  110.61 ms per token,     9.04 tokens per second)\n",
            "llama_print_timings:       total time =    2611.60 ms /   147 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Wells' other series include NBC's ER and Third Watch.\n",
            "hyp1: Among Wells' other series can be mentioned NBC's ER and Third Watch.\n",
            "hyp2: Jeffrey Bell is an American writer, producer and editor who has worked in the television industry since the 1980s. He is best known for his work on The X-files, where he was the showrunner for the first two seasons of the series before being replaced by Chris Carter. in addition to the x-files, he has also been the showrunner for the lone gunmen (\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =      11.26 ms /    16 runs   (    0.70 ms per token,  1420.83 tokens per second)\n",
            "llama_print_timings: prompt eval time =     330.08 ms /   113 tokens (    2.92 ms per token,   342.34 tokens per second)\n",
            "llama_print_timings:        eval time =    1907.37 ms /    15 runs   (  127.16 ms per token,     7.86 tokens per second)\n",
            "llama_print_timings:       total time =    2784.49 ms /   128 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Should I sign this paper? Shall I deny the things that I have done and been pardoned for ... so that you may say that I did not do it?\n",
            "hyp1: Should I sign this paper? Should I deny my actions that have not been forgiven ... so that you can claim that I did not do them?\n",
            "hyp2: Should I sign this paper? Should I deny my actions that have been forgiven ... so you can claim I didn't do them?\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.94 ms /    16 runs   (    0.50 ms per token,  2014.61 tokens per second)\n",
            "llama_print_timings: prompt eval time =     274.78 ms /    60 tokens (    4.58 ms per token,   218.36 tokens per second)\n",
            "llama_print_timings:        eval time =    1622.07 ms /    15 runs   (  108.14 ms per token,     9.25 tokens per second)\n",
            "llama_print_timings:       total time =    2090.50 ms /    75 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The back of a stop sign with many stickers on it.\n",
            "hyp1: \"A picture of a stop sign with lots of stickers on it.\"\n",
            "hyp2: A stop sign with lots of stickers on the back.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.41 ms /    16 runs   (    0.53 ms per token,  1903.40 tokens per second)\n",
            "llama_print_timings: prompt eval time =     286.69 ms /    74 tokens (    3.87 ms per token,   258.12 tokens per second)\n",
            "llama_print_timings:        eval time =    1563.72 ms /    15 runs   (  104.25 ms per token,     9.59 tokens per second)\n",
            "llama_print_timings:       total time =    2085.02 ms /    89 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Boy Scouts postpones decision to admit homosexuals.\n",
            "hyp1: A youth group that focuses on boys is delaying whether to allow homosexuals to become members.\n",
            "hyp2: A youth group that focuses on girls is delaying deciding whether to allow homosexuals to become members.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.67 ms /    16 runs   (    0.54 ms per token,  1845.23 tokens per second)\n",
            "llama_print_timings: prompt eval time =     291.81 ms /    68 tokens (    4.29 ms per token,   233.03 tokens per second)\n",
            "llama_print_timings:        eval time =    1828.69 ms /    15 runs   (  121.91 ms per token,     8.20 tokens per second)\n",
            "llama_print_timings:       total time =    2350.35 ms /    83 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Russia warns that it will react if interests are attacked in Ukraine.\n",
            "hyp1: Ukraine has issued a warning that it will respond if its interests are attacked in Russia.\n",
            "hyp2: Russia has issued a warning that it will respond if its interests are attacked in Ukraine.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.02 ms /    16 runs   (    0.56 ms per token,  1774.43 tokens per second)\n",
            "llama_print_timings: prompt eval time =     414.51 ms /   135 tokens (    3.07 ms per token,   325.69 tokens per second)\n",
            "llama_print_timings:        eval time =    1536.12 ms /    15 runs   (  102.41 ms per token,     9.76 tokens per second)\n",
            "llama_print_timings:       total time =    2360.01 ms /   150 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Zimmerman pulled over for speeding in Texas, released with warning\n",
            "hyp1: Zimmerman was released with a warning after being pulled over for speeding in Texas.\n",
            "hyp2: According to reports, Trayvon Martin was stopped by George Zimmerman on suspicion of violating a traffic law. Zimmerman reported this to the police and they drew up a misdemeanor report. but because there was no evidence to support the suspicions of a crime and because Trayvon did not pose a threat to George or anyone else, he was let off the hook.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.47 ms /    16 runs   (    0.59 ms per token,  1689.90 tokens per second)\n",
            "llama_print_timings: prompt eval time =     316.16 ms /    92 tokens (    3.44 ms per token,   290.99 tokens per second)\n",
            "llama_print_timings:        eval time =    1480.72 ms /    15 runs   (   98.71 ms per token,    10.13 tokens per second)\n",
            "llama_print_timings:       total time =    2059.44 ms /   107 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Myanmar's Suu Kyi calls for unity in party amid squabbles.\n",
            "hyp1: In the midst of an argument, Suu Kyi urges the party to unity.\n",
            "hyp2: Myanmar leader Aung San Suu Kyi urged her party to unite at a rally on Monday, just days after it was rocked by a power struggle.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.15 ms /    16 runs   (    0.57 ms per token,  1748.63 tokens per second)\n",
            "llama_print_timings: prompt eval time =     321.29 ms /   108 tokens (    2.97 ms per token,   336.15 tokens per second)\n",
            "llama_print_timings:        eval time =    1576.48 ms /    15 runs   (  105.10 ms per token,     9.51 tokens per second)\n",
            "llama_print_timings:       total time =    2206.41 ms /   123 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: AAA spokesman Jerry Cheske said prices may have affected some plans, but cheap hotel deals mitigated the impact.\n",
            "hyp1: AAA spokesperson Jerry Cheske stated that some plans may have been affected by prices; however, the impact was increased due to affordable hotel offers.\n",
            "hyp2: AAA spokesperson Jerry Cheske stated that some plans may have been affected by prices; however, the impact was reduced due to affordable hotel deals.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.13 ms /    16 runs   (    0.57 ms per token,  1752.08 tokens per second)\n",
            "llama_print_timings: prompt eval time =     419.43 ms /   173 tokens (    2.42 ms per token,   412.47 tokens per second)\n",
            "llama_print_timings:        eval time =    1648.25 ms /    15 runs   (  109.88 ms per token,     9.10 tokens per second)\n",
            "llama_print_timings:       total time =    2567.95 ms /   188 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The pill, which they call the \"polypill,\" would contain aspirin, a cholesterol-lowering drug, three blood pressure-lowering drugs at half the standard dose, and folic acid.\n",
            "hyp1: What they refer to as \"polypill\" (the birth control pill) must contain acetylsalicylic acid, an agent to raise the cholesterol level, three antihypertensive agents in half the standard dose and folic acid.\n",
            "hyp2: What they refer to as \"polypill\" (the birth control pill) must contain acetylsalicylic acid, an agent to lower the cholesterol level, three antihypertensive agents in half the standard dose and folic acid.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.44 ms /    15 runs   (    0.50 ms per token,  2015.32 tokens per second)\n",
            "llama_print_timings: prompt eval time =     292.89 ms /    68 tokens (    4.31 ms per token,   232.17 tokens per second)\n",
            "llama_print_timings:        eval time =    1461.17 ms /    14 runs   (  104.37 ms per token,     9.58 tokens per second)\n",
            "llama_print_timings:       total time =    1954.38 ms /    82 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Snowden sees \"no chance\" for a fair trial in the United States.\n",
            "hyp1: Snowden believes in the possibility of a fair trial in the United States.\n",
            "hyp2: Snowden does not believe in the possibility of a fair trial in the United States.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.42 ms /    16 runs   (    0.46 ms per token,  2156.04 tokens per second)\n",
            "llama_print_timings: prompt eval time =     289.84 ms /    70 tokens (    4.14 ms per token,   241.51 tokens per second)\n",
            "llama_print_timings:        eval time =    1617.28 ms /    15 runs   (  107.82 ms per token,     9.27 tokens per second)\n",
            "llama_print_timings:       total time =    2114.84 ms /    85 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Sendmail said the system can even be set to allow only business use.\n",
            "hyp1: According to Sendmail, the system can even be configured to not only allow business use.\n",
            "hyp2: According to Sendmail, the system can even be configured to only allow business use.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.90 ms /    16 runs   (    0.56 ms per token,  1798.36 tokens per second)\n",
            "llama_print_timings: prompt eval time =     321.03 ms /   122 tokens (    2.63 ms per token,   380.03 tokens per second)\n",
            "llama_print_timings:        eval time =    1650.37 ms /    15 runs   (  110.02 ms per token,     9.09 tokens per second)\n",
            "llama_print_timings:       total time =    2348.89 ms /   137 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: New strong earthquake hits the shattered region of Pakistan.\n",
            "hyp1: The vulnerable area of ​​Pakistan has been hit by a new strong earthquake.\n",
            "hyp2: According to the latest information from the authorities, a new earthquake has hit the area around the destroyed nuclear power plant in Chernobyl. The earthquake measured 5.4 on the Richter scale and caused great devastation, including extensive fires and radiation releases. The authorities are working to limit the radiation and help those affected by the devastation.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.52 ms /    16 runs   (    0.47 ms per token,  2126.81 tokens per second)\n",
            "llama_print_timings: prompt eval time =     282.88 ms /    68 tokens (    4.16 ms per token,   240.38 tokens per second)\n",
            "llama_print_timings:        eval time =    1580.23 ms /    15 runs   (  105.35 ms per token,     9.49 tokens per second)\n",
            "llama_print_timings:       total time =    2065.22 ms /    83 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: \"The gloves are off,\" UNIFI official Rob O'Neill said.\n",
            "hyp1: UNIFI official Rob O'Neill said: \"The gloves are off.\"\n",
            "hyp2: Rob O'Neill, a UNIFI employee, took off his gloves.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.82 ms /    16 runs   (    0.49 ms per token,  2046.04 tokens per second)\n",
            "llama_print_timings: prompt eval time =     286.19 ms /    75 tokens (    3.82 ms per token,   262.06 tokens per second)\n",
            "llama_print_timings:        eval time =    1673.80 ms /    15 runs   (  111.59 ms per token,     8.96 tokens per second)\n",
            "llama_print_timings:       total time =    2190.13 ms /    90 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: At least 11 more cases in Indiana and three in Illinois are suspected.\n",
            "hyp1: There are believed to be an additional twelve cases in Indiana as well as three more cases in Illinois.\n",
            "hyp2: At least 11 additional  cases are suspected in Indiana and three more in Illinois.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.70 ms /    16 runs   (    0.61 ms per token,  1649.65 tokens per second)\n",
            "llama_print_timings: prompt eval time =     415.42 ms /   143 tokens (    2.91 ms per token,   344.23 tokens per second)\n",
            "llama_print_timings:        eval time =    1604.12 ms /    15 runs   (  106.94 ms per token,     9.35 tokens per second)\n",
            "llama_print_timings:       total time =    2431.94 ms /   158 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Saferworld team leader for transfer controls and small arms Roy Isbister said: The EU embargo prohibits the direct or indirect supply of military equipment for use in Myanmar.\n",
            "hyp1: The head of Saferworld's transfer controls and small arms team, Roy Isbister, said the US embargo strictly prohibits the direct or indirect supply of military equipment intended for use in Myanmar.\n",
            "hyp2: The head of Saferworld's transfer controls and small arms team, Roy Isbister, said the EU embargo strictly prohibits the direct or indirect supply of military equipment intended for use in Myanmar.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.81 ms /    16 runs   (    0.49 ms per token,  2048.13 tokens per second)\n",
            "llama_print_timings: prompt eval time =     304.37 ms /    97 tokens (    3.14 ms per token,   318.69 tokens per second)\n",
            "llama_print_timings:        eval time =    1562.44 ms /    15 runs   (  104.16 ms per token,     9.60 tokens per second)\n",
            "llama_print_timings:       total time =    2132.77 ms /   112 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The value will be about $124 million, including convertible securities, according to Corel.\n",
            "hyp1: The value is estimated to be about $124 million, excluding convertible securities, according to Corel's statement.\n",
            "hyp2: The value is estimated to be about $124 million, including convertible securities, according to Corel's statement.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.33 ms /    16 runs   (    0.52 ms per token,  1920.77 tokens per second)\n",
            "llama_print_timings: prompt eval time =     314.50 ms /    89 tokens (    3.53 ms per token,   282.99 tokens per second)\n",
            "llama_print_timings:        eval time =    1635.52 ms /    15 runs   (  109.03 ms per token,     9.17 tokens per second)\n",
            "llama_print_timings:       total time =    2211.82 ms /   104 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Obama will speak on Syria from the White House at 1:15 PM EDT.\n",
            "hyp1: Obama will speak on Syria at 1:15 pm at the White House.\n",
            "hyp2: President Barack Obama will give a speech on the situation in Syria from the White House Rose Garden at 1:15 p.m. (EDT).\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.15 ms /    16 runs   (    0.57 ms per token,  1747.87 tokens per second)\n",
            "llama_print_timings: prompt eval time =     397.84 ms /   130 tokens (    3.06 ms per token,   326.77 tokens per second)\n",
            "llama_print_timings:        eval time =    1573.56 ms /    15 runs   (  104.90 ms per token,     9.53 tokens per second)\n",
            "llama_print_timings:       total time =    2552.65 ms /   145 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Gunmen surround Libya's Foreign Ministry to press demands.\n",
            "hyp1: A group of men with guns surrounded the Libyan Foreign Ministry in an attempt to push through their demands. They demanded that the government resign and form a new one that would be more favorable to their interests. But when the police tried to arrest them, they responded with violence and set fire to the building. The event was seen as a revolt by some, but an attack by others, and caused great.\n",
            "hyp2: Libya's foreign ministry is surrounded by armed men with demands.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.22 ms /    16 runs   (    0.51 ms per token,  1945.53 tokens per second)\n",
            "llama_print_timings: prompt eval time =     301.74 ms /    78 tokens (    3.87 ms per token,   258.50 tokens per second)\n",
            "llama_print_timings:        eval time =    1578.72 ms /    15 runs   (  105.25 ms per token,     9.50 tokens per second)\n",
            "llama_print_timings:       total time =    2108.04 ms /    93 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: At least 11 more cases in Indiana and three in Illinois are suspected.\n",
            "hyp1: At least 14 cases of measles have been confirmed in the United States, including the reported cases in Indiana and Illinois.\n",
            "hyp2: At least 11 more cases are suspected in Indiana and three in Illinois.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.85 ms /    16 runs   (    0.55 ms per token,  1807.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =     330.80 ms /   124 tokens (    2.67 ms per token,   374.85 tokens per second)\n",
            "llama_print_timings:        eval time =    1563.24 ms /    15 runs   (  104.22 ms per token,     9.60 tokens per second)\n",
            "llama_print_timings:       total time =    2248.37 ms /   139 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: While some other parts of Africa have been used as staging grounds for the terror group, Malawi had not previously been a major focus of al-Qaeda investigations.\n",
            "hyp1: While some other regions of Africa have served as staging grounds for the terrorist group al-Qaeda, Malawi had not previously been the subject of much investigation.\n",
            "hyp2: While some other regions in Africa have served as rallying points for the al-Qaeda terrorist group, Malawi had not previously been the focus of investigations.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.40 ms /    16 runs   (    0.53 ms per token,  1904.54 tokens per second)\n",
            "llama_print_timings: prompt eval time =     292.58 ms /    62 tokens (    4.72 ms per token,   211.90 tokens per second)\n",
            "llama_print_timings:        eval time =    1674.46 ms /    15 runs   (  111.63 ms per token,     8.96 tokens per second)\n",
            "llama_print_timings:       total time =    2277.94 ms /    77 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Syria's military police chief defected to the opposition\n",
            "hyp1: The head of the military police in Syria has defected to the opposition.\n",
            "hyp2: The top military leader in Syria has switched sides and gone over to the opposition.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.89 ms /    16 runs   (    0.56 ms per token,  1798.76 tokens per second)\n",
            "llama_print_timings: prompt eval time =     314.73 ms /    94 tokens (    3.35 ms per token,   298.67 tokens per second)\n",
            "llama_print_timings:        eval time =    1504.23 ms /    15 runs   (  100.28 ms per token,     9.97 tokens per second)\n",
            "llama_print_timings:       total time =    2092.84 ms /   109 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Syria denies it has an undeclared nuclear program.\n",
            "hyp1: Syria denies it has an undeclared nuclear program.\n",
            "hyp2: The Syrian government firmly maintains that it has no nuclear weapons program and that it never had such weapons. They also claim that allegations that it has such weapons are baseless and part of a US conspiracy to destabilize the country.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.23 ms /    16 runs   (    0.58 ms per token,  1732.73 tokens per second)\n",
            "llama_print_timings: prompt eval time =     323.37 ms /   119 tokens (    2.72 ms per token,   368.00 tokens per second)\n",
            "llama_print_timings:        eval time =    1641.88 ms /    15 runs   (  109.46 ms per token,     9.14 tokens per second)\n",
            "llama_print_timings:       total time =    2305.67 ms /   134 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Andy Murray deserves the title of nobility, says David Cameron.\n",
            "hyp1: David Cameron says Andy Murray deserves a peerage.\n",
            "hyp2: David Cameron has said that Andy Murray is a worthy recipient of the Order of the British Empire (OBE), which is awarded for services to country and society. Cameron said in a statement on Monday that Murray had \"made a lasting contribution to the sport by winning two Wimbledon titles and becoming world number one.\" He added that the Scot had \"inspired generations\".\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =      10.97 ms /    16 runs   (    0.69 ms per token,  1458.12 tokens per second)\n",
            "llama_print_timings: prompt eval time =     343.20 ms /   122 tokens (    2.81 ms per token,   355.47 tokens per second)\n",
            "llama_print_timings:        eval time =    1841.98 ms /    15 runs   (  122.80 ms per token,     8.14 tokens per second)\n",
            "llama_print_timings:       total time =    2791.62 ms /   137 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Gunman kills 6 in shooting at Wisconsin Sikh temple.\n",
            "hyp1: Six people killed by gunman in shooting at Wisconsin Sikh temple.\n",
            "hyp2: A gunman has killed at least six people and injured several others at a Sikh temple in Oak Creek, Wisconsin. According to local authorities, the incident happened at Gurdwara Sach Kaur Sahib, which is a holy place for Sikhs. The man entered the temple shortly after 12 local time (18:00 GMT).\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.33 ms /    16 runs   (    0.58 ms per token,  1714.53 tokens per second)\n",
            "llama_print_timings: prompt eval time =     296.20 ms /    74 tokens (    4.00 ms per token,   249.83 tokens per second)\n",
            "llama_print_timings:        eval time =    1602.26 ms /    15 runs   (  106.82 ms per token,     9.36 tokens per second)\n",
            "llama_print_timings:       total time =    2123.79 ms /    89 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Ankeet Chavan granted conditional bail for marriage.\n",
            "hyp1: A court has issued an order for a couple to marry. The couple must now go to court and sign the documents confirming their relationship.\n",
            "hyp2: Conditional bail for marriage was granted by Ankeet Chavan.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.61 ms /    16 runs   (    0.48 ms per token,  2101.94 tokens per second)\n",
            "llama_print_timings: prompt eval time =     301.36 ms /    71 tokens (    4.24 ms per token,   235.60 tokens per second)\n",
            "llama_print_timings:        eval time =    1620.99 ms /    15 runs   (  108.07 ms per token,     9.25 tokens per second)\n",
            "llama_print_timings:       total time =    2137.75 ms /    86 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Three feared dead after helicopter crashes into pub.\n",
            "hyp1: It is assumed that 13 people have lost their lives after a helicopter crashed into a restaurant.\n",
            "hyp2: It is assumed that three people have lost their lives after a helicopter crashed into a restaurant.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.76 ms /    16 runs   (    0.55 ms per token,  1825.65 tokens per second)\n",
            "llama_print_timings: prompt eval time =     297.07 ms /    71 tokens (    4.18 ms per token,   239.00 tokens per second)\n",
            "llama_print_timings:        eval time =    1740.74 ms /    15 runs   (  116.05 ms per token,     8.62 tokens per second)\n",
            "llama_print_timings:       total time =    2386.05 ms /    86 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Myanmar's Suu Kyi calls for unity in party amid squabbles.\n",
            "hyp1: Suu Kyi urges party unity amid strife.\n",
            "hyp2: Aung San Suu Syi of Myanmar encourages unity within the party amid controversy.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.52 ms /    16 runs   (    0.47 ms per token,  2127.38 tokens per second)\n",
            "llama_print_timings: prompt eval time =     276.68 ms /    57 tokens (    4.85 ms per token,   206.02 tokens per second)\n",
            "llama_print_timings:        eval time =    1527.44 ms /    15 runs   (  101.83 ms per token,     9.82 tokens per second)\n",
            "llama_print_timings:       total time =    1994.72 ms /    72 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: A girl playing in a pile of colorful balls.\n",
            "hyp1: a girl is playing in a pile of multicolored balls.\n",
            "hyp2: A woman is playing in a pile of multicolored balls.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.47 ms /    16 runs   (    0.53 ms per token,  1889.69 tokens per second)\n",
            "llama_print_timings: prompt eval time =     297.15 ms /    88 tokens (    3.38 ms per token,   296.15 tokens per second)\n",
            "llama_print_timings:        eval time =    1584.17 ms /    15 runs   (  105.61 ms per token,     9.47 tokens per second)\n",
            "llama_print_timings:       total time =    2128.84 ms /   103 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Brazil drew 2-2 against England when the Maracana reopened.\n",
            "hyp1: Brazil drew 2-2 with England when the Maracana reopened.\n",
            "hyp2: Brazil drew with England at the Maracana, which was an important game for both teams. It was an exciting event that contained many goals and chances in both directions.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.53 ms /    16 runs   (    0.60 ms per token,  1679.44 tokens per second)\n",
            "llama_print_timings: prompt eval time =     391.15 ms /   139 tokens (    2.81 ms per token,   355.36 tokens per second)\n",
            "llama_print_timings:        eval time =    1634.80 ms /    15 runs   (  108.99 ms per token,     9.18 tokens per second)\n",
            "llama_print_timings:       total time =    2424.68 ms /   154 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Bush plans to meet with Israeli Prime Minister Ariel Sharon and new Palestinian Prime Minister Mahmoud Abbas in the Jordanian port of Aqaba on Wednesday.\n",
            "hyp1: Bush has scheduled a meeting with both Israeli Prime Minister Ariel Sharon and newly appointed Palestinian Prime Minister Mahmoud Abbas in the port city of Aqaba, Jordan, to take place on Wednesday.\n",
            "hyp2: Bush has scheduled a meeting with both Israeli Prime Minister Mahmoud Abbas and newly appointed Palestinian Prime Minister Ariel Sharon in the Jordanian port city of Aqaba, which will take place on Wednesday.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.08 ms /    16 runs   (    0.51 ms per token,  1980.20 tokens per second)\n",
            "llama_print_timings: prompt eval time =     301.05 ms /    70 tokens (    4.30 ms per token,   232.52 tokens per second)\n",
            "llama_print_timings:        eval time =    1556.47 ms /    15 runs   (  103.76 ms per token,     9.64 tokens per second)\n",
            "llama_print_timings:       total time =    2074.38 ms /    85 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Gunmen surround Libya's Foreign Ministry to press demands.\n",
            "hyp1: A group of unarmed men have surrounded Libya's foreign ministry to enforce their demands.\n",
            "hyp2: A group of gunmen have surrounded Libya's foreign ministry to enforce their demands.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =      12.67 ms /    16 runs   (    0.79 ms per token,  1262.53 tokens per second)\n",
            "llama_print_timings: prompt eval time =     281.69 ms /    58 tokens (    4.86 ms per token,   205.90 tokens per second)\n",
            "llama_print_timings:        eval time =    1884.79 ms /    15 runs   (  125.65 ms per token,     7.96 tokens per second)\n",
            "llama_print_timings:       total time =    2499.28 ms /    73 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Kevin Rudd sworn in as Prime Minister of Australia.\n",
            "hyp1: Kevin Rudd accepted his duties as Prime Minister of Australia.\n",
            "hyp2: Kevin Rudd did not accept his duties as Prime Minister of Australia.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.36 ms /    16 runs   (    0.52 ms per token,  1914.10 tokens per second)\n",
            "llama_print_timings: prompt eval time =     282.25 ms /    68 tokens (    4.15 ms per token,   240.92 tokens per second)\n",
            "llama_print_timings:        eval time =    1568.64 ms /    15 runs   (  104.58 ms per token,     9.56 tokens per second)\n",
            "llama_print_timings:       total time =    2056.64 ms /    83 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Couple marry in UK's first Scientology wedding.\n",
            "hyp1: A couple divorces at the very first wedding according to Scientology tradition in the UK.\n",
            "hyp2: A couple gets married in the very first wedding according to Scientology tradition in Great Britain.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       4.55 ms /     9 runs   (    0.51 ms per token,  1978.02 tokens per second)\n",
            "llama_print_timings: prompt eval time =     301.99 ms /    75 tokens (    4.03 ms per token,   248.35 tokens per second)\n",
            "llama_print_timings:        eval time =     895.43 ms /     8 runs   (  111.93 ms per token,     8.93 tokens per second)\n",
            "llama_print_timings:       total time =    1393.17 ms /    83 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Man who set himself on fire on National Mall dies of injuries.\n",
            "hyp1: An individual who set himself on fire on the National Mall has died as a result of his injuries.\n",
            "hyp2: An individual who allegedly set himself on fire on the National Mall has died as a result of his injuries.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.51 ms /    16 runs   (    0.53 ms per token,  1880.36 tokens per second)\n",
            "llama_print_timings: prompt eval time =     409.31 ms /   138 tokens (    2.97 ms per token,   337.15 tokens per second)\n",
            "llama_print_timings:        eval time =    1537.53 ms /    15 runs   (  102.50 ms per token,     9.76 tokens per second)\n",
            "llama_print_timings:       total time =    2357.04 ms /   153 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Helmand province is the world's largest opium-producing region.\n",
            "hyp1: The Helmand province of Afghanistan is known for its large production of opium poppies, which are used to make heroin. According to the United Nations Office on Drugs and Crime (UNODC), in 2016 Helmand was the largest producer of opium in the world with a record 65,000 tonnes produced. This huge amount of opium provides a significant income.\n",
            "hyp2: Helmand province is the region in the world that produces the most opium.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.12 ms /    16 runs   (    0.45 ms per token,  2246.88 tokens per second)\n",
            "llama_print_timings: prompt eval time =     295.83 ms /    66 tokens (    4.48 ms per token,   223.10 tokens per second)\n",
            "llama_print_timings:        eval time =    1631.56 ms /    15 runs   (  108.77 ms per token,     9.19 tokens per second)\n",
            "llama_print_timings:       total time =    2148.40 ms /    81 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The search in Turkey ended with the last missing miners being found.\n",
            "hyp1: The search operation in Turkey ended with the discovery of the last missing miners.\n",
            "hyp2: The search operation in Turkey ended with the discovery of the first missing miners.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =      10.17 ms /    16 runs   (    0.64 ms per token,  1574.03 tokens per second)\n",
            "llama_print_timings: prompt eval time =     302.45 ms /    88 tokens (    3.44 ms per token,   290.96 tokens per second)\n",
            "llama_print_timings:        eval time =    1954.42 ms /    15 runs   (  130.29 ms per token,     7.67 tokens per second)\n",
            "llama_print_timings:       total time =    2712.69 ms /   103 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: When the bomb exploded at the Casa de España, customers were having dinner and playing bingo.\n",
            "hyp1: At the time the bomb detonated near the Casa de España, customers were dining and playing bingo.\n",
            "hyp2: At the time the bomb detonated near the Centro Español, customers were dining and playing bingo.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.06 ms /    16 runs   (    0.57 ms per token,  1765.61 tokens per second)\n",
            "llama_print_timings: prompt eval time =     335.41 ms /   120 tokens (    2.80 ms per token,   357.77 tokens per second)\n",
            "llama_print_timings:        eval time =    1594.51 ms /    15 runs   (  106.30 ms per token,     9.41 tokens per second)\n",
            "llama_print_timings:       total time =    2266.86 ms /   135 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: China's new premier rejects US hacking claims.\n",
            "hyp1: Xi Jinping's comments during a visit to Australia on Monday were his most vocal condemnation yet of recent claims that Beijing helped the Donald Trump administration win the election. \"We will not allow any kind of cyber attack against our country or any other nation,\" he said. \"We will take the necessary measures to protect our country's sovereignty.\"\n",
            "hyp2: China's new premier rejects US hacking allegations.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.14 ms /    16 runs   (    0.57 ms per token,  1750.74 tokens per second)\n",
            "llama_print_timings: prompt eval time =     387.77 ms /   141 tokens (    2.75 ms per token,   363.62 tokens per second)\n",
            "llama_print_timings:        eval time =    1570.77 ms /    15 runs   (  104.72 ms per token,     9.55 tokens per second)\n",
            "llama_print_timings:       total time =    2361.34 ms /   156 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Investigators are looking into the motives of the LAX airport shooter.\n",
            "hyp1: An investigator has been looking into the shooting at Los Angeles International Airport (LAX), which killed one officer and injured two others. According to CNN, the investigator reported that the shooter, a former military police officer, had \"mental health issues\" and \"was under investigation\" by US authorities for domestic violence and substance abuse issues. The investigator also said the shooter had \"taken his own life\" after.\n",
            "hyp2: They are investigating the motives of the shooter at LAX airport.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.20 ms /    16 runs   (    0.51 ms per token,  1950.51 tokens per second)\n",
            "llama_print_timings: prompt eval time =     305.53 ms /   104 tokens (    2.94 ms per token,   340.39 tokens per second)\n",
            "llama_print_timings:        eval time =    1529.68 ms /    15 runs   (  101.98 ms per token,     9.81 tokens per second)\n",
            "llama_print_timings:       total time =    2132.83 ms /   119 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: South Africa has the world's highest number of cases with 4.7 million people infected with HIV or AIDS.\n",
            "hyp1: South Africa has the unfortunate distinction of having the highest number of HIV or AIDS cases in the world, with an estimated 4.7 million people affected.\n",
            "hyp2: South Africa has the highest number of HIV or AIDS cases in the world, with an estimated 4.7 million people affected.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.09 ms /    16 runs   (    0.51 ms per token,  1977.75 tokens per second)\n",
            "llama_print_timings: prompt eval time =     311.90 ms /    84 tokens (    3.71 ms per token,   269.32 tokens per second)\n",
            "llama_print_timings:        eval time =    1594.47 ms /    15 runs   (  106.30 ms per token,     9.41 tokens per second)\n",
            "llama_print_timings:       total time =    2143.38 ms /    99 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: 2 dead, 2 injured in Nevada middle school shooting.\n",
            "hyp1: In an exchange of gunfire at an elementary school in Nevada, two people have lost their lives while two others have been injured.\n",
            "hyp2: In an exchange of gunfire at an elementary school in Nevada, four people have lost their lives while others have been injured.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.41 ms /    16 runs   (    0.59 ms per token,  1699.78 tokens per second)\n",
            "llama_print_timings: prompt eval time =     299.92 ms /    87 tokens (    3.45 ms per token,   290.08 tokens per second)\n",
            "llama_print_timings:        eval time =    1708.16 ms /    15 runs   (  113.88 ms per token,     8.78 tokens per second)\n",
            "llama_print_timings:       total time =    2274.45 ms /   102 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Investigators are looking into the motives of the LAX airport shooter.\n",
            "hyp1: An investigator is reviewing or looking into the reasons why the shooter at LAX airport acted the way he did.\n",
            "hyp2: An investigator examines or looks at the reasons why the shooter at LAX airport would act the way he did.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.26 ms /    16 runs   (    0.58 ms per token,  1728.80 tokens per second)\n",
            "llama_print_timings: prompt eval time =     405.41 ms /   158 tokens (    2.57 ms per token,   389.73 tokens per second)\n",
            "llama_print_timings:        eval time =    1550.35 ms /    15 runs   (  103.36 ms per token,     9.68 tokens per second)\n",
            "llama_print_timings:       total time =    2405.67 ms /   173 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: \"Still, the 'somewhat ambiguous ruling' could be a setback for Static Control depending on how it developed its competing product,\" said Merrill Lynch analyst Steven Milunovich.\n",
            "hyp1: Steven Milunovich, a Merrill Lynch analyst, suggested that Static Control's possible unclear victory in a lawsuit could potentially be a disadvantage for the company, depending on how it created its competing product.\n",
            "hyp2: Steven Milunovich, an analyst at Merrill Lynch, suggested that the ambiguous verdict in the lawsuit could potentially be a disadvantage for the company Static Controls, depending on how they created their competing product.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.40 ms /    16 runs   (    0.53 ms per token,  1904.76 tokens per second)\n",
            "llama_print_timings: prompt eval time =     312.95 ms /    85 tokens (    3.68 ms per token,   271.61 tokens per second)\n",
            "llama_print_timings:        eval time =    1575.26 ms /    15 runs   (  105.02 ms per token,     9.52 tokens per second)\n",
            "llama_print_timings:       total time =    2146.30 ms /   100 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The International Atomic Energy Association is the watchdog of the United Nations.\n",
            "hyp1: The International Atomic Energy Agency (IAEA) acts as NATO's guardian in the field of atomic energy.\n",
            "hyp2: The International Atomic Energy Agency (IAEA) acts as the UN's watchdog in the field of atomic energy.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.89 ms /    16 runs   (    0.49 ms per token,  2027.37 tokens per second)\n",
            "llama_print_timings: prompt eval time =     292.99 ms /    67 tokens (    4.37 ms per token,   228.68 tokens per second)\n",
            "llama_print_timings:        eval time =    1571.92 ms /    15 runs   (  104.79 ms per token,     9.54 tokens per second)\n",
            "llama_print_timings:       total time =    2174.35 ms /    82 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: BlackBerry loses $965 million in the second quarter.\n",
            "hyp1: In the second quarter, blackberry lost 965 million USD.\n",
            "hyp2: In the second quarter, blackberry experienced a decline of $965 million.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.78 ms /    16 runs   (    0.49 ms per token,  2057.08 tokens per second)\n",
            "llama_print_timings: prompt eval time =     316.20 ms /    94 tokens (    3.36 ms per token,   297.28 tokens per second)\n",
            "llama_print_timings:        eval time =    1536.70 ms /    15 runs   (  102.45 ms per token,     9.76 tokens per second)\n",
            "llama_print_timings:       total time =    2120.30 ms /   109 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The Mexican government has adopted a series of measures against drug trafficking and organized crime in Mexico since 2006.\n",
            "hyp1: The Mexican Supreme Court has enacted several measures against drug trafficking and organized crime in Mexico since 2006.\n",
            "hyp2: The Mexican government has adopted several measures against drug trafficking and organized crime in Mexico since 2006.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.29 ms /    16 runs   (    0.58 ms per token,  1721.91 tokens per second)\n",
            "llama_print_timings: prompt eval time =     314.12 ms /   100 tokens (    3.14 ms per token,   318.35 tokens per second)\n",
            "llama_print_timings:        eval time =    1695.58 ms /    15 runs   (  113.04 ms per token,     8.85 tokens per second)\n",
            "llama_print_timings:       total time =    2320.62 ms /   115 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Indian troops already fighting rebels in Kashmir are now engaged in a massive campaign to destroy poppy crops in the region.\n",
            "hyp1: Indian soldiers already fighting insurgents in Kashmir are now involved in a major operation to destroy poppy crops in the area.\n",
            "hyp2: Indian soldiers already fighting insurgents in Kashmir are now involved in a major operation to destroy mustard crops in the area.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.44 ms /    16 runs   (    0.46 ms per token,  2150.83 tokens per second)\n",
            "llama_print_timings: prompt eval time =     281.07 ms /    63 tokens (    4.46 ms per token,   224.14 tokens per second)\n",
            "llama_print_timings:        eval time =    1552.28 ms /    15 runs   (  103.49 ms per token,     9.66 tokens per second)\n",
            "llama_print_timings:       total time =    2029.27 ms /    78 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Updated - Two explosions near Boston Marathon finish line.\n",
            "hyp1: Recently, two explosions occurred near the finish area during the Boston Marathon.\n",
            "hyp2: Update on two explosions near the finish area during the Boston Marathon.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.74 ms /    16 runs   (    0.55 ms per token,  1830.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =     329.65 ms /   120 tokens (    2.75 ms per token,   364.02 tokens per second)\n",
            "llama_print_timings:        eval time =    1635.64 ms /    15 runs   (  109.04 ms per token,     9.17 tokens per second)\n",
            "llama_print_timings:       total time =    2299.07 ms /   135 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Mandela back in hospital in 'serious but stable' condition.\n",
            "hyp1: Mandela was readmitted to hospital and is in a serious but stable condition.\n",
            "hyp2: Former president Nelson Mandela has returned to a hospital in South Africa after being admitted for a medical evaluation, his lawyer confirmed on Monday. Mandela was admitted to hospital on Sunday and was examined by doctors after experiencing breathing difficulties and headaches. His lawyer Makaziwe Mandela said he was in good spirits and expected to be discharged soon.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.34 ms /    16 runs   (    0.58 ms per token,  1713.25 tokens per second)\n",
            "llama_print_timings: prompt eval time =     268.81 ms /    61 tokens (    4.41 ms per token,   226.93 tokens per second)\n",
            "llama_print_timings:        eval time =    1898.05 ms /    15 runs   (  126.54 ms per token,     7.90 tokens per second)\n",
            "llama_print_timings:       total time =    2483.67 ms /    76 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Syria denies it has an undeclared nuclear program.\n",
            "hyp1: Syria does not deny that it has a publicly known nuclear program.\n",
            "hyp2: Syria denies that it has a nuclear program that is not publicly known.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.70 ms /    16 runs   (    0.48 ms per token,  2077.92 tokens per second)\n",
            "llama_print_timings: prompt eval time =     294.88 ms /    65 tokens (    4.54 ms per token,   220.42 tokens per second)\n",
            "llama_print_timings:        eval time =    1589.54 ms /    15 runs   (  105.97 ms per token,     9.44 tokens per second)\n",
            "llama_print_timings:       total time =    2078.37 ms /    80 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Over 100 dead as typhoon hits central Philippines.\n",
            "hyp1: The typhoon causes over 100 deaths in the central Philippines.\n",
            "hyp2: The typhoon causes over 100 deaths in central Indonesia.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.14 ms /    16 runs   (    0.51 ms per token,  1966.33 tokens per second)\n",
            "llama_print_timings: prompt eval time =     277.75 ms /    58 tokens (    4.79 ms per token,   208.82 tokens per second)\n",
            "llama_print_timings:        eval time =    1592.67 ms /    15 runs   (  106.18 ms per token,     9.42 tokens per second)\n",
            "llama_print_timings:       total time =    2065.53 ms /    73 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Russia ratified the amended version of the treaty.\n",
            "hyp1: Russia gave its approval to the revised version of the agreement.\n",
            "hyp2: England gave its approval to the revised version of the agreement.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =      10.95 ms /    16 runs   (    0.68 ms per token,  1461.32 tokens per second)\n",
            "llama_print_timings: prompt eval time =     299.49 ms /    66 tokens (    4.54 ms per token,   220.38 tokens per second)\n",
            "llama_print_timings:        eval time =    1902.78 ms /    15 runs   (  126.85 ms per token,     7.88 tokens per second)\n",
            "llama_print_timings:       total time =    2542.83 ms /    81 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: New strong earthquake hits the shattered region of Pakistan.\n",
            "hyp1: A very strong earthquake has hit the so far undamaged area of ​​Pakistan.\n",
            "hyp2: A very strong earthquake has hit the damaged area in Pakistan.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.06 ms /    16 runs   (    0.57 ms per token,  1765.81 tokens per second)\n",
            "llama_print_timings: prompt eval time =     313.67 ms /   120 tokens (    2.61 ms per token,   382.57 tokens per second)\n",
            "llama_print_timings:        eval time =    1499.08 ms /    15 runs   (   99.94 ms per token,    10.01 tokens per second)\n",
            "llama_print_timings:       total time =    2148.38 ms /   135 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Man who set himself on fire on National Mall dies of injuries.\n",
            "hyp1: The man who set himself on fire on the National Mall dies from his injuries.\n",
            "hyp2: A man set himself on fire on the National Mall, causing serious injuries. The incident occurred shortly after he had thrown a firebomb at the White House and shouted \"death to America!\" before running into the crowd and setting himself on fire. The man was taken to hospital with burns and breathing problems, but his life was not in danger.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.07 ms /    16 runs   (    0.50 ms per token,  1982.90 tokens per second)\n",
            "llama_print_timings: prompt eval time =     297.33 ms /    85 tokens (    3.50 ms per token,   285.87 tokens per second)\n",
            "llama_print_timings:        eval time =    1578.35 ms /    15 runs   (  105.22 ms per token,     9.50 tokens per second)\n",
            "llama_print_timings:       total time =    2113.50 ms /   100 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: You will figure things out! I will get bored and feel tied down because that's who I am.\n",
            "hyp1: You will come up with new ideas! I will get tired and feel limited, because that is my nature.\n",
            "hyp2: You will come up with new ideas! I was tired and felt limited, because that was my nature.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =      11.22 ms /    16 runs   (    0.70 ms per token,  1425.90 tokens per second)\n",
            "llama_print_timings: prompt eval time =     325.39 ms /   120 tokens (    2.71 ms per token,   368.78 tokens per second)\n",
            "llama_print_timings:        eval time =    1782.77 ms /    15 runs   (  118.85 ms per token,     8.41 tokens per second)\n",
            "llama_print_timings:       total time =    2698.21 ms /   135 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The US believes the Syrian government used chemical weapons.\n",
            "hyp1: The US believes that chemical weapons were used by the Syrian government.\n",
            "hyp2: The US government has concluded that the Syrian government was likely behind the use of chemical weapons in Idlib province on April 4. This is based on a combination of evidence, including satellite imagery, eyewitness reports and interviews with victims and experts. However, the United States will not confirm or deny the existence of chemical weapons until an independent investigation has been conducted.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.69 ms /    16 runs   (    0.48 ms per token,  2079.81 tokens per second)\n",
            "llama_print_timings: prompt eval time =     280.64 ms /    65 tokens (    4.32 ms per token,   231.61 tokens per second)\n",
            "llama_print_timings:        eval time =    1519.95 ms /    15 runs   (  101.33 ms per token,     9.87 tokens per second)\n",
            "llama_print_timings:       total time =    2001.07 ms /    80 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The Pope calls for action against climate change in the draft encyclical.\n",
            "hyp1: Pope argues against action against climate change in draft encyclical.\n",
            "hyp2: The Pope calls for action against climate change in the draft of an encyclical.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.63 ms /    16 runs   (    0.54 ms per token,  1852.92 tokens per second)\n",
            "llama_print_timings: prompt eval time =     293.76 ms /    86 tokens (    3.42 ms per token,   292.76 tokens per second)\n",
            "llama_print_timings:        eval time =    1630.72 ms /    15 runs   (  108.71 ms per token,     9.20 tokens per second)\n",
            "llama_print_timings:       total time =    2168.08 ms /   101 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Helmand province is the world's largest opium-producing region.\n",
            "hyp1: Opium cultivation is most widespread in Helmand province, making it the world's largest producer of opium.\n",
            "hyp2: Opium cultivation is not widespread in Helmand province, making it the world's largest producer of opium.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =      10.15 ms /    16 runs   (    0.63 ms per token,  1576.67 tokens per second)\n",
            "llama_print_timings: prompt eval time =     325.61 ms /   120 tokens (    2.71 ms per token,   368.54 tokens per second)\n",
            "llama_print_timings:        eval time =    1707.29 ms /    15 runs   (  113.82 ms per token,     8.79 tokens per second)\n",
            "llama_print_timings:       total time =    2581.67 ms /   135 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: A girl playing in a pile of colorful balls.\n",
            "hyp1: An image of a young girl lying on her back, her legs straight and her feet dangling over the edge of a bed or sofa. She is wearing a bright yellow t-shirt and shorts, and her hair is combed to the side in an elegant ponytail. Her eyes are closed, and she looks peaceful, as if she is completely absorbed in the game.\n",
            "hyp2: A girl playing in a sea of ​​balls.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.92 ms /    16 runs   (    0.50 ms per token,  2018.93 tokens per second)\n",
            "llama_print_timings: prompt eval time =     289.31 ms /    69 tokens (    4.19 ms per token,   238.50 tokens per second)\n",
            "llama_print_timings:        eval time =    1623.08 ms /    15 runs   (  108.21 ms per token,     9.24 tokens per second)\n",
            "llama_print_timings:       total time =    2117.61 ms /    84 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Egypt's leaders impose a state of emergency in three cities.\n",
            "hyp1: Egypt's leaders impose a state of emergency in three cities.\n",
            "hyp2: To deal with a certain situation, Egypt's leaders have implemented a state of emergency in three cities.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.09 ms /    16 runs   (    0.51 ms per token,  1978.73 tokens per second)\n",
            "llama_print_timings: prompt eval time =     297.93 ms /    93 tokens (    3.20 ms per token,   312.15 tokens per second)\n",
            "llama_print_timings:        eval time =    1553.93 ms /    15 runs   (  103.60 ms per token,     9.65 tokens per second)\n",
            "llama_print_timings:       total time =    2109.31 ms /   108 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Obama will speak on Syria from the White House at 1:15 PM EDT.\n",
            "hyp1: There will be a speech from Obama regarding Syria from the American White House at 1:15 p.m. EDT.\n",
            "hyp2: There will be a speech from Obama regarding Syria from the American White House at 3:13 p.m. EDT.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.75 ms /    16 runs   (    0.55 ms per token,  1828.99 tokens per second)\n",
            "llama_print_timings: prompt eval time =     332.32 ms /   128 tokens (    2.60 ms per token,   385.17 tokens per second)\n",
            "llama_print_timings:        eval time =    1511.51 ms /    15 runs   (  100.77 ms per token,     9.92 tokens per second)\n",
            "llama_print_timings:       total time =    2192.56 ms /   143 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Russia ratified the amended version of the treaty.\n",
            "hyp1: Russia ratified the new version of the treaty.\n",
            "hyp2: The Government of Russia has approved and signed the Revised Treaty on the Non-Proliferation of Nuclear Weapons (NPT) in New York City, USA on September 26, 2022. This ended a 30-year process that began after the original NPT was adopted in 1970. The Treaty entered into effective March 5 2\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.41 ms /    16 runs   (    0.59 ms per token,  1699.60 tokens per second)\n",
            "llama_print_timings: prompt eval time =     297.36 ms /    84 tokens (    3.54 ms per token,   282.48 tokens per second)\n",
            "llama_print_timings:        eval time =    1626.47 ms /    15 runs   (  108.43 ms per token,     9.22 tokens per second)\n",
            "llama_print_timings:       total time =    2170.51 ms /    99 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: A black and white image of a cat lying on a carpet.\n",
            "hyp1: A black and white photo of a cat on a carpet.\n",
            "hyp2: A cute little kitten is lying comfortably on her soft white bed. The fluffy ears are angled back and the sweet eyes gently look up at you with an expression of calmness.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =      11.65 ms /    16 runs   (    0.73 ms per token,  1373.74 tokens per second)\n",
            "llama_print_timings: prompt eval time =     321.68 ms /    95 tokens (    3.39 ms per token,   295.32 tokens per second)\n",
            "llama_print_timings:        eval time =    1827.39 ms /    15 runs   (  121.83 ms per token,     8.21 tokens per second)\n",
            "llama_print_timings:       total time =    2625.33 ms /   110 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Kevin Rudd sworn in as Prime Minister of Australia.\n",
            "hyp1: Kevin Rudd took office as Prime Minister of Australia.\n",
            "hyp2: On 2 July 2013, Kevin Rudd, leader of the Australian Labor Party (AWP), was sworn in as the country's 28th Prime Minister. He succeeded Julia Gillard, who had served two terms.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.79 ms /    16 runs   (    0.55 ms per token,  1821.08 tokens per second)\n",
            "llama_print_timings: prompt eval time =     291.16 ms /    82 tokens (    3.55 ms per token,   281.64 tokens per second)\n",
            "llama_print_timings:        eval time =    1579.91 ms /    15 runs   (  105.33 ms per token,     9.49 tokens per second)\n",
            "llama_print_timings:       total time =    2113.23 ms /    97 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Texans languishing under skyrocketing home insurance premiums may finally be getting relief.\n",
            "hyp1: Texas residents who have been burdened with exorbitant home insurance premiums can finally breathe a sigh of relief.\n",
            "hyp2: Texas residents who have been burdened with exorbitant property taxes may finally get relief.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.73 ms /    16 runs   (    0.55 ms per token,  1831.71 tokens per second)\n",
            "llama_print_timings: prompt eval time =     395.86 ms /   132 tokens (    3.00 ms per token,   333.45 tokens per second)\n",
            "llama_print_timings:        eval time =    1492.64 ms /    15 runs   (   99.51 ms per token,    10.05 tokens per second)\n",
            "llama_print_timings:       total time =    2246.85 ms /   147 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Updated - Two explosions near Boston Marathon finish line.\n",
            "hyp1: Update: Two explosions occurred near the finish line of the Boston Marathon.\n",
            "hyp2: Two explosions occurred during the course of the Boston Marathon, killing three people and injuring 264. The explosions occurred at 8:00 a.m. local time (1:00 p.m. UTC) on Boylston Street, just meters from the finish line. It is not yet clear what caused the explosions, but authorities suspect they were terror-related.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.27 ms /    16 runs   (    0.58 ms per token,  1725.81 tokens per second)\n",
            "llama_print_timings: prompt eval time =     313.73 ms /    98 tokens (    3.20 ms per token,   312.37 tokens per second)\n",
            "llama_print_timings:        eval time =    1651.08 ms /    15 runs   (  110.07 ms per token,     9.08 tokens per second)\n",
            "llama_print_timings:       total time =    2235.32 ms /   113 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Anthony Weiner slips to fourth place in new poll.\n",
            "hyp1: Anthony Weiner slips to fourth place in new poll\n",
            "hyp2: According to a new Quinnipiac University poll, New York Rep. Anthony Weiner is now ranked fourth among Democratic candidates for the 2020 presidential election. He has dropped to fourth since last month, but his decline has been slower than some other contenders.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.71 ms /    16 runs   (    0.54 ms per token,  1837.18 tokens per second)\n",
            "llama_print_timings: prompt eval time =     292.58 ms /    71 tokens (    4.12 ms per token,   242.67 tokens per second)\n",
            "llama_print_timings:        eval time =    1706.68 ms /    15 runs   (  113.78 ms per token,     8.79 tokens per second)\n",
            "llama_print_timings:       total time =    2218.25 ms /    86 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: East Timor bans martial arts clubs due to killings.\n",
            "hyp1: East Timor to ban clubs that practice martial arts, as a result of deadly acts of violence.\n",
            "hyp2: East Timor has banned martial arts clubs as a result of deadly violence.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.43 ms /    16 runs   (    0.59 ms per token,  1696.71 tokens per second)\n",
            "llama_print_timings: prompt eval time =     329.81 ms /   127 tokens (    2.60 ms per token,   385.07 tokens per second)\n",
            "llama_print_timings:        eval time =    1629.58 ms /    15 runs   (  108.64 ms per token,     9.20 tokens per second)\n",
            "llama_print_timings:       total time =    2537.04 ms /   142 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Egypt's leaders impose a state of emergency in three cities.\n",
            "hyp1: The leaders of Egypt issue a state of emergency in three cities.\n",
            "hyp2: According to Egypt's official news agency, President Abdel Fattah Al-Sisi has issued a decree declaring a state of emergency in the cities of Alexandria, Cairo and Suez. This decree will give the government increased powers to take measures to control the spread of the coronavirus disease (covid-19). The state of exception will remain in effect.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to create the CSV files containing the predictions."
      ],
      "metadata": {
        "id": "xIY-lNkCfb9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "def create_csv_prediction_file (data, filename):\n",
        "\n",
        "  # Data to be written to the CSV file\n",
        "  #data = [\n",
        "  #    {\"id\": 1, \"label\": \"A\", \"explanation\": \"Explanation for A\"},\n",
        "  #    {\"id\": 2, \"label\": \"B\", \"explanation\": \"Explanation for B\"},\n",
        "  #    {\"id\": 3, \"label\": \"C\", \"explanation\": \"Explanation for C\"},\n",
        "  #]\n",
        "\n",
        "  # Name of the CSV file\n",
        "  #csv_file = \"data.csv\"\n",
        "  csv_file = filename\n",
        "\n",
        "  # Field names\n",
        "  fields = [\"id\", \"label\", \"explanation\"]\n",
        "\n",
        "  # Writing to CSV file\n",
        "  with open(csv_file, mode='w', newline='') as file:\n",
        "      writer = csv.DictWriter(file, fieldnames=fields)\n",
        "\n",
        "      # Write the header\n",
        "      writer.writeheader()\n",
        "\n",
        "      # Write the data\n",
        "      for row in data:\n",
        "          writer.writerow(row)\n",
        "\n",
        "  print(\"CSV file created successfully.\")\n"
      ],
      "metadata": {
        "id": "np7tmKILfUtg"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_csv_prediction_file (prediction_en, 'eloquent2024_mc_mistral_en_prediction.csv')\n",
        "create_csv_prediction_file (prediction_sv, 'eloquent2024_mc_mistral_sv_prediction.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMSsoCgcfzG3",
        "outputId": "ef7ae381-76ef-424c-9419-179509908e8f"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file created successfully.\n",
            "CSV file created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('eloquent2024_mc_mistral_en_prediction.csv')\n",
        "files.download('eloquent2024_mc_mistral_sv_prediction.csv')"
      ],
      "metadata": {
        "id": "dia3xJw7TwSL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "9f786764-73f2-4dfc-aaa5-f91728d5b613"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2f0ae44d-69d7-4bc7-89e3-ddf5e916ea57\", \"eloquent2024_mc_mistral_en_prediction.csv\", 1221)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_564a41ef-ad91-4ca7-85ef-6c0bd3432aca\", \"eloquent2024_mc_mistral_sv_prediction.csv\", 1221)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2647583556a2419d83b97a1d4e57555b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c466da3506e7425ca7f5874d08460a99",
              "IPY_MODEL_d3b2d3fd3ca24c3996a07f678229db32",
              "IPY_MODEL_dee9385e8bcd47f7af8e4b84b04e5a43"
            ],
            "layout": "IPY_MODEL_05bc6238f46b4cab82a21be889550a84"
          }
        },
        "c466da3506e7425ca7f5874d08460a99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1417532ecdd24e4f99754c4dad4af4d8",
            "placeholder": "​",
            "style": "IPY_MODEL_bdcc141c7bd544ee8420423779449ea3",
            "value": "mistral-7b-instruct-v0.2.Q6_K.gguf: 100%"
          }
        },
        "d3b2d3fd3ca24c3996a07f678229db32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fba8a0aed5654779ac9d8646ff25b1a7",
            "max": 5942065440,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_94dd097903c3479f94a51b6e938f7a3d",
            "value": 5942065440
          }
        },
        "dee9385e8bcd47f7af8e4b84b04e5a43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b88d2665c5694762a7d8f096ea0785d9",
            "placeholder": "​",
            "style": "IPY_MODEL_090c755b65054c7ca0f8d9bd059c21b1",
            "value": " 5.94G/5.94G [00:58&lt;00:00, 139MB/s]"
          }
        },
        "05bc6238f46b4cab82a21be889550a84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1417532ecdd24e4f99754c4dad4af4d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdcc141c7bd544ee8420423779449ea3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fba8a0aed5654779ac9d8646ff25b1a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94dd097903c3479f94a51b6e938f7a3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b88d2665c5694762a7d8f096ea0785d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "090c755b65054c7ca0f8d9bd059c21b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}