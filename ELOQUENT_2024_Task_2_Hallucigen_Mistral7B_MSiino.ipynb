{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marco-siino/eloquent2024/blob/main/ELOQUENT_2024_Task_2_Hallucigen_Mistral7B_MSiino.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ubSCVmmlBKX"
      },
      "source": [
        "Installing dependencies. You might need to tweak the CMAKE_ARGS for the `llama-cpp-python` pip package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKL68Itp9Bm-",
        "outputId": "be8bd5a4-cdda-47ec-e891-624842306363"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python>=0.1.79\n",
            "  Downloading llama_cpp_python-0.2.74.tar.gz (49.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 MB\u001b[0m \u001b[31m191.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting typing-extensions>=4.5.0 (from llama-cpp-python>=0.1.79)\n",
            "  Downloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
            "Collecting numpy>=1.20.0 (from llama-cpp-python>=0.1.79)\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m264.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python>=0.1.79)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m130.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jinja2>=2.11.3 (from llama-cpp-python>=0.1.79)\n",
            "  Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m164.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting MarkupSafe>=2.0 (from jinja2>=2.11.3->llama-cpp-python>=0.1.79)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.74-cp310-cp310-linux_x86_64.whl size=61855856 sha256=7c49d3d4a20bb7c1096a5c9eadb816a051471fba20f94b86656614e195efb37e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-v4scoyns/wheels/21/1f/6d/ee16805ea65ed00d89776a8ce82f3631340a43fc41edabed97\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: typing-extensions, numpy, MarkupSafe, diskcache, jinja2, llama-cpp-python\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.11.0\n",
            "    Uninstalling typing_extensions-4.11.0:\n",
            "      Successfully uninstalled typing_extensions-4.11.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 2.1.5\n",
            "    Uninstalling MarkupSafe-2.1.5:\n",
            "      Successfully uninstalled MarkupSafe-2.1.5\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.4\n",
            "    Uninstalling Jinja2-3.1.4:\n",
            "      Successfully uninstalled Jinja2-3.1.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.2.1+cu121 requires nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-nccl-cu12==2.19.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-2.1.5 diskcache-5.6.3 jinja2-3.1.4 llama-cpp-python-0.2.74 numpy-1.26.4 typing-extensions-4.11.0\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.11.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.2.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Collecting huggingface-hub>=0.21.2 (from datasets)\n",
            "  Downloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "Successfully installed datasets-2.19.1 dill-0.3.8 huggingface-hub-0.23.0 multiprocess-0.70.16 xxhash-3.4.1\n",
            "Collecting deep-translator\n",
            "  Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (2.31.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2024.2.2)\n",
            "Installing collected packages: deep-translator\n",
            "Successfully installed deep-translator-1.11.4\n"
          ]
        }
      ],
      "source": [
        "# GPU llama-cpp-python; Starting from version llama-cpp-python==0.1.79, it supports GGUF\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on \" pip install 'llama-cpp-python>=0.1.79' --force-reinstall --upgrade --no-cache-dir\n",
        "# For download the models\n",
        "!pip install huggingface_hub\n",
        "!pip install datasets\n",
        "!pip install -U deep-translator\n",
        "\n",
        "import datasets\n",
        "from datasets import load_dataset\n",
        "from deep_translator import GoogleTranslator\n",
        "import json\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "import tqdm.notebook as tqdm\n",
        "\n",
        "# Seed to shuffle the json training set.\n",
        "seed_value = 42\n",
        "random.seed(seed_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2ns57iDlBKa"
      },
      "source": [
        "Downloading an instruction-finetuned Mistral model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uDMqQmBfAhYO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2647583556a2419d83b97a1d4e57555b",
            "c466da3506e7425ca7f5874d08460a99",
            "d3b2d3fd3ca24c3996a07f678229db32",
            "dee9385e8bcd47f7af8e4b84b04e5a43",
            "05bc6238f46b4cab82a21be889550a84",
            "1417532ecdd24e4f99754c4dad4af4d8",
            "bdcc141c7bd544ee8420423779449ea3",
            "fba8a0aed5654779ac9d8646ff25b1a7",
            "94dd097903c3479f94a51b6e938f7a3d",
            "b88d2665c5694762a7d8f096ea0785d9",
            "090c755b65054c7ca0f8d9bd059c21b1"
          ]
        },
        "outputId": "bcc17345-e45f-4988-971b-7c83e9fd3563"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "mistral-7b-instruct-v0.2.Q6_K.gguf:   0%|          | 0.00/5.94G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2647583556a2419d83b97a1d4e57555b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.cache/huggingface/hub/models--TheBloke--Mistral-7B-Instruct-v0.2-GGUF/snapshots/3a6fbf4a41a1d52e415a4958cde6856d34b2db93/mistral-7b-instruct-v0.2.Q6_K.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
            "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q6_K:  226 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 1000000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q6_K\n",
            "llm_load_print_meta: model params     = 7.24 B\n",
            "llm_load_print_meta: model size       = 5.53 GiB (6.56 BPW) \n",
            "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.30 MiB\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloaded 32/33 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =  5666.09 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  5461.00 MiB\n",
            "...................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 8192\n",
            "llama_new_context_with_model: n_batch    = 800\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 1000000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   560.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =    24.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 4\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '18'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Guessed chat format: mistral-instruct\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "model_name_or_path = \"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\"\n",
        "model_basename = \"mistral-7b-instruct-v0.2.Q6_K.gguf\"\n",
        "\n",
        "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n",
        "\n",
        "# This config has been tested on an RTX 3080 (VRAM of 16GB).\n",
        "# you might need to tweak with respect to your hardware.\n",
        "from llama_cpp import Llama\n",
        "lcpp_llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_threads=4, #16, # CPU cores\n",
        "    n_batch=800, #8000, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
        "    n_gpu_layers=32, # Change this value based on your model and your GPU VRAM pool.\n",
        "    n_ctx=8192, # Context window\n",
        "    logits_all=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the dataset for the three subtasks."
      ],
      "metadata": {
        "id": "jeXgLOpd4ztp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load the trial data for both English and Swedish\n",
        "trial_ds = load_dataset(\"Eloquent/HalluciGen-PG\", name=\"trial\")\n",
        "\n",
        "#load the trial data only for Swedish\n",
        "trial_ds_sv = load_dataset(\"Eloquent/HalluciGen-PG\", name=\"trial\", split=\"trial_swedish\")\n",
        "\n",
        "#load the test data for the detection step in both English and Swedish\n",
        "test_ds = load_dataset(\"Eloquent/HalluciGen-PG\", name=\"test_detection\")"
      ],
      "metadata": {
        "id": "rjFefzHKeKQk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds['test_detection_swedish'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0J_zqEDg4di",
        "outputId": "54e1b799-e5b3-401a-9511-7e1d9dfb57ff"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 0,\n",
              " 'source': 'Kvinnor kommer att möta högre bilförsäkringspremier.',\n",
              " 'hyp1': 'Det betyder att kvinnor kan förvänta sig att betala högre priser för sina fordonstilläggsförsäkringar.',\n",
              " 'hyp2': 'Kvinnor kommer att få högre premier för bilförsäkring.'}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trial_ds['trial_english'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVNUDVeFhp-g",
        "outputId": "59d25f3c-630a-461c-d3eb-7353ec0c9d46"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 0,\n",
              " 'source': 'The population has declined in some 210 of the 280 municipalities in Sweden, mainly in inland central and northern Sweden.',\n",
              " 'type': 'antonym',\n",
              " 'hyp1': \"In the majority of Sweden's 280 municipalities, the population has gone up.\",\n",
              " 'hyp2': \"In the majority of Sweden's 280 municipalities, the population has gone down.\",\n",
              " 'label': 'hyp1'}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the dataset object to a list\n",
        "trial_en_list = list(trial_ds['trial_english'])\n",
        "trial_sv_list = list(trial_ds['trial_swedish'])\n",
        "\n",
        "# Shuffle the list\n",
        "random.shuffle(trial_en_list)\n",
        "random.shuffle(trial_sv_list)\n",
        "\n"
      ],
      "metadata": {
        "id": "wpopYrrziQIq"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trial_sv_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_9tDXP8jHQQ",
        "outputId": "c9ccc8bc-e05d-4abf-a202-2bc37c6a9205"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'id': 2,\n",
              "  'source': 'Län med befolkningsminskning kommer att vara Vermillion, Posey och Madison.',\n",
              "  'type': 'named entity',\n",
              "  'hyp1': 'Vermillion, Posey och Madison är län som kommer att uppleva minskande befolkning.',\n",
              "  'hyp2': 'Vermillion, Posey och Marion är län som kommer att uppleva minskande befolkning.',\n",
              "  'label': 'hyp2'},\n",
              " {'id': 12,\n",
              "  'source': 'Israels Peres uppmanar parterna att återgå till fredssamtalen.',\n",
              "  'type': 'addition',\n",
              "  'hyp1': 'Peres i Israel ger en uppmaning att återgå till fredssamtalen.',\n",
              "  'hyp2': 'Israels president Shimon Peres uppmanade på måndagen världssamfundet att återuppta fredsförhandlingarna med palestinierna och betonade vikten av en tvåstatslösning.',\n",
              "  'label': 'hyp2'},\n",
              " {'id': 11,\n",
              "  'source': 'Grekisk högerextrem ledare fängslad i väntan på rättegång.',\n",
              "  'type': 'addition',\n",
              "  'hyp1': '\\xa0I väntan på rättegång har en grekisk högerextrem ledare fängslats.',\n",
              "  'hyp2': 'En grekisk högerextremistisk ledare har gripits och kommer att ställas inför rätta, anklagad för att ha organiserat våldsamma protester under de senaste åren. Ledaren, som går under namnet Nikos Michaloliakos, är en tidigare medlem av det grekiska nationalistpartiet Gyllene gryning. Han misstänks också för att ha varit inblandad i mordet på en anti-rasism-.',\n",
              "  'label': 'hyp2'},\n",
              " {'id': 7,\n",
              "  'source': 'Nordkorea carnar utlänningar i söder och ger råd om evakuering.',\n",
              "  'type': 'negation',\n",
              "  'hyp1': 'Nordkorea varnar inte utländska medborgare i södra delarna av landet och ger råd om evakuering.',\n",
              "  'hyp2': 'Nordkorea varnar utländska medborgare i södra delarna av landet och ger råd om evakuering.',\n",
              "  'label': 'hyp1'},\n",
              " {'id': 18,\n",
              "  'source': 'Vi har träffats, men du minns inte mig. Jag arbetade för ett företag som du anlitade för att få ditt minne raderat.',\n",
              "  'type': 'pronoun',\n",
              "  'hyp1': 'Vi har träffats tidigare, men du minns mig inte. Du var anställd av ett företag som jag anlitade för att radera mitt minne.',\n",
              "  'hyp2': 'Vi har träffats tidigare, men du minns mig inte. Jag var anställd av ett företag som du anlitade för att radera ditt minne.',\n",
              "  'label': 'hyp1'},\n",
              " {'id': 3,\n",
              "  'source': 'Google presenterar en prototyp för en självkörande bil.',\n",
              "  'type': 'addition',\n",
              "  'hyp1': 'En prototyp för en självkörande bil presentera av Google.',\n",
              "  'hyp2': 'Enligt ett blogginlägg från Google har företaget utvecklat en prototyp av en självkörande bil som kan köra helt på egen hand. Bilen, som kallas \"Waymo One\", har testats i Nevada-öknen och har visat att den kan navigera i komplexa trafikmiljöer utan att behöva någon mänsklig övervakning.',\n",
              "  'label': 'hyp2'},\n",
              " {'id': 9,\n",
              "  'source': 'Irans kärnvapenförhandlingar går in på tredje dagen.',\n",
              "  'type': 'number',\n",
              "  'hyp1': 'Diskussioner om irans kärnvapenprogram har nått sin tredje dag.',\n",
              "  'hyp2': 'Diskussioner om irans kärnvapenprogram har nått sin fjärde dag.',\n",
              "  'label': 'hyp2'},\n",
              " {'id': 0,\n",
              "  'source': 'Men intäkterna från mjukvarulicenser, ett mått som finansanalytiker följer noga, minskade med 21 procent till 107,6 miljoner dollar.',\n",
              "  'type': 'number',\n",
              "  'hyp1': 'Intäkter från programvarulicenser, en metrik som noggrant övervakas av finansiella analytiker, minskade med 21 procent till ett belopp av 107,6 miljoner dollar.',\n",
              "  'hyp2': 'Intäkter från programvarulicenser, en metrik som noggrant övervakas av finansiella analytiker, minskade med 42 procent till ett belopp av 107,6 miljoner dollar.',\n",
              "  'label': 'hyp2'},\n",
              " {'id': 17,\n",
              "  'source': ' Den iranska regeringen sa: det iranska kärnkraftsprogrammet förblir fredligt.',\n",
              "  'type': 'tense',\n",
              "  'hyp1': 'Regeringen i Iran förklarade att nationens kärnenergiprogram kommer att förbli fredligt.',\n",
              "  'hyp2': 'Regeringen i Iran förklarade att nationens kärnenergiprogram brukade vara fredligt.',\n",
              "  'label': 'hyp2'},\n",
              " {'id': 15,\n",
              "  'source': 'Efter att demonstranter rusat upp på scenen och två gånger brutit strömmen till mikrofonen, avslutade Hedges talet tidigt.',\n",
              "  'type': 'number',\n",
              "  'hyp1': 'När demonstranter stormade scenen och två gånger stängde av mikrofonens ström, avslutade Hedges sitt tal i förtid.',\n",
              "  'hyp2': 'När demonstranter stormade scenen och fem gånger stängde av mikrofonens ström, avslutade Hedges sitt tal i förtid.',\n",
              "  'label': 'hyp2'},\n",
              " {'id': 5,\n",
              "  'source': 'Spaniens prinsessa vittnar i historisk bedrägeriundersökning.',\n",
              "  'type': 'gender',\n",
              "  'hyp1': 'Spanska prinsessan blir en del av en utredning om historisk bedrägeri.',\n",
              "  'hyp2': 'Spanska prinsen blir en del av en utredning om historisk bedrägeri.',\n",
              "  'label': 'hyp2'},\n",
              " {'id': 13,\n",
              "  'source': 'Federal Trade Commission (FTC) bad kongressen idag om ytterligare behörighet för att bekämpa oönskad Internet-spam, som nu står för upp till hälften av all e-posttrafik. ',\n",
              "  'type': 'named entity',\n",
              "  'hyp1': 'Federal Trade Commission (FTC) har idag begärt ytterligare befogenhet från underhuset för att bekämpa oönskat internet-spam, som nu står för upp till hälften av all e-posttrafik.',\n",
              "  'hyp2': 'Federal Trade Commission (FTC) har idag begärt ytterligare befogenhet från kongressen för att bekämpa oönskat internet-spam, som nu står för upp till hälften av all e-posttrafik.',\n",
              "  'label': 'hyp1'},\n",
              " {'id': 16,\n",
              "  'source': 'Förare i spansk tågolycka frågas ut av domaren.',\n",
              "  'type': 'addition',\n",
              "  'hyp1': 'Enligt rapporterna från El mundo, en domare vid domstolen för Andalusien har påbörjat en utredning efter att ha mottagit en begäran om information från de spanska myndigheterna angående kraschen av ett höghastighetståg mellan Madrid och Sevilla förra veckan. Enligt uppgift var 77 personer som var ombord på tåget när det spårade ur. I ett uttalande på sin hemsida, sade domstolen:',\n",
              "  'hyp2': 'Förare i spansk tågolycka utfrågas av domare.',\n",
              "  'label': 'hyp1'},\n",
              " {'id': 14,\n",
              "  'source': 'Utfrågningen ägde rum en dag efter att Pentagon för första gången pekade ut en officer, Dallager, för att han inte tog upp skandalen.',\n",
              "  'type': 'named entity',\n",
              "  'hyp1': 'Förhöret ägde rum en dag efter att FBI offentligt identifierade en officer, Dallager, för första gången och hävdade att han inte hade åtgärdat skandalen.',\n",
              "  'hyp2': 'Förhöret ägde rum en dag efter att Pentagon offentligt identifierade en officer, Dallager, för första gången och hävdade att han inte hade åtgärdat skandalen.',\n",
              "  'label': 'hyp1'},\n",
              " {'id': 1,\n",
              "  'source': 'Hongkong-universitet samarbetar med universitet, företag och statliga sektorer i Kina för att samordna utbildningsprogram och forskningscentra för att främja högteknologisk forskning, kommersialisering och tekniköverföring.',\n",
              "  'type': 'natural',\n",
              "  'hyp1': 'University of Hong Kong samarbetar med olika kinesiska universitet, företag och regeringsdepartement för att samordna utbildningsprogram och forskningscentra med syftet att främja avancerad teknologisk forskning, kommersialisering och tekniköverföring.',\n",
              "  'hyp2': 'University of Hong Kong samarbetar med olika kinesiska universitet, företag och statliga sektorer för att samordna utbildningsprogram och forskningscentra med syftet att främja avancerad teknologisk forskning, kommersialisering och tekniköverföring.',\n",
              "  'label': 'hyp1'},\n",
              " {'id': 19,\n",
              "  'source': 'Deras ledare, Abu Bakr al-Azdi, överlämnade sig själv i juni; hans ställföreträdare dödades i en skjutning nyligen med saudiska styrkor.',\n",
              "  'type': 'date',\n",
              "  'hyp1': 'Abu Bakr al-Azdi, gruppledaren, hade nyligen överlämnat sig själv den 16 Juni 2015, medan hans biträdande nyligen dödades i ett möte med saudiska styrkor.',\n",
              "  'hyp2': 'Abu Bakr al-Azdi, gruppledaren, hade nyligen överlämnat sig själv i juni, medan hans biträdande nyligen dödades i ett möte med saudiska styrkor.',\n",
              "  'label': 'hyp1'},\n",
              " {'id': 6,\n",
              "  'source': 'Mannen använder en slägga för att bryta betongblocket som finns på den andre mannen.',\n",
              "  'type': 'antonym',\n",
              "  'hyp1': 'Mannen använder en slägga för att krossa betongblocket som ligger ovanpå den andra mannen.',\n",
              "  'hyp2': 'Mannen använder en slägga för att krossa betongblocket som ligger under den andra mannen.',\n",
              "  'label': 'hyp2'},\n",
              " {'id': 4,\n",
              "  'source': 'Lagförslaget säger att en kvinna som genomgår en sådan abort inte kunde åtalas.',\n",
              "  'type': 'negation',\n",
              "  'hyp1': 'Förslaget stadgar att en kvinna som genomgår en sådan abort kan åtalas.',\n",
              "  'hyp2': 'Förslaget stadgar att en kvinna som genomgår en sådan abort inte kan åtalas.',\n",
              "  'label': 'hyp1'},\n",
              " {'id': 8,\n",
              "  'source': 'Beväpnad man bland 7 döda efter lägenhetsskjutning i Florida.',\n",
              "  'type': 'addition',\n",
              "  'hyp1': 'En beväpnad man var bland de sju döda efter en lägenhetsskjutning i Florida.',\n",
              "  'hyp2': 'En man med ett skjutvapen har dödat minst sju personer och skadat flera andra efter att ha öppnat eld i en lägenhet i Fort Lauderdale, Florida, tidigt på morgonen lokal tid. Enligt lokala myndigheter var offren för skjutningen hemmahörande i olika delar av landet. Den misstänkta skytten är gripen. Det är för närvarande oklart vad som motiverade dådet.',\n",
              "  'label': 'hyp2'},\n",
              " {'id': 10,\n",
              "  'source': 'AstraZeneca betalar 4,1 miljarder dollar för att köpa ut Bristol-Myers Squibb ur diabetesalliansen.',\n",
              "  'type': 'natural',\n",
              "  'hyp1': 'AstraZeneca har kommit överens om att överföra 4,1 miljarder dollar till Bristol-Myers Squibb i utbyte mot kontroll över deras samarbete inom diabetesvården.',\n",
              "  'hyp2': 'AstraZeneca har kommit överens om att överföra 4,1 miljarder dollar till Bristol-Myers Squibb för att köpa ut företaget ur diabetesalliansen.',\n",
              "  'label': 'hyp1'}]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create few-shot samples from training set."
      ],
      "metadata": {
        "id": "fHPX3E0PCJ9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " prompt_context = \"\"\"[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "zafRW5vU6y1n"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_few_shot_samples(train_list,nr_samples,language):\n",
        "\n",
        "  nr_few_shot_samples = nr_samples\n",
        "\n",
        "  few_shot_samples = ''\n",
        "\n",
        "  for i in range(0,nr_few_shot_samples):\n",
        "    if language=='sv':\n",
        "      src = GoogleTranslator(source='sv', target='en').translate(train_list[i]['source'])\n",
        "      hyp1 = GoogleTranslator(source='sv', target='en').translate(train_list[i]['hyp1'])\n",
        "      hyp2 = GoogleTranslator(source='sv', target='en').translate(train_list[i]['hyp2'])\n",
        "    else:\n",
        "      src = train_list[i]['source']\n",
        "      hyp1 = train_list[i]['hyp1']\n",
        "      hyp2 = train_list[i]['hyp2']\n",
        "\n",
        "    label = train_list[i]['label']\n",
        "\n",
        "    few_shot_samples += '<s>'+prompt_context+\"src: \"+src+\"\\nhyp1: \"+hyp1 + '\\nhyp2: '+hyp2+'\\nlabel: [/INST] \\n ' + label + \"\\n</s> \\n\\n \"\n",
        "\n",
        "  return few_shot_samples"
      ],
      "metadata": {
        "id": "99ag8gav51mC"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_samples_en = create_few_shot_samples(trial_en_list,16,'en')"
      ],
      "metadata": {
        "id": "o8IaDH-7GmrN"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_samples_sv = create_few_shot_samples(trial_sv_list,20,'sv')"
      ],
      "metadata": {
        "id": "pW64JRxsr0os"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(few_shot_samples_en)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WHgxqxAxcu1",
        "outputId": "9d9f345a-03f4-4222-d2ac-bd3264d41c1a"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: This state of affairs has not changed in more than 100 years, but hopefully at some stage - and perhaps soon - change will come.\n",
            "hyp1: There has been no change in the status quo in over 100 years, but there is hope that change will soon come. \n",
            "hyp2: The state of affairs is1-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-65561-6556\n",
            "label: [/INST] \n",
            " hyp2\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The draft agenda as drawn up by the Conference of Presidents pursuant to Rule 95 of the Rules of Procedure has been distributed.\n",
            "hyp1: The Conference of Presidents hasn't distributed the draft agenda.\n",
            "hyp2: The Conference of Presidents has distributed the draft agenda.\n",
            "label: [/INST] \n",
            " hyp1\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: There would be a dual ceiling: either the application of a 20 % rate of taxation on interest payments or the supply of information.\n",
            "hyp1: It would be possible to apply a 20 % rate of taxation on interest payments or the supply of information\n",
            "hyp2: It would be possible to apply a 20 % rate of taxation on interest payments.\n",
            "label: [/INST] \n",
            " hyp2\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: In addition to these losses, there were also significant losses in terms of infrastructures, totalling approximately EUR 15 million.\n",
            "hyp1: There were losses in the amount of approximately 15 million dollars.\n",
            "hyp2: There were infrastructure-related losses in the amount of approximately 15 million euros.\n",
            "label: [/INST] \n",
            " hyp1\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: I am always grateful for comments and suggestions from the floor, because I believe I need to take everyone's views into account to be an effective President.\n",
            "hyp1: I think I need to listen to everyone's views in order to be an ineffective President.\n",
            "hyp2: I think I need to listen to everyone's views in order to be an effective President.\n",
            "label: [/INST] \n",
            " hyp1\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: We need quite specific legislative proposals, including proposals based on Articles 13 and 137 of the Treaty of Amsterdam.\n",
            "hyp1: Legislative proposals based on the Treaty of Amsterdam are needed.\n",
            "hyp2: Legislative proposals based solely on the Treaty of Amsterdam are needed.\n",
            "label: [/INST] \n",
            " hyp2\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: In 1998, 1 700 000 net jobs were created in Europe, and although I admit that the employment situation is far from ideal, it has improved.\n",
            "hyp1: In 1700 there were 1 998 000 net jobs created in Europe.\n",
            "hyp2: In 1998 there were 1 700 000 net jobs created in Europe.\n",
            "label: [/INST] \n",
            " hyp1\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: We have done so: on 5 February we published an extremely detailed press release dealing with the questions you have raised.\n",
            "hyp1: We published a press release that dealt with the questions we raised.\n",
            "hyp2: We published a press release that dealt with the questions you raised.\n",
            "label: [/INST] \n",
            " hyp1\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Madam President, I am speaking on behalf of our colleague, Mr Francis Decourrière, who drafted one of the motions for a resolution.\n",
            "hyp1: One of the motions for a resolution was drafted by Mr Francis Decourrière.\n",
            "hyp2: One of the motions for a resolution was drafted by Mrs Francis Decourrière.\n",
            "label: [/INST] \n",
            " hyp2\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Furthermore, we are currently thinking of organising economic discussions in Brussels, in the form of study days, in which this House would be involved.\n",
            "hyp1: Studying days in which this House would be involved is one of the things we are thinking of doing.\n",
            "hyp2: Studying days and months in which this House would be involved is one of the things we are thinking of doing.\n",
            "label: [/INST] \n",
            " hyp2\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The fact is that a key omission from the proposals on agricultural policy in Agenda 2000 is a chapter on renewable energy.\n",
            "hyp1: Agenda 2030 does not include a chapter on renewable energy.\n",
            "hyp2: Agenda 2000 does not include a chapter on renewable energy.\n",
            "label: [/INST] \n",
            " hyp1\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: This is a very important issue, because we have no other way of cleaning up our beaches, especially those on the North Sea and Baltic coasts.\n",
            "hyp1: There is no other way to clean up the beaches on the North Sea and Baltic coast.\n",
            "hyp2: There is no other way to clean up the beaches on the North Sea and  the Bothnian Bay coastline.\n",
            "label: [/INST] \n",
            " hyp2\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The population has declined in some 210 of the 280 municipalities in Sweden, mainly in inland central and northern Sweden.\n",
            "hyp1: In the majority of Sweden's 280 municipalities, the population has gone up.\n",
            "hyp2: In the majority of Sweden's 280 municipalities, the population has gone down.\n",
            "label: [/INST] \n",
            " hyp1\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Mr President, I did prepare a speech but I have left it aside because many of my points have already been excellently made by previous speakers.\n",
            "hyp1: Many of the points they were going to make in their speech were already made by previous speakers, so they left it aside.\n",
            "hyp2: Many of the points I was going to make in my speech were already made by previous speakers, so I left it aside.\n",
            "label: [/INST] \n",
            " hyp1\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The European Commission proposes that this information should enter into force within a period of three years from 1 July 1998.\n",
            "hyp1: The EU wants this information to enter into force in three years.\n",
            "hyp2: The EU wants this information to enter into force in thirty years.\n",
            "label: [/INST] \n",
            " hyp2\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Amendment No 1 in the French version deletes illegal immigration and Amendment No 4 omits the expression 'police authorities'.\n",
            "hyp1: The French version excludes the expression'police authorities'.\n",
            "hyp2: The French version excludes the expression 'police authorities' from Amendment No 4.\n",
            "label: [/INST] \n",
            " hyp1\n",
            "</s> \n",
            "\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(few_shot_samples_sv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3ye1cEtsOIw",
        "outputId": "d30df782-181f-4b87-8a53-99acd6941058"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Counties with population declines will be Vermillion, Posey and Madison.\n",
            "hyp1: Vermillion, Posey and Madison are counties that will experience declining populations.\n",
            "hyp2: Vermillion, Posey and Marion are counties that will experience declining populations.\n",
            "label: [/INST] \n",
            " hyp2\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Israel's Peres calls on the parties to return to peace talks.\n",
            "hyp1: Peres in Israel calls for return to peace talks.\n",
            "hyp2: Israeli President Shimon Peres on Monday called on the world community to resume peace talks with the Palestinians, stressing the importance of a two-state solution.\n",
            "label: [/INST] \n",
            " hyp2\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Greek far-right leader jailed awaiting trial.\n",
            "hyp1: Pending trial, a Greek far-right leader has been imprisoned.\n",
            "hyp2: A Greek far-right leader has been arrested and will stand trial, accused of organizing violent protests in recent years. The leader, who goes by the name Nikos Michaloliakos, is a former member of the Greek nationalist party Golden Dawn. He is also suspected of having been involved in the murder of an anti-racism-.\n",
            "label: [/INST] \n",
            " hyp2\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: North Korea carves out foreigners in the South and advises evacuation.\n",
            "hyp1: North Korea does not warn foreign nationals in the southern parts of the country and advise evacuation.\n",
            "hyp2: North Korea warns foreign nationals in southern parts of the country and advises evacuation.\n",
            "label: [/INST] \n",
            " hyp1\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: We have met, but you don't remember me. I worked for a company you hired to get your memory erased.\n",
            "hyp1: We've met before, but you don't remember me. You were employed by a company I hired to erase my memory.\n",
            "hyp2: We've met before, but you don't remember me. I was employed by a company you hired to erase your memory.\n",
            "label: [/INST] \n",
            " hyp1\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Google presents a prototype for a self-driving car.\n",
            "hyp1: A prototype for a self-driving car presented by Google.\n",
            "hyp2: According to a blog post from Google, the company has developed a prototype of a self-driving car that can drive completely on its own. The car, called \"Waymo One\", has been tested in the Nevada desert and has shown that it can navigate complex traffic environments without the need for any human supervision.\n",
            "label: [/INST] \n",
            " hyp2\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Iran's nuclear negotiations enter third day.\n",
            "hyp1: Discussions about Iran's nuclear weapons program have reached their third day.\n",
            "hyp2: Discussions about Iran's nuclear weapons program have reached their fourth day.\n",
            "label: [/INST] \n",
            " hyp2\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: But revenue from software licenses, a metric watched closely by financial analysts, fell 21 percent to $107.6 million.\n",
            "hyp1: Revenue from software licenses, a metric closely watched by financial analysts, fell 21 percent to $107.6 million.\n",
            "hyp2: Revenue from software licenses, a metric closely watched by financial analysts, fell 42 percent to $107.6 million.\n",
            "label: [/INST] \n",
            " hyp2\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The Iranian government said: Iran's nuclear program remains peaceful.\n",
            "hyp1: The government of Iran declared that the nation's nuclear energy program will remain peaceful.\n",
            "hyp2: The government of Iran declared that the nation's nuclear energy program used to be peaceful.\n",
            "label: [/INST] \n",
            " hyp2\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: After protesters rushed the stage and twice cut power to the microphone, Hedges ended the speech early.\n",
            "hyp1: As protesters stormed the stage and twice cut off the power to the microphone, Hedges ended his speech prematurely.\n",
            "hyp2: As protesters stormed the stage and five times cut off the power to the microphone, Hedges ended his speech prematurely.\n",
            "label: [/INST] \n",
            " hyp2\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Spain's princess testifies in historic fraud inquiry.\n",
            "hyp1: The Spanish princess becomes part of an investigation into historical fraud.\n",
            "hyp2: The Spanish prince becomes part of an investigation into historical fraud.\n",
            "label: [/INST] \n",
            " hyp2\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The Federal Trade Commission (FTC) today asked Congress for additional authority to combat unwanted Internet spam, which now accounts for up to half of all email traffic.\n",
            "hyp1: The Federal Trade Commission (FTC) today requested additional authority from the House of Commons to combat unwanted Internet spam, which now accounts for up to half of all email traffic.\n",
            "hyp2: The Federal Trade Commission (FTC) today requested additional authority from Congress to combat unwanted Internet spam, which now accounts for up to half of all email traffic.\n",
            "label: [/INST] \n",
            " hyp1\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Driver in Spanish train accident questioned by judge.\n",
            "hyp1: According to reports from El mundo, a judge at the Court of Andalusia has begun an investigation after receiving a request for information from the Spanish authorities regarding the crash of a high-speed train between Madrid and Seville last week. According to reports, 77 people were on board the train when it derailed. In a statement on its website, the court said:\n",
            "hyp2: Driver in Spanish train accident questioned by judge.\n",
            "label: [/INST] \n",
            " hyp1\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The hearing took place a day after the Pentagon for the first time singled out an officer, Dallager, for failing to address the scandal.\n",
            "hyp1: The hearing took place a day after the FBI publicly identified an officer, Dallager, for the first time and claimed he had failed to fix the scandal.\n",
            "hyp2: The hearing came a day after the Pentagon publicly identified an officer, Dallager, for the first time and claimed he had failed to address the scandal.\n",
            "label: [/INST] \n",
            " hyp1\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Hong Kong University collaborates with universities, enterprises and government sectors in China to coordinate educational programs and research centers to promote high-tech research, commercialization and technology transfer.\n",
            "hyp1: The University of Hong Kong collaborates with various Chinese universities, companies and government departments to coordinate educational programs and research centers with the aim of promoting advanced technological research, commercialization and technology transfer.\n",
            "hyp2: The University of Hong Kong collaborates with various Chinese universities, enterprises and government sectors to coordinate educational programs and research centers with the aim of promoting advanced technological research, commercialization and technology transfer.\n",
            "label: [/INST] \n",
            " hyp1\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Their leader, Abu Bakr al-Azdi, surrendered himself in June; his deputy was killed in a recent shootout with Saudi forces.\n",
            "hyp1: Abu Bakr al-Azdi, the group's leader, had recently surrendered himself on June 16, 2015, while his deputy was recently killed in an encounter with Saudi forces.\n",
            "hyp2: Abu Bakr al-Azdi, the group's leader, had recently surrendered himself in June, while his deputy was recently killed in an encounter with Saudi forces.\n",
            "label: [/INST] \n",
            " hyp1\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The man uses a sledgehammer to break the concrete block that is on the other man.\n",
            "hyp1: The man uses a sledgehammer to break the concrete block that is on top of the other man.\n",
            "hyp2: The man uses a sledgehammer to break the concrete block that is under the other man.\n",
            "label: [/INST] \n",
            " hyp2\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The bill states that a woman undergoing such an abortion could not be prosecuted.\n",
            "hyp1: The proposal stipulates that a woman who undergoes such an abortion can be prosecuted.\n",
            "hyp2: The proposal stipulates that a woman who undergoes such an abortion cannot be prosecuted.\n",
            "label: [/INST] \n",
            " hyp1\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Gunman among 7 dead after apartment shooting in Florida.\n",
            "hyp1: A gunman was among the seven dead after a Florida apartment shooting.\n",
            "hyp2: A gunman has killed at least seven people and injured several others after opening fire at an apartment in Fort Lauderdale, Florida, early this morning local time. According to local authorities, the victims of the shooting were from different parts of the country. The suspected shooter has been arrested. It is currently unclear what motivated the act.\n",
            "label: [/INST] \n",
            " hyp2\n",
            "</s> \n",
            "\n",
            " <s>[INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: AstraZeneca pays $4.1 billion to buy Bristol-Myers Squibb out of diabetes alliance.\n",
            "hyp1: AstraZeneca has agreed to transfer $4.1 billion to Bristol-Myers Squibb in exchange for control of their diabetes care collaboration.\n",
            "hyp2: AstraZeneca has agreed to transfer $4.1 billion to Bristol-Myers Squibb to buy the company out of the diabetes alliance.\n",
            "label: [/INST] \n",
            " hyp1\n",
            "</s> \n",
            "\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run! (English test set)"
      ],
      "metadata": {
        "id": "r6Cqo0JqotN9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKo1-X5OvT4b",
        "outputId": "240fa395-6168-4b84-e895-c76ffe56d1e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.89 ms /    16 runs   (    0.49 ms per token,  2028.65 tokens per second)\n",
            "llama_print_timings: prompt eval time =   71482.99 ms /    85 tokens (  840.98 ms per token,     1.19 tokens per second)\n",
            "llama_print_timings:        eval time =    1574.25 ms /    15 runs   (  104.95 ms per token,     9.53 tokens per second)\n",
            "llama_print_timings:       total time =    2264.06 ms /   100 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: It has enabled us to support and encourage an exchange of experiences and to pursue activities to raise the level of competence throughout Europe.\n",
            "hyp1: You can support and encourage an exchange of experiences to raise the level of competence in Europe.\n",
            "hyp2: We can support and encourage an exchange of experiences to raise the level of competence in Europe.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.60 ms /    16 runs   (    0.48 ms per token,  2104.43 tokens per second)\n",
            "llama_print_timings: prompt eval time =     305.73 ms /    95 tokens (    3.22 ms per token,   310.73 tokens per second)\n",
            "llama_print_timings:        eval time =    1645.82 ms /    15 runs   (  109.72 ms per token,     9.11 tokens per second)\n",
            "llama_print_timings:       total time =    2282.81 ms /   110 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Therefore, I am calling for an increase in the premiums for all varieties of leaf tobacco for the 1999, 2000 and 2001 harvests.\n",
            "hyp1: I want to see a decrease in the premiums for all varieties of leaf tobacco.\n",
            "hyp2: I want to see an increase in the premiums for all varieties of leaf tobacco.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.00 ms /    16 runs   (    0.56 ms per token,  1777.38 tokens per second)\n",
            "llama_print_timings: prompt eval time =     304.41 ms /   105 tokens (    2.90 ms per token,   344.93 tokens per second)\n",
            "llama_print_timings:        eval time =    1756.98 ms /    15 runs   (  117.13 ms per token,     8.54 tokens per second)\n",
            "llama_print_timings:       total time =    2457.46 ms /   120 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: In other words, a person may be prevented from coming near their victim when there is reason to fear that violent acts will be carried out again.\n",
            "hyp1: When there is reason to fear that a violent act will be carried out again, a person may not be allowed to come near their victim.\n",
            "hyp2: When there is reason to fear that a violent act will be carried out again, a person may not be allowed to come near her victim.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =      11.37 ms /    16 runs   (    0.71 ms per token,  1407.58 tokens per second)\n",
            "llama_print_timings: prompt eval time =     284.73 ms /    68 tokens (    4.19 ms per token,   238.82 tokens per second)\n",
            "llama_print_timings:        eval time =    1882.61 ms /    15 runs   (  125.51 ms per token,     7.97 tokens per second)\n",
            "llama_print_timings:       total time =    2594.71 ms /    83 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: None of my 34 amendments were adopted and my arguments against the ridiculous administrative burden proposed were not heeded.\n",
            "hyp1: Our arguments against the ridiculous administrative burden were not heard.\n",
            "hyp2: My arguments against the ridiculous administrative burden were not heard.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.52 ms /    16 runs   (    0.53 ms per token,  1878.15 tokens per second)\n",
            "llama_print_timings: prompt eval time =     288.32 ms /    89 tokens (    3.24 ms per token,   308.68 tokens per second)\n",
            "llama_print_timings:        eval time =    1695.82 ms /    15 runs   (  113.05 ms per token,     8.85 tokens per second)\n",
            "llama_print_timings:       total time =    2305.82 ms /   104 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The final key feature of the report is its orientation. It aims to protest against certain drifts that are becoming evident, and I will mention two instances.\n",
            "hyp1: The final main feature of the report is its orientation protesting against drifts, and I will provide two instances of that.\n",
            "hyp2: The final features of the report are its orientation and two instances.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.66 ms /    16 runs   (    0.60 ms per token,  1656.14 tokens per second)\n",
            "llama_print_timings: prompt eval time =     293.50 ms /   106 tokens (    2.77 ms per token,   361.16 tokens per second)\n",
            "llama_print_timings:        eval time =    1571.48 ms /    15 runs   (  104.77 ms per token,     9.55 tokens per second)\n",
            "llama_print_timings:       total time =    2257.44 ms /   121 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: It is high time an appropriate level of social protection was created at European level for those whose work does not fit into the usual pattern.\n",
            "hyp1: It's time for an appropriate level of social protection for those whose work doesn't fit into the usual pattern.\n",
            "hyp2: It's time for an appropriate level of social protection for those whose work doesn't fit into the usual pattern, such as firefighters or security officers. \n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.11 ms /    16 runs   (    0.57 ms per token,  1756.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =     311.66 ms /    94 tokens (    3.32 ms per token,   301.61 tokens per second)\n",
            "llama_print_timings:        eval time =    1562.39 ms /    15 runs   (  104.16 ms per token,     9.60 tokens per second)\n",
            "llama_print_timings:       total time =    2187.09 ms /   109 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: These sums, however, have still not produced the desired effect: the substantial fall in the average earnings of the Palestinians is proof of this.\n",
            "hyp1: The fall in average earnings of Egyptians is proof that these sums have not produced the desired effect.\n",
            "hyp2: The fall in average earnings of Palestinians is proof that these sums have not produced the desired effect.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =      11.58 ms /    16 runs   (    0.72 ms per token,  1381.57 tokens per second)\n",
            "llama_print_timings: prompt eval time =     326.22 ms /    93 tokens (    3.51 ms per token,   285.09 tokens per second)\n",
            "llama_print_timings:        eval time =    1778.61 ms /    15 runs   (  118.57 ms per token,     8.43 tokens per second)\n",
            "llama_print_timings:       total time =    2728.40 ms /   108 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Mr President, the approach adopted by the rapporteur to the Commission's 1999 annual economic report is comprehensive and also sensible.\n",
            "hyp1: The approach taken by the rapporteur to the 1997 annual economic report is comprehensive and sensible.\n",
            "hyp2: The approach taken by the rapporteur to the 1999 annual economic report is comprehensive and sensible.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.39 ms /    16 runs   (    0.52 ms per token,  1907.71 tokens per second)\n",
            "llama_print_timings: prompt eval time =     278.73 ms /    75 tokens (    3.72 ms per token,   269.07 tokens per second)\n",
            "llama_print_timings:        eval time =    1533.45 ms /    15 runs   (  102.23 ms per token,     9.78 tokens per second)\n",
            "llama_print_timings:       total time =    2076.46 ms /    90 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The Berlin summit, which will focus solely on Agenda 2000, is of particular importance for the future of the European Union.\n",
            "hyp1: The future of the European Union is important to the Berlin summit.\n",
            "hyp2: The Berlin summit is important to the future of the European Union.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.77 ms /    16 runs   (    0.55 ms per token,  1825.44 tokens per second)\n",
            "llama_print_timings: prompt eval time =     290.76 ms /   106 tokens (    2.74 ms per token,   364.56 tokens per second)\n",
            "llama_print_timings:        eval time =    1628.39 ms /    15 runs   (  108.56 ms per token,     9.21 tokens per second)\n",
            "llama_print_timings:       total time =    2298.88 ms /   121 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: As a result, I cannot promise the chairman of the Committee on Budgets that there will be 100 % funding for the financial programme.\n",
            "hyp1: I can't promise that there will be 100 percent funding for the financial program.\n",
            "hyp2: Because of this, it's not possible for me to promise the chairman of the Committee on Budgets that there would be 210 % funding for the financial programme.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.68 ms /    16 runs   (    0.54 ms per token,  1843.96 tokens per second)\n",
            "llama_print_timings: prompt eval time =     290.73 ms /   111 tokens (    2.62 ms per token,   381.79 tokens per second)\n",
            "llama_print_timings:        eval time =    1659.05 ms /    15 runs   (  110.60 ms per token,     9.04 tokens per second)\n",
            "llama_print_timings:       total time =    2317.62 ms /   126 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: We voted for Amendments Nos 22 to 25, even though they do not take adequate measures to prevent these problems from happening.\n",
            "hyp1: Amendments Nos 22 to 25 didn't take enough measures to prevent problems from happening, so we voted for them.\n",
            "hyp2: Amendments Nos 22 to 25 didn't take enough measures to prevent problems from happening, yet we voted for them.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.98 ms /    16 runs   (    0.50 ms per token,  2005.01 tokens per second)\n",
            "llama_print_timings: prompt eval time =     332.94 ms /   101 tokens (    3.30 ms per token,   303.36 tokens per second)\n",
            "llama_print_timings:        eval time =    1648.07 ms /    15 runs   (  109.87 ms per token,     9.10 tokens per second)\n",
            "llama_print_timings:       total time =    2579.74 ms /   116 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Secondly, we say that the type approval rules as from 2005 must provide in precise terms that new cars must be recycling-friendly.\n",
            "hyp1: The type approval rules from 2005 must give precise terms for new cars to be recycling-friendly.\n",
            "hyp2: The type approval rules as of 2005 must give precise terms for new cars to be recycling-friendly.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.50 ms /    16 runs   (    0.47 ms per token,  2132.76 tokens per second)\n",
            "llama_print_timings: prompt eval time =     282.28 ms /    91 tokens (    3.10 ms per token,   322.37 tokens per second)\n",
            "llama_print_timings:        eval time =    1546.07 ms /    15 runs   (  103.07 ms per token,     9.70 tokens per second)\n",
            "llama_print_timings:       total time =    2155.78 ms /   106 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: B4-0188/99 by Mrs d'Ancona, on behalf of the PSE Group, on the death penalty against Greg Summers - Texas, USA; Leonard Peltier\n",
            "hyp1: The death penalty was imposed on Gregory Winters in Texas, USA.\n",
            "hyp2: The death penalty was imposed on Greg Summers in Texas, USA.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.84 ms /    16 runs   (    0.55 ms per token,  1809.14 tokens per second)\n",
            "llama_print_timings: prompt eval time =     302.19 ms /   116 tokens (    2.61 ms per token,   383.86 tokens per second)\n",
            "llama_print_timings:        eval time =    1656.36 ms /    15 runs   (  110.42 ms per token,     9.06 tokens per second)\n",
            "llama_print_timings:       total time =    2364.10 ms /   131 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: We struggle with water on a daily basis in the Netherlands - in the polders, the delta where the Meuse, the Rhine and the Scheldt flow into the sea.\n",
            "hyp1: In the Netherlands, we struggle with water on a daily basis because of the Meuse, Rhine, Scheldt, Noord, Voer and Dieze\n",
            "hyp2: In the Netherlands, we struggle with water on a daily basis because of the Meuse, Rhine and Scheldt.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.67 ms /    16 runs   (    0.54 ms per token,  1845.44 tokens per second)\n",
            "llama_print_timings: prompt eval time =     272.03 ms /    76 tokens (    3.58 ms per token,   279.38 tokens per second)\n",
            "llama_print_timings:        eval time =    1881.85 ms /    15 runs   (  125.46 ms per token,     7.97 tokens per second)\n",
            "llama_print_timings:       total time =    2442.03 ms /    91 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: It may lower costs by 10 % and is, of course, only applicable over the shorter road legs of the combined transport journey.\n",
            "hyp1: It is only applicable to the short road legs of the combined transport journey.\n",
            "hyp2: It is applicable to the road legs of the combined transport journey.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.12 ms /    16 runs   (    0.51 ms per token,  1971.41 tokens per second)\n",
            "llama_print_timings: prompt eval time =     282.01 ms /    78 tokens (    3.62 ms per token,   276.58 tokens per second)\n",
            "llama_print_timings:        eval time =    1576.98 ms /    15 runs   (  105.13 ms per token,     9.51 tokens per second)\n",
            "llama_print_timings:       total time =    2130.59 ms /    93 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Nothing has been done in the 27 years since then, while we have liberalised sea transport, air transport and road transport.\n",
            "hyp1: Since then, we have restricted air transport, road transport and sea transport.\n",
            "hyp2: Since then, we have liberalised air transport, road transport and sea transport.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.77 ms /    16 runs   (    0.49 ms per token,  2059.73 tokens per second)\n",
            "llama_print_timings: prompt eval time =     270.07 ms /    80 tokens (    3.38 ms per token,   296.22 tokens per second)\n",
            "llama_print_timings:        eval time =    1574.95 ms /    15 runs   (  105.00 ms per token,     9.52 tokens per second)\n",
            "llama_print_timings:       total time =    2123.34 ms /    95 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: This concerns the applicant countries which are still in the process of membership negotiations, but which in the meantime are being discriminated against.\n",
            "hyp1: This is about the countries that are still in membership negotiations and are also being discriminated against.\n",
            "hyp2: The countries that are still in membership negotiations are being discriminated against.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.00 ms /    16 runs   (    0.50 ms per token,  2000.75 tokens per second)\n",
            "llama_print_timings: prompt eval time =     290.54 ms /   105 tokens (    2.77 ms per token,   361.40 tokens per second)\n",
            "llama_print_timings:        eval time =    1560.41 ms /    15 runs   (  104.03 ms per token,     9.61 tokens per second)\n",
            "llama_print_timings:       total time =    2194.69 ms /   120 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: I would be grateful if you could tell me whether or not Rambouillet was a European Union initiative or a Franco-British initiative, as I did not quite understand this point.\n",
            "hyp1: I need to know if Rambouillet is a European Union initiative or a Franco-British initiative.\n",
            "hyp2: We need to know if Rambouillet is a European Union initiative or a Franco-British initiative.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.21 ms /    16 runs   (    0.58 ms per token,  1736.30 tokens per second)\n",
            "llama_print_timings: prompt eval time =     287.75 ms /    96 tokens (    3.00 ms per token,   333.62 tokens per second)\n",
            "llama_print_timings:        eval time =    1915.96 ms /    15 runs   (  127.73 ms per token,     7.83 tokens per second)\n",
            "llama_print_timings:       total time =    2564.98 ms /   111 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Madam President, I referred earlier to inherited inertia, and this leads me to the third group of amendments, to which Mr Fabre-Aubrespy referred.\n",
            "hyp1: The third group of amendments was referred to by Ms Fabre- Aubrespy.\n",
            "hyp2: The third group of amendments was referred to by Mr Fabre- Aubrespy.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.29 ms /    16 runs   (    0.46 ms per token,  2194.49 tokens per second)\n",
            "llama_print_timings: prompt eval time =     299.20 ms /    74 tokens (    4.04 ms per token,   247.32 tokens per second)\n",
            "llama_print_timings:        eval time =    1468.04 ms /    15 runs   (   97.87 ms per token,    10.22 tokens per second)\n",
            "llama_print_timings:       total time =    2018.31 ms /    89 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Other elements have contributed to the proposal to conclude this partnership agreement, especially the very worrying regional context in Central Asia.\n",
            "hyp1: There are other elements that contributed to the proposal to conclude the partnership agreement.\n",
            "hyp2: There are no other elements that contributed to the proposal to conclude the partnership agreement.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.44 ms /    16 runs   (    0.53 ms per token,  1896.41 tokens per second)\n",
            "llama_print_timings: prompt eval time =     415.11 ms /   150 tokens (    2.77 ms per token,   361.35 tokens per second)\n",
            "llama_print_timings:        eval time =    1603.35 ms /    15 runs   (  106.89 ms per token,     9.36 tokens per second)\n",
            "llama_print_timings:       total time =    2486.90 ms /   165 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: A way must also be found to tax all kinds of cross-border capital movements which are carried out for profit, in order to put a brake, albeit in a limited way, on the increase in the volume of parasitic, speculative capital.\n",
            "hyp1: To put a brake on the increase in the volume of speculative capital, which is often regulated under the Tobin Tax, a way must be found to tax cross-border capital movements carried out for profit.\n",
            "hyp2: To put a brake on the increase in the volume of speculative capital, a way must be found to tax cross-border capital movements carried out for profit.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.75 ms /    16 runs   (    0.48 ms per token,  2065.58 tokens per second)\n",
            "llama_print_timings: prompt eval time =     275.41 ms /    86 tokens (    3.20 ms per token,   312.27 tokens per second)\n",
            "llama_print_timings:        eval time =    1519.99 ms /    15 runs   (  101.33 ms per token,     9.87 tokens per second)\n",
            "llama_print_timings:       total time =    2088.52 ms /   101 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: An important report was also put together under the leadership of Susan Waddington extending the debate to cover the issue of the trade in women.\n",
            "hyp1: Susan Waddington extended the debate on the trade in women to include an important report.\n",
            "hyp2: Susan Waddington extended the debate on the trade in women by putting together a critical report. \n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =      12.61 ms /    16 runs   (    0.79 ms per token,  1268.73 tokens per second)\n",
            "llama_print_timings: prompt eval time =     268.51 ms /    69 tokens (    3.89 ms per token,   256.97 tokens per second)\n",
            "llama_print_timings:        eval time =    1903.24 ms /    15 runs   (  126.88 ms per token,     7.88 tokens per second)\n",
            "llama_print_timings:       total time =    2590.83 ms /    84 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Over recent years, these have largely been behind the development of a significant number of economic policies, which is good for employment.\n",
            "hyp1: A lot of economic policies are good for employment. \n",
            "hyp2: A lot of economic policies are good for employment because of these.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.73 ms /    16 runs   (    0.48 ms per token,  2070.93 tokens per second)\n",
            "llama_print_timings: prompt eval time =     309.04 ms /    85 tokens (    3.64 ms per token,   275.04 tokens per second)\n",
            "llama_print_timings:        eval time =    1582.41 ms /    15 runs   (  105.49 ms per token,     9.48 tokens per second)\n",
            "llama_print_timings:       total time =    2201.57 ms /   100 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The presidency is very much of this view, as economic and political stability in Jordan is a crucial factor in peace-keeping in the Middle East.\n",
            "hyp1: Peace-keeping in the Middle East depends on economic and political stability in the north of Jordan.\n",
            "hyp2: Peace-keeping in the Middle East depends on economic and political stability in Jordan.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.83 ms /    16 runs   (    0.49 ms per token,  2044.47 tokens per second)\n",
            "llama_print_timings: prompt eval time =     287.86 ms /   105 tokens (    2.74 ms per token,   364.77 tokens per second)\n",
            "llama_print_timings:        eval time =    1590.37 ms /    15 runs   (  106.02 ms per token,     9.43 tokens per second)\n",
            "llama_print_timings:       total time =    2241.43 ms /   120 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: In addition, the Commission is unable to accept Amendments Nos 27 and 28, because they go beyond the scope of this programme.\n",
            "hyp1: The Commission can't accept Amendments 27 and 28 because they go past the scope of the programme.\n",
            "hyp2: The Commission can't accept Amendments 27 and 28 even though they go past the scope of the programme.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       6.94 ms /    14 runs   (    0.50 ms per token,  2017.29 tokens per second)\n",
            "llama_print_timings: prompt eval time =     287.69 ms /    95 tokens (    3.03 ms per token,   330.21 tokens per second)\n",
            "llama_print_timings:        eval time =    1350.98 ms /    13 runs   (  103.92 ms per token,     9.62 tokens per second)\n",
            "llama_print_timings:       total time =    1976.52 ms /   108 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: On the contrary, things came to a head in the crisis and we now know for certain that this Augean stable at any rate is going to be cleaned out.\n",
            "hyp1: The crisis came to a head and we now know that the Hercules stable will be cleaned out.\n",
            "hyp2: The crisis came to a head and we now know that the Augean stable will be cleaned out.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =      10.48 ms /    16 runs   (    0.66 ms per token,  1526.43 tokens per second)\n",
            "llama_print_timings: prompt eval time =     280.73 ms /    85 tokens (    3.30 ms per token,   302.78 tokens per second)\n",
            "llama_print_timings:        eval time =    1878.32 ms /    15 runs   (  125.22 ms per token,     7.99 tokens per second)\n",
            "llama_print_timings:       total time =    2645.18 ms /   100 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: As you will remember, on 30 October 1997, the Commission presented a Community initiative for the European Capital of Culture.\n",
            "hyp1: The European Capital of Culture community initiative was presented by the Commission in 1997.\n",
            "hyp2: The European Capital of Culture was presented by the Commission in 1997.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.93 ms /    16 runs   (    0.50 ms per token,  2018.42 tokens per second)\n",
            "llama_print_timings: prompt eval time =     287.19 ms /    83 tokens (    3.46 ms per token,   289.00 tokens per second)\n",
            "llama_print_timings:        eval time =    1611.34 ms /    15 runs   (  107.42 ms per token,     9.31 tokens per second)\n",
            "llama_print_timings:       total time =    2250.75 ms /    98 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: A second point on the timing: this Committee of Experts will make its report available - if I recall correctly - on 15 March.\n",
            "hyp1: If I remember correctly, the report will be made available on March 15.\n",
            "hyp2: If I remember correctly, the report will be made available on March 17.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.83 ms /    16 runs   (    0.49 ms per token,  2044.21 tokens per second)\n",
            "llama_print_timings: prompt eval time =     281.50 ms /    85 tokens (    3.31 ms per token,   301.96 tokens per second)\n",
            "llama_print_timings:        eval time =    1569.03 ms /    15 runs   (  104.60 ms per token,     9.56 tokens per second)\n",
            "llama_print_timings:       total time =    2136.14 ms /   100 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: There are four points in relation to which the Committee on Agriculture and Rural Development has made changes which we believe will be adopted.\n",
            "hyp1: The Committee on Agriculture and Rural Development made changes that we think will be adopted.\n",
            "hyp2: The Committee on Agriculture and Rural Development did not make changes that we think will be adopted.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.85 ms /    16 runs   (    0.49 ms per token,  2037.44 tokens per second)\n",
            "llama_print_timings: prompt eval time =     270.05 ms /    73 tokens (    3.70 ms per token,   270.32 tokens per second)\n",
            "llama_print_timings:        eval time =    1526.21 ms /    15 runs   (  101.75 ms per token,     9.83 tokens per second)\n",
            "llama_print_timings:       total time =    2056.80 ms /    88 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: In the Nordic countries there are excellent examples of this, and for that reason I recommend that Amendment No 1 be rejected.\n",
            "hyp1: There are excellent examples of this in the Nordic countries.\n",
            "hyp2: There are excellent examples of this Amendment in the Nordic countries.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.82 ms /    16 runs   (    0.61 ms per token,  1629.99 tokens per second)\n",
            "llama_print_timings: prompt eval time =     268.74 ms /    71 tokens (    3.79 ms per token,   264.19 tokens per second)\n",
            "llama_print_timings:        eval time =    1868.38 ms /    15 runs   (  124.56 ms per token,     8.03 tokens per second)\n",
            "llama_print_timings:       total time =    2579.56 ms /    86 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: It is we politicians who are most concerned, because we understand the link between energy consumption and CO2 emissions.\n",
            "hyp1: Politicians understand the link between CO2 emissions and energy consumption.\n",
            "hyp2: Politicians don't understand the link between CO2 emissions and energy consumption.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.05 ms /    16 runs   (    0.50 ms per token,  1988.32 tokens per second)\n",
            "llama_print_timings: prompt eval time =     298.61 ms /    80 tokens (    3.73 ms per token,   267.91 tokens per second)\n",
            "llama_print_timings:        eval time =    1679.07 ms /    15 runs   (  111.94 ms per token,     8.93 tokens per second)\n",
            "llama_print_timings:       total time =    2257.16 ms /    95 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Having pressed the Commission to present this proposal, the Socialist Group - to which I have the privilege of belonging - now supports it enthusiastically.\n",
            "hyp1: The Communist Group now supports the proposal after pressing the Commission to present it.\n",
            "hyp2: The Socialist Group now supports the proposal after pressing the Commission to present it.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.25 ms /    16 runs   (    0.52 ms per token,  1939.86 tokens per second)\n",
            "llama_print_timings: prompt eval time =     293.39 ms /    77 tokens (    3.81 ms per token,   262.45 tokens per second)\n",
            "llama_print_timings:        eval time =    1566.62 ms /    15 runs   (  104.44 ms per token,     9.57 tokens per second)\n",
            "llama_print_timings:       total time =    2121.15 ms /    92 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: For example, unemployment is very high in Réunion and in the West Indies. Therefore, we should not devote our efforts to integrating unlimited numbers of immigrants.\n",
            "hyp1: The West Indies and Réunion have low unemployment.\n",
            "hyp2: The West Indies and Réunion have high unemployment.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.57 ms /    16 runs   (    0.54 ms per token,  1866.54 tokens per second)\n",
            "llama_print_timings: prompt eval time =     277.22 ms /    81 tokens (    3.42 ms per token,   292.19 tokens per second)\n",
            "llama_print_timings:        eval time =    1744.02 ms /    15 runs   (  116.27 ms per token,     8.60 tokens per second)\n",
            "llama_print_timings:       total time =    2324.43 ms /    96 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: A uniform phased reduction of 3 % will primarily penalise the sectors of production which receive the most aid, such as tobacco growing.\n",
            "hyp1: Tobacco growing will benefit by a 3 % phased reduction in aid.\n",
            "hyp2: Tobacco growing will be affected by a 3 % phased reduction in aid.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.20 ms /    16 runs   (    0.57 ms per token,  1739.32 tokens per second)\n",
            "llama_print_timings: prompt eval time =     310.49 ms /    95 tokens (    3.27 ms per token,   305.97 tokens per second)\n",
            "llama_print_timings:        eval time =    2192.00 ms /    15 runs   (  146.13 ms per token,     6.84 tokens per second)\n",
            "llama_print_timings:       total time =    3053.85 ms /   110 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The situation in India is appalling as 50 million women are missing because they are eliminated right from the stage of conception.\n",
            "hyp1: 50 million women are missing in India due to the fact that they are eliminated before they are conceived.\n",
            "hyp2: 50 million women are missing in India due to the fact that they are eliminated directly after they have been conceived.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.45 ms /    16 runs   (    0.59 ms per token,  1692.41 tokens per second)\n",
            "llama_print_timings: prompt eval time =     286.05 ms /    82 tokens (    3.49 ms per token,   286.67 tokens per second)\n",
            "llama_print_timings:        eval time =    1655.85 ms /    15 runs   (  110.39 ms per token,     9.06 tokens per second)\n",
            "llama_print_timings:       total time =    2230.55 ms /    97 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Nevertheless, we want to limit the Council regulations to general provisions and to cover the remaining provisions in implementing regulations.\n",
            "hyp1: We want the regulations to only cover general provisions and the rest of the regulations.\n",
            "hyp2: We want the Council's regulations to only cover general provisions while the rest of the provisions should be covered by implementing regulations\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.95 ms /    16 runs   (    0.50 ms per token,  2011.57 tokens per second)\n",
            "llama_print_timings: prompt eval time =     289.31 ms /    92 tokens (    3.14 ms per token,   318.00 tokens per second)\n",
            "llama_print_timings:        eval time =    1621.86 ms /    15 runs   (  108.12 ms per token,     9.25 tokens per second)\n",
            "llama_print_timings:       total time =    2242.85 ms /   107 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: I hope that the Euro-Mediterranean Conference to held in Stuttgart between 4 and 6 April will enable us to make progress in that direction.\n",
            "hyp1: I hope the Euro-Mediterranean Conference will help us make progress in that direction.\n",
            "hyp2: I hope the Euro-Mediterranean Conference will help you make progress in that direction.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.77 ms /    16 runs   (    0.55 ms per token,  1825.03 tokens per second)\n",
            "llama_print_timings: prompt eval time =     279.66 ms /    88 tokens (    3.18 ms per token,   314.67 tokens per second)\n",
            "llama_print_timings:        eval time =    1847.70 ms /    15 runs   (  123.18 ms per token,     8.12 tokens per second)\n",
            "llama_print_timings:       total time =    2509.04 ms /   103 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: In 1996, the Commission submitted its proposal to have Turkey included in the Socrates, Youth for Europe and Leonardo programmes.\n",
            "hyp1: Turkey was included in the Commission's proposal in 1996.\n",
            "hyp2: Turkey was included in the Commission's proposal for three different programmes in 1996.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.97 ms /    16 runs   (    0.50 ms per token,  2008.28 tokens per second)\n",
            "llama_print_timings: prompt eval time =     321.58 ms /   106 tokens (    3.03 ms per token,   329.62 tokens per second)\n",
            "llama_print_timings:        eval time =    1515.97 ms /    15 runs   (  101.06 ms per token,     9.89 tokens per second)\n",
            "llama_print_timings:       total time =    2179.53 ms /   121 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: We cannot accept Amendments Nos 19, 21 and 51, which are aimed at regulating the import and export of genetically modified organisms.\n",
            "hyp1: Amendments Nos 21 and 23 are aimed at regulating genetically modified organisms.\n",
            "hyp2: Amendments Nos 19 and 21 are aimed at regulating genetically modified organisms.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.95 ms /    16 runs   (    0.50 ms per token,  2013.09 tokens per second)\n",
            "llama_print_timings: prompt eval time =     298.56 ms /    96 tokens (    3.11 ms per token,   321.54 tokens per second)\n",
            "llama_print_timings:        eval time =    2005.15 ms /    15 runs   (  133.68 ms per token,     7.48 tokens per second)\n",
            "llama_print_timings:       total time =    2635.53 ms /   111 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: I would also like to draw attention to Amendments Nos 27, 29 and 32 on public inquiries or consultation in connection with trial releases.\n",
            "hyp1: Amendments Nos 27 and 29 are about consultations related to trial releases.\n",
            "hyp2: Amendments Nos 27 and 29 are related to trial releases.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.50 ms /    16 runs   (    0.53 ms per token,  1882.57 tokens per second)\n",
            "llama_print_timings: prompt eval time =     291.76 ms /    82 tokens (    3.56 ms per token,   281.06 tokens per second)\n",
            "llama_print_timings:        eval time =    1641.84 ms /    15 runs   (  109.46 ms per token,     9.14 tokens per second)\n",
            "llama_print_timings:       total time =    2228.71 ms /    97 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: A true strategic partnership should be established with this great country of a billion inhabitants, compared with 1.2 billion in China.\n",
            "hyp1: China has 1.2 billion people, while this great country has a billion inhabitants.\n",
            "hyp2: China has 1.2 billion people, while this great country has a million inhabitants.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.55 ms /    16 runs   (    0.53 ms per token,  1870.25 tokens per second)\n",
            "llama_print_timings: prompt eval time =     310.43 ms /   105 tokens (    2.96 ms per token,   338.25 tokens per second)\n",
            "llama_print_timings:        eval time =    1753.43 ms /    15 runs   (  116.90 ms per token,     8.55 tokens per second)\n",
            "llama_print_timings:       total time =    2666.34 ms /   120 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: This is a positive development, and I can of course endorse the outcome which appears in the form of Compromise Amendments Nos 189 to 201.\n",
            "hyp1: The outcome appears in the form of Compromise Amendments Nos 189 to 201.\n",
            "hyp2: The outcome appears in the form of Compromise Amendments Nos 190 to 202.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.27 ms /    16 runs   (    0.52 ms per token,  1934.94 tokens per second)\n",
            "llama_print_timings: prompt eval time =     286.78 ms /    87 tokens (    3.30 ms per token,   303.37 tokens per second)\n",
            "llama_print_timings:        eval time =    1950.22 ms /    15 runs   (  130.01 ms per token,     7.69 tokens per second)\n",
            "llama_print_timings:       total time =    2555.99 ms /   102 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: For instance, in the summer it would perhaps be possible to set up surveillance operations to detect the flimsy craft used to make the crossing.\n",
            "hyp1: It would be possible to detect the flimsy craft that makes the crossing in the summer.\n",
            "hyp2: It would be possible to overlook the flimsy craft that makes the crossing in the summer.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =      12.50 ms /    16 runs   (    0.78 ms per token,  1280.20 tokens per second)\n",
            "llama_print_timings: prompt eval time =     301.15 ms /    83 tokens (    3.63 ms per token,   275.61 tokens per second)\n",
            "llama_print_timings:        eval time =    1592.26 ms /    15 runs   (  106.15 ms per token,     9.42 tokens per second)\n",
            "llama_print_timings:       total time =    2187.32 ms /    98 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The increase in the number of staff employed must be strictly controlled, and this is a point on which I disagree with the Committee of Wise Men.\n",
            "hyp1: I agree with the idea of controlling the increase in the number of staff.\n",
            "hyp2: I don't agree with the idea of controlling the increase in the number of staff.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.90 ms /    16 runs   (    0.56 ms per token,  1797.95 tokens per second)\n",
            "llama_print_timings: prompt eval time =     270.21 ms /    74 tokens (    3.65 ms per token,   273.87 tokens per second)\n",
            "llama_print_timings:        eval time =    1837.35 ms /    15 runs   (  122.49 ms per token,     8.16 tokens per second)\n",
            "llama_print_timings:       total time =    2382.90 ms /    89 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The public must be aware of the collective responsibility borne by the whole Commission, but also of every individual Commissioner's accountability.\n",
            "hyp1: Every individual accountability must be known by the Commissioners.\n",
            "hyp2: Every individual Commissioner's accountability must be known by the public.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.96 ms /    16 runs   (    0.50 ms per token,  2010.81 tokens per second)\n",
            "llama_print_timings: prompt eval time =     304.32 ms /    86 tokens (    3.54 ms per token,   282.60 tokens per second)\n",
            "llama_print_timings:        eval time =    1608.15 ms /    15 runs   (  107.21 ms per token,     9.33 tokens per second)\n",
            "llama_print_timings:       total time =    2216.67 ms /   101 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: We need to ensure that investigations at the external borders of the EU are effective and to investigate the main routes used for illegal activities.\n",
            "hyp1: We need to make sure that the investigations at the EU's external borders are effective.\n",
            "hyp2: We need to make sure that the investigations at the EU's external borders are not effective.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.81 ms /    16 runs   (    0.49 ms per token,  2049.18 tokens per second)\n",
            "llama_print_timings: prompt eval time =     304.73 ms /   112 tokens (    2.72 ms per token,   367.54 tokens per second)\n",
            "llama_print_timings:        eval time =    1580.85 ms /    15 runs   (  105.39 ms per token,     9.49 tokens per second)\n",
            "llama_print_timings:       total time =    2265.13 ms /   127 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: For the same reason, we cannot accept Amendments Nos 5, 6, 7, 11, 16 and 20 or the second part of Amendments Nos 9 and 15.\n",
            "hyp1: The second part of Amendments 9 and 15 cannot be accepted for the same reason.\n",
            "hyp2: The fifth part of Amendments 9 and 15 cannot be accepted for the same reason.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.97 ms /    16 runs   (    0.50 ms per token,  2008.54 tokens per second)\n",
            "llama_print_timings: prompt eval time =     290.94 ms /   101 tokens (    2.88 ms per token,   347.15 tokens per second)\n",
            "llama_print_timings:        eval time =    1617.47 ms /    15 runs   (  107.83 ms per token,     9.27 tokens per second)\n",
            "llama_print_timings:       total time =    2234.29 ms /   116 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Commissioner, the Malta forum will take place in March, and it is a great pity that the European Parliament will not be allowed to participate in it.\n",
            "hyp1: It is sad that the European Parliament won't be able to participate in the Malta forum in March.\n",
            "hyp2: It is sad that the European Parliament won't be able to participate in the Malta forum in March 2004.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =      16.70 ms /    16 runs   (    1.04 ms per token,   958.20 tokens per second)\n",
            "llama_print_timings: prompt eval time =     338.43 ms /    92 tokens (    3.68 ms per token,   271.84 tokens per second)\n",
            "llama_print_timings:        eval time =    2142.41 ms /    15 runs   (  142.83 ms per token,     7.00 tokens per second)\n",
            "llama_print_timings:       total time =    3348.90 ms /   107 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: And I should like to warn this House that we must be careful when voting on Amendment No 98 so that we are not inconsistent.\n",
            "hyp1: I would like to warn the House that all of us need to be careful when voting on the amendment.\n",
            "hyp2: I would like to warn the House that they need to be careful when voting on the amendment.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.24 ms /    16 runs   (    0.52 ms per token,  1941.28 tokens per second)\n",
            "llama_print_timings: prompt eval time =     314.59 ms /   102 tokens (    3.08 ms per token,   324.23 tokens per second)\n",
            "llama_print_timings:        eval time =    1652.76 ms /    15 runs   (  110.18 ms per token,     9.08 tokens per second)\n",
            "llama_print_timings:       total time =    2321.70 ms /   117 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: In addition, there will be no benefit to exchequers as the taxable interest payments will be outside the 15 Member States.\n",
            "hyp1: There will be no benefit to exchequers as interest payments outside of the 15 Member States will be taxed.\n",
            "hyp2: There will be no benefit to exchequers as only interest payments outside of the 15 Member States can be taxed.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.26 ms /    16 runs   (    0.52 ms per token,  1936.58 tokens per second)\n",
            "llama_print_timings: prompt eval time =     291.25 ms /    85 tokens (    3.43 ms per token,   291.84 tokens per second)\n",
            "llama_print_timings:        eval time =    1578.19 ms /    15 runs   (  105.21 ms per token,     9.50 tokens per second)\n",
            "llama_print_timings:       total time =    2167.28 ms /   100 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Mr Florenz said, in his nice way, that we would be taking a softer line and the directive would only apply from the year 2020.\n",
            "hyp1: The directive would not apply from the year 2020.\n",
            "hyp2: The directive would only apply from the year 2020.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.78 ms /    16 runs   (    0.49 ms per token,  2055.76 tokens per second)\n",
            "llama_print_timings: prompt eval time =     272.62 ms /    79 tokens (    3.45 ms per token,   289.78 tokens per second)\n",
            "llama_print_timings:        eval time =    1549.32 ms /    15 runs   (  103.29 ms per token,     9.68 tokens per second)\n",
            "llama_print_timings:       total time =    2097.58 ms /    94 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Establishing a new framework directive is certainly going to take some doing: the way it was dealt with at first reading was extremely confusing.\n",
            "hyp1: The way in which the framework directive was dealt with was very clear.\n",
            "hyp2: The way in which the framework directive was dealt with was very confusing.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.74 ms /    16 runs   (    0.55 ms per token,  1830.66 tokens per second)\n",
            "llama_print_timings: prompt eval time =     308.36 ms /    90 tokens (    3.43 ms per token,   291.87 tokens per second)\n",
            "llama_print_timings:        eval time =    1875.98 ms /    15 runs   (  125.07 ms per token,     8.00 tokens per second)\n",
            "llama_print_timings:       total time =    2720.76 ms /   105 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: However, there is one issue, outlined in paragraph 11, which clearly detracts from the report's otherwise good intentions.\n",
            "hyp1: Paragraph 11 contains an issue that detracts from the good intentions of the report.\n",
            "hyp2: Paragraph 11.4 contains an issue that detracts from the good intentions of the report.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =      13.92 ms /    16 runs   (    0.87 ms per token,  1149.34 tokens per second)\n",
            "llama_print_timings: prompt eval time =     311.01 ms /    98 tokens (    3.17 ms per token,   315.10 tokens per second)\n",
            "llama_print_timings:        eval time =    1860.74 ms /    15 runs   (  124.05 ms per token,     8.06 tokens per second)\n",
            "llama_print_timings:       total time =    2737.75 ms /   113 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: In Kyoto, the European Union committed itself to reducing greenhouse gas emissions by 8 % compared to 1990 levels before 2012.\n",
            "hyp1: The European Union pledged in Kyoto to reduce greenhouse gas emissions by 8 percent.\n",
            "hyp2: The European Union pledged in Kyoto to reduce greenhouse gas emissions by 8 metric tons of CO2.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.60 ms /    16 runs   (    0.48 ms per token,  2104.16 tokens per second)\n",
            "llama_print_timings: prompt eval time =     305.09 ms /   104 tokens (    2.93 ms per token,   340.88 tokens per second)\n",
            "llama_print_timings:        eval time =    1567.51 ms /    15 runs   (  104.50 ms per token,     9.57 tokens per second)\n",
            "llama_print_timings:       total time =    2211.68 ms /   119 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: It is not only a pressing problem for the Member States in the Mediterranean region, but also concerns, to a large extent, the entire European Union.\n",
            "hyp1: It's a big problem for the Member States in the Mediterranean region and it's also a big problem for the European Union.\n",
            "hyp2: It's a big problem for the United States in the Mediterranean region and it's also a big problem for the European Union.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.33 ms /    16 runs   (    0.58 ms per token,  1715.27 tokens per second)\n",
            "llama_print_timings: prompt eval time =     295.87 ms /   103 tokens (    2.87 ms per token,   348.13 tokens per second)\n",
            "llama_print_timings:        eval time =    1721.33 ms /    15 runs   (  114.76 ms per token,     8.71 tokens per second)\n",
            "llama_print_timings:       total time =    2369.31 ms /   118 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: It taxes the interest on the savings of the small investor and the small saver and eases the tax burden on companies and large conglomerates.\n",
            "hyp1: It lowers the tax burden on companies and large conglomerates by taxing the interest on savings of small investers and savers.\n",
            "hyp2: It lowers the tax burden on companies and large conglomerates by taxing the interest on savings.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.99 ms /    16 runs   (    0.50 ms per token,  2003.00 tokens per second)\n",
            "llama_print_timings: prompt eval time =     288.26 ms /    78 tokens (    3.70 ms per token,   270.59 tokens per second)\n",
            "llama_print_timings:        eval time =    1575.37 ms /    15 runs   (  105.02 ms per token,     9.52 tokens per second)\n",
            "llama_print_timings:       total time =    2121.36 ms /    93 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Amendment No 3 refers to the putting in place of a Community plan of action which, however, has already been established.\n",
            "hyp1: The Community plan of action has already been put in place successfully and with only a few delays.\n",
            "hyp2: The Community plan of action has already been put in place.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.59 ms /    16 runs   (    0.47 ms per token,  2108.31 tokens per second)\n",
            "llama_print_timings: prompt eval time =     285.47 ms /    87 tokens (    3.28 ms per token,   304.77 tokens per second)\n",
            "llama_print_timings:        eval time =    1643.59 ms /    15 runs   (  109.57 ms per token,     9.13 tokens per second)\n",
            "llama_print_timings:       total time =    2212.07 ms /   102 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Berlin was the scene of his rise to fame and it was to Berlin that he returned in 1945 to promote dialogue with the German people.\n",
            "hyp1: In 1945 she came back to Berlin to promote dialogue with the Germans.\n",
            "hyp2: In 1945 he came back to Berlin to promote dialogue with the Germans.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.27 ms /    16 runs   (    0.45 ms per token,  2199.62 tokens per second)\n",
            "llama_print_timings: prompt eval time =     282.94 ms /    84 tokens (    3.37 ms per token,   296.88 tokens per second)\n",
            "llama_print_timings:        eval time =    1582.69 ms /    15 runs   (  105.51 ms per token,     9.48 tokens per second)\n",
            "llama_print_timings:       total time =    2151.62 ms /    99 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: However, I am opposed to the proposal contained in paragraph 8 of the resolution, because it will lead to increased supranationality.\n",
            "hyp1: Paragraph 8 of the resolution will lead to decreased supranationality.\n",
            "hyp2: Paragraph 8 of the resolution will lead to increased supranationality.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.02 ms /    16 runs   (    0.56 ms per token,  1774.43 tokens per second)\n",
            "llama_print_timings: prompt eval time =     288.09 ms /    97 tokens (    2.97 ms per token,   336.71 tokens per second)\n",
            "llama_print_timings:        eval time =    1778.25 ms /    15 runs   (  118.55 ms per token,     8.44 tokens per second)\n",
            "llama_print_timings:       total time =    2405.31 ms /   112 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: According to what I have heard, the ISO will already have a label ready for the summer of 1999, in other words in a few months' time.\n",
            "hyp1: The MSZT will have a label ready in a few months, according to what I've heard.\n",
            "hyp2: The ISO will have a label ready in a few months, according to what I've heard.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.50 ms /    16 runs   (    0.47 ms per token,  2131.91 tokens per second)\n",
            "llama_print_timings: prompt eval time =     294.05 ms /    74 tokens (    3.97 ms per token,   251.66 tokens per second)\n",
            "llama_print_timings:        eval time =    1601.09 ms /    15 runs   (  106.74 ms per token,     9.37 tokens per second)\n",
            "llama_print_timings:       total time =    2153.29 ms /    89 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Nevertheless, there are two particular points that cannot go unmentioned. The first relates to road transport and is the issue of working time.\n",
            "hyp1: The particular issue of working time within road transport cannot be ignored.\n",
            "hyp2: The issues of working time and road transport cannot be ignored.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.65 ms /    16 runs   (    0.48 ms per token,  2091.78 tokens per second)\n",
            "llama_print_timings: prompt eval time =     277.27 ms /    88 tokens (    3.15 ms per token,   317.38 tokens per second)\n",
            "llama_print_timings:        eval time =    1557.07 ms /    15 runs   (  103.80 ms per token,     9.63 tokens per second)\n",
            "llama_print_timings:       total time =    2135.09 ms /   103 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: In May 1998, the Single Market Council adopted provisions regarding the development of a single market in medicinal products.\n",
            "hyp1: The Development of a Single Market in Medicinal Products was adopted by the Single Market Council in 1989.\n",
            "hyp2: The Development of a Single Market in Medicinal Products was adopted by the Single Market Council.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.99 ms /    16 runs   (    0.50 ms per token,  2002.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =     283.19 ms /    86 tokens (    3.29 ms per token,   303.68 tokens per second)\n",
            "llama_print_timings:        eval time =    1505.25 ms /    15 runs   (  100.35 ms per token,     9.97 tokens per second)\n",
            "llama_print_timings:       total time =    2093.05 ms /   101 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: We should remember that the nuclear power station at Chernobyl was, until April 1986, said to have presented 'few risks'.\n",
            "hyp1: The nuclear power station at Chernobyl was said to have presented'few risks'.\n",
            "hyp2: The nuclear power station at Chernobyl was said to have presented 'no risks'.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.33 ms /    16 runs   (    0.58 ms per token,  1715.08 tokens per second)\n",
            "llama_print_timings: prompt eval time =     283.10 ms /    90 tokens (    3.15 ms per token,   317.91 tokens per second)\n",
            "llama_print_timings:        eval time =    1870.88 ms /    15 runs   (  124.73 ms per token,     8.02 tokens per second)\n",
            "llama_print_timings:       total time =    2494.16 ms /   105 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The report we asked for from the committee of experts is part of the consequences of our withholding the discharge for 1996.\n",
            "hyp1: The committee of experts gave us a report on the consequences of withholding the discharge.\n",
            "hyp2: The committee of experts gave us a report as part of the consequences of withholding the discharge.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.40 ms /    16 runs   (    0.53 ms per token,  1904.31 tokens per second)\n",
            "llama_print_timings: prompt eval time =     308.66 ms /    87 tokens (    3.55 ms per token,   281.87 tokens per second)\n",
            "llama_print_timings:        eval time =    1562.69 ms /    15 runs   (  104.18 ms per token,     9.60 tokens per second)\n",
            "llama_print_timings:       total time =    2155.20 ms /   102 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The irregularities which have been identified must never happen again, and this means that both the methods and the approach need to be changed.\n",
            "hyp1: The approach and methods need to be changed because they need to never happen again.\n",
            "hyp2: It's imperative to never repeat these irregularities, which requires a change in the methods and approach. \n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.65 ms /    16 runs   (    0.48 ms per token,  2091.23 tokens per second)\n",
            "llama_print_timings: prompt eval time =     290.48 ms /    98 tokens (    2.96 ms per token,   337.37 tokens per second)\n",
            "llama_print_timings:        eval time =    1544.74 ms /    15 runs   (  102.98 ms per token,     9.71 tokens per second)\n",
            "llama_print_timings:       total time =    2146.25 ms /   113 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: I am referring more particularly here to Amendments Nos 4 and 33, which provide for the directive to be monitored by the Commission.\n",
            "hyp1: Amendments Nos 4 and 33 give the Commission no power to monitor the directive.\n",
            "hyp2: Amendments Nos 4 and 33 give the Commission the power to monitor the directive.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.14 ms /    16 runs   (    0.51 ms per token,  1964.88 tokens per second)\n",
            "llama_print_timings: prompt eval time =     271.78 ms /    82 tokens (    3.31 ms per token,   301.71 tokens per second)\n",
            "llama_print_timings:        eval time =    1561.09 ms /    15 runs   (  104.07 ms per token,     9.61 tokens per second)\n",
            "llama_print_timings:       total time =    2100.85 ms /    97 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: That is why it is also important that the innovation dimension should become a more integral part of interregional and cross-border cooperation.\n",
            "hyp1: It is important that innovation is included in interregional and cross border cooperation.\n",
            "hyp2: It is important that innovation becomes a more essential part of interregional and cross border cooperation.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.14 ms /    16 runs   (    0.57 ms per token,  1750.55 tokens per second)\n",
            "llama_print_timings: prompt eval time =     272.50 ms /    78 tokens (    3.49 ms per token,   286.23 tokens per second)\n",
            "llama_print_timings:        eval time =    1795.65 ms /    15 runs   (  119.71 ms per token,     8.35 tokens per second)\n",
            "llama_print_timings:       total time =    2390.43 ms /    93 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Environmental considerations must feature prominently in this process, particularly sustainable development, as it is one of the distinguishing features of the European Union.\n",
            "hyp1: Sustainable development is a distinguishing feature of the European Union.\n",
            "hyp2: One of the distinguishing features of the European Union is the environment.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.78 ms /    16 runs   (    0.49 ms per token,  2057.61 tokens per second)\n",
            "llama_print_timings: prompt eval time =     295.50 ms /    98 tokens (    3.02 ms per token,   331.64 tokens per second)\n",
            "llama_print_timings:        eval time =    1509.82 ms /    15 runs   (  100.65 ms per token,     9.93 tokens per second)\n",
            "llama_print_timings:       total time =    2124.42 ms /   113 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Mrs Haug's report is an important addition to the Agenda 2000 debate, and to the ongoing preparation process for the next phase of the programme.\n",
            "hyp1: An important addition to the Agenda 2000 debate is Mx Haug's report.\n",
            "hyp2: An important addition to the Agenda 2000 debate is Mrs Haug's report.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.19 ms /    15 runs   (    0.48 ms per token,  2085.94 tokens per second)\n",
            "llama_print_timings: prompt eval time =     301.88 ms /   109 tokens (    2.77 ms per token,   361.08 tokens per second)\n",
            "llama_print_timings:        eval time =    1430.72 ms /    14 runs   (  102.19 ms per token,     9.79 tokens per second)\n",
            "llama_print_timings:       total time =    2094.46 ms /   123 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Fortunately, there has been a cease-fire since 1994 which is opening the way for negotiation, and the Minsk Group is taking charge of this.\n",
            "hyp1: There has been an open-fire since 1994 and the Minsk Group is in charge of this.\n",
            "hyp2: There has been a cease-fire since 1994 and the Minsk Group is in charge of this.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.11 ms /    16 runs   (    0.51 ms per token,  1972.39 tokens per second)\n",
            "llama_print_timings: prompt eval time =     288.16 ms /   110 tokens (    2.62 ms per token,   381.74 tokens per second)\n",
            "llama_print_timings:        eval time =    1669.69 ms /    15 runs   (  111.31 ms per token,     8.98 tokens per second)\n",
            "llama_print_timings:       total time =    2317.93 ms /   125 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Women make up 80 % of the people who carry out secretarial and office work, while men occupy 87 % of the managerial positions.\n",
            "hyp1: Eighty percent of the people who work in the office are women, while 87 percent are men.\n",
            "hyp2: Eighty percent of the people who work in the secretarial positions at the office are women, while 87 percent of those with managerial positions are men.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.72 ms /    16 runs   (    0.61 ms per token,  1645.92 tokens per second)\n",
            "llama_print_timings: prompt eval time =     293.22 ms /    93 tokens (    3.15 ms per token,   317.17 tokens per second)\n",
            "llama_print_timings:        eval time =    1830.86 ms /    15 runs   (  122.06 ms per token,     8.19 tokens per second)\n",
            "llama_print_timings:       total time =    2561.72 ms /   108 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: In a similar manner, I could accept the proposed Amendment No 11, which would require the same separation but in a shorter time period.\n",
            "hyp1: The proposed Amendment No 11 requires the same separation, but in a shorter period of time.\n",
            "hyp2: The proposed Amendment No 11 previously required the same separation, but in a shorter period of time.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.00 ms /    16 runs   (    0.56 ms per token,  1777.19 tokens per second)\n",
            "llama_print_timings: prompt eval time =     301.02 ms /    85 tokens (    3.54 ms per token,   282.37 tokens per second)\n",
            "llama_print_timings:        eval time =    1535.31 ms /    15 runs   (  102.35 ms per token,     9.77 tokens per second)\n",
            "llama_print_timings:       total time =    2112.87 ms /   100 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The outcome will hinge on how the new decision-making procedures work out in practice. We cannot therefore support paragraphs 30 and 31.\n",
            "hyp1: Paragraphs 300 and 31 are not supported by us.\n",
            "hyp2: Paragraphs 30 and 31 are not supported by us.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.99 ms /    16 runs   (    0.50 ms per token,  2001.75 tokens per second)\n",
            "llama_print_timings: prompt eval time =     280.78 ms /    84 tokens (    3.34 ms per token,   299.16 tokens per second)\n",
            "llama_print_timings:        eval time =    1754.23 ms /    15 runs   (  116.95 ms per token,     8.55 tokens per second)\n",
            "llama_print_timings:       total time =    2344.60 ms /    99 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: But the US, in threatening a trade boycott, is overstepping the bounds of that clear position and the bounds of the transatlantic partnership.\n",
            "hyp1: The US is overstepping its bounds by threatening to boycott the oil trade.\n",
            "hyp2: The US is overstepping its bounds by threatening to boycott trade.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.24 ms /    16 runs   (    0.52 ms per token,  1941.28 tokens per second)\n",
            "llama_print_timings: prompt eval time =     388.60 ms /   161 tokens (    2.41 ms per token,   414.31 tokens per second)\n",
            "llama_print_timings:        eval time =    1997.17 ms /    15 runs   (  133.14 ms per token,     7.51 tokens per second)\n",
            "llama_print_timings:       total time =    2906.03 ms /   176 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: In short, therefore, the Commission accepts Amendment Nos 1, 5, 6, 7, 8, 9 and 11, whilst rejecting Amendments Nos 2, 3, 4, 10 and 12.\n",
            "hyp1: The Commission accepts Amendments 1 to 9 and 11 and rejects Amendments 3, 4, 10 and 12.\n",
            "hyp2: The Commission accepts Amendments 1, 5, 6, 7, 8, 9 and 11 and rejects Amendments 3, 4, 10 and 12.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.60 ms /    16 runs   (    0.54 ms per token,  1861.55 tokens per second)\n",
            "llama_print_timings: prompt eval time =     313.23 ms /    98 tokens (    3.20 ms per token,   312.87 tokens per second)\n",
            "llama_print_timings:        eval time =    1644.42 ms /    15 runs   (  109.63 ms per token,     9.12 tokens per second)\n",
            "llama_print_timings:       total time =    2458.01 ms /   113 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: While the Kalanke ruling was a setback, the European Court of Justice's decision in the 1997 Marschall judgment was encouraging.\n",
            "hyp1: The European Court of Justice's 1997 Marschall judgement was an encouraging one.\n",
            "hyp2: The European Court of Justice's 1997 Marschall judgement was not an encouraging one.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.75 ms /    16 runs   (    0.48 ms per token,  2065.58 tokens per second)\n",
            "llama_print_timings: prompt eval time =     291.12 ms /    89 tokens (    3.27 ms per token,   305.72 tokens per second)\n",
            "llama_print_timings:        eval time =    1539.18 ms /    15 runs   (  102.61 ms per token,     9.75 tokens per second)\n",
            "llama_print_timings:       total time =    2126.05 ms /   104 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: People would think we were mad if we said tomorrow that drivers were allowed to take their cars on the road without an insurance certificate.\n",
            "hyp1: If we said drivers could take their cars on the road without insurance, people would not think we were crazy.\n",
            "hyp2: If we said drivers could take their cars on the road without insurance, people would think we were crazy.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.24 ms /    16 runs   (    0.51 ms per token,  1942.45 tokens per second)\n",
            "llama_print_timings: prompt eval time =     273.96 ms /    79 tokens (    3.47 ms per token,   288.36 tokens per second)\n",
            "llama_print_timings:        eval time =    1646.57 ms /    15 runs   (  109.77 ms per token,     9.11 tokens per second)\n",
            "llama_print_timings:       total time =    2194.73 ms /    94 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: As we have already said, the adoption of such a stance is incompatible with the aims of the Treaty, because agricultural expenditure is compulsory.\n",
            "hyp1: The aims of the Treaty are compatible with the stance adopted.\n",
            "hyp2: The aims of the Treaty are incompatible with the stance adopted.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.35 ms /    14 runs   (    0.60 ms per token,  1676.04 tokens per second)\n",
            "llama_print_timings: prompt eval time =     305.52 ms /    79 tokens (    3.87 ms per token,   258.58 tokens per second)\n",
            "llama_print_timings:        eval time =    1634.56 ms /    13 runs   (  125.74 ms per token,     7.95 tokens per second)\n",
            "llama_print_timings:       total time =    2393.43 ms /    92 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Of the 279 local authorities that exist in Sweden, 211 - all of them in the interior of the country - have lost part of their population.\n",
            "hyp1: The majority of the national authorities in Sweden have lost population.\n",
            "hyp2: The majority of the local authorities in Sweden have lost population.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.77 ms /    16 runs   (    0.49 ms per token,  2059.20 tokens per second)\n",
            "llama_print_timings: prompt eval time =     327.75 ms /   126 tokens (    2.60 ms per token,   384.45 tokens per second)\n",
            "llama_print_timings:        eval time =    1518.01 ms /    15 runs   (  101.20 ms per token,     9.88 tokens per second)\n",
            "llama_print_timings:       total time =    2261.68 ms /   141 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: This is also the reason why we cannot accept Amendment No 25 or Amendments Nos 46 and 54, but we can endorse Amendment No 43.\n",
            "hyp1: I can endorse Amendment No 43 but I can't accept Amendments No 25 and Nos 46 and 54.\n",
            "hyp2: We can endorse Amendment No 43 but we can't accept Amendments No 25 and Nos 46 and 54.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.88 ms /    16 runs   (    0.49 ms per token,  2030.46 tokens per second)\n",
            "llama_print_timings: prompt eval time =     291.62 ms /    95 tokens (    3.07 ms per token,   325.76 tokens per second)\n",
            "llama_print_timings:        eval time =    1548.85 ms /    15 runs   (  103.26 ms per token,     9.68 tokens per second)\n",
            "llama_print_timings:       total time =    2145.21 ms /   110 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Madam President, in yesterday's International Herald Tribune, the following appeared in the section 'News from 50 years ago'.\n",
            "hyp1: The section \"news from 50 years ago\" appeared in yesterday'sInternational Herald Tribune.\n",
            "hyp2: This appeared in section \"news from 50 years ago\" in yesterday's International Herald Tribune.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.48 ms /    16 runs   (    0.47 ms per token,  2138.47 tokens per second)\n",
            "llama_print_timings: prompt eval time =     287.68 ms /    88 tokens (    3.27 ms per token,   305.89 tokens per second)\n",
            "llama_print_timings:        eval time =    1574.10 ms /    15 runs   (  104.94 ms per token,     9.53 tokens per second)\n",
            "llama_print_timings:       total time =    2151.72 ms /   103 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The current status quo of a 12-mile limit for the area is the best possible compromise for the whole of the French fishing sector.\n",
            "hyp1: The current 12 kilometer limit is the best compromise for the entire French fishing sector.\n",
            "hyp2: The current 12-mile limit is the best compromise for the entire French fishing sector.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.86 ms /    16 runs   (    0.55 ms per token,  1806.28 tokens per second)\n",
            "llama_print_timings: prompt eval time =     279.75 ms /    84 tokens (    3.33 ms per token,   300.27 tokens per second)\n",
            "llama_print_timings:        eval time =    1792.78 ms /    15 runs   (  119.52 ms per token,     8.37 tokens per second)\n",
            "llama_print_timings:       total time =    2405.19 ms /    99 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: It has come to my knowledge that we have 300 freelance interpreters whom the Commission has not paid properly since October.\n",
            "hyp1: 300 interpreters have been adequetly paid  by the Commission. \n",
            "hyp2: 300 interpreters have not been paid properly by the Commission.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.91 ms /    16 runs   (    0.49 ms per token,  2021.73 tokens per second)\n",
            "llama_print_timings: prompt eval time =     305.61 ms /    94 tokens (    3.25 ms per token,   307.58 tokens per second)\n",
            "llama_print_timings:        eval time =    1645.65 ms /    15 runs   (  109.71 ms per token,     9.11 tokens per second)\n",
            "llama_print_timings:       total time =    2252.17 ms /   109 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: For example, the conclave of foreign ministers in Luxembourg on 21 February made substantial progress on a number of issues.\n",
            "hyp1: Significant progress was made on a number of issues during the foreign minister's conclave in Luxembourg City.\n",
            "hyp2: Significant progress was made on a number of issues during the foreign minister's conclave in Luxembourg.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.13 ms /    16 runs   (    0.51 ms per token,  1968.75 tokens per second)\n",
            "llama_print_timings: prompt eval time =     291.85 ms /    86 tokens (    3.39 ms per token,   294.67 tokens per second)\n",
            "llama_print_timings:        eval time =    1496.46 ms /    15 runs   (   99.76 ms per token,    10.02 tokens per second)\n",
            "llama_print_timings:       total time =    2072.54 ms /   101 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Cigarettes account for the other 95 %, but as far as they are concerned, this report only deals with technical adjustments to the taxation regime.\n",
            "hyp1: The report only deals with technical adjustments to the taxation regime and excludes cigarettes.\n",
            "hyp2: The report only deals with technical adjustments to the taxation regime.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.47 ms /    16 runs   (    0.59 ms per token,  1689.90 tokens per second)\n",
            "llama_print_timings: prompt eval time =     290.07 ms /    80 tokens (    3.63 ms per token,   275.79 tokens per second)\n",
            "llama_print_timings:        eval time =    1863.22 ms /    15 runs   (  124.21 ms per token,     8.05 tokens per second)\n",
            "llama_print_timings:       total time =    2618.25 ms /    95 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: But at that time the agricultural proportion of the budget, accounting for over 70 %, was considerably higher than it is today.\n",
            "hyp1: More than 70 dollars from the budget was devoted to agriculture at that time.\n",
            "hyp2: More than 70 percent of the budget was devoted to agriculture at that time.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.28 ms /    16 runs   (    0.58 ms per token,  1725.07 tokens per second)\n",
            "llama_print_timings: prompt eval time =     312.65 ms /    96 tokens (    3.26 ms per token,   307.05 tokens per second)\n",
            "llama_print_timings:        eval time =    1865.46 ms /    15 runs   (  124.36 ms per token,     8.04 tokens per second)\n",
            "llama_print_timings:       total time =    2707.00 ms /   111 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: In two days' time, we shall have the debate and the decisions of the European Council on Agenda 2000 and the financial perspective.\n",
            "hyp1: The European Council on Agenda 2000 and financial perspective will be debated in twenty two days.\n",
            "hyp2: The European Council on Agenda 2000 and financial perspective will be debated in two days.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.85 ms /    16 runs   (    0.49 ms per token,  2039.00 tokens per second)\n",
            "llama_print_timings: prompt eval time =     304.40 ms /    86 tokens (    3.54 ms per token,   282.52 tokens per second)\n",
            "llama_print_timings:        eval time =    1586.90 ms /    15 runs   (  105.79 ms per token,     9.45 tokens per second)\n",
            "llama_print_timings:       total time =    2182.87 ms /   101 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The Member States are, in other words, being asked to harmonise their economic policies - which is one of the reasons why I voted against this report.\n",
            "hyp1: I voted against the report because Member States are being asked to change their economic policies.\n",
            "hyp2: I voted against the report because Member States are being asked to reconcile their economic policies.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.72 ms /    16 runs   (    0.48 ms per token,  2072.27 tokens per second)\n",
            "llama_print_timings: prompt eval time =     275.36 ms /    80 tokens (    3.44 ms per token,   290.53 tokens per second)\n",
            "llama_print_timings:        eval time =    1566.69 ms /    15 runs   (  104.45 ms per token,     9.57 tokens per second)\n",
            "llama_print_timings:       total time =    2124.55 ms /    95 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: We have also considered this, and we think that these vehicles should only be exempt from Articles 4 and 7 of the proposal.\n",
            "hyp1: We think the vehicles should be exempt articles number 4 and 7 from the proposal.\n",
            "hyp2: We think the vehicles should be exempt from the proposal.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.39 ms /    16 runs   (    0.46 ms per token,  2164.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =     279.88 ms /    96 tokens (    2.92 ms per token,   343.00 tokens per second)\n",
            "llama_print_timings:        eval time =    1579.39 ms /    15 runs   (  105.29 ms per token,     9.50 tokens per second)\n",
            "llama_print_timings:       total time =    2172.66 ms /   111 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: How is rail transport supposed to compete with road transport if large containers cannot be brought to the trains and we are therefore forced to accept the 44 tonne limit?\n",
            "hyp1: If rail transport can't be brought to the trains, how can large containers compete with road transportation?\n",
            "hyp2: If large containers can't be brought to the trains, how can rail transport compete with road transportation?\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.03 ms /    16 runs   (    0.56 ms per token,  1771.68 tokens per second)\n",
            "llama_print_timings: prompt eval time =     301.34 ms /    98 tokens (    3.07 ms per token,   325.22 tokens per second)\n",
            "llama_print_timings:        eval time =    1776.46 ms /    15 runs   (  118.43 ms per token,     8.44 tokens per second)\n",
            "llama_print_timings:       total time =    2614.93 ms /   113 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: For the latter, the initial birth of several operators is now giving way to the reconcentration of the sector in the hands of a single company.\n",
            "hyp1: The establishment of a number of operators is giving way to the reconcentration of the sector in the hands of one company.\n",
            "hyp2: Several operators have given way to the reconcentration of the sector in the hands of one company.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.51 ms /    16 runs   (    0.47 ms per token,  2129.36 tokens per second)\n",
            "llama_print_timings: prompt eval time =     285.28 ms /    68 tokens (    4.20 ms per token,   238.36 tokens per second)\n",
            "llama_print_timings:        eval time =    1512.27 ms /    15 runs   (  100.82 ms per token,     9.92 tokens per second)\n",
            "llama_print_timings:       total time =    2023.44 ms /    83 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: At the same time, and I always thought this was correct, we have quite simply deleted Chapter 8 from the Convention entirely.\n",
            "hyp1: I have completely removed Chapter 8 from the Convention.\n",
            "hyp2: We have completely removed Chapter 8 from the Convention.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.72 ms /    16 runs   (    0.48 ms per token,  2072.81 tokens per second)\n",
            "llama_print_timings: prompt eval time =     296.34 ms /    85 tokens (    3.49 ms per token,   286.83 tokens per second)\n",
            "llama_print_timings:        eval time =    1570.06 ms /    15 runs   (  104.67 ms per token,     9.55 tokens per second)\n",
            "llama_print_timings:       total time =    2140.20 ms /   100 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: It is true that even greater efforts could be made in this direction, as my honourable colleague said, and indeed we are demanding progress here.\n",
            "hyp1: I agree with my colleague and fellow lawyers that more efforts could be made in this direction.\n",
            "hyp2: I agree with my colleague that more efforts could be made in this direction.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.11 ms /    16 runs   (    0.51 ms per token,  1974.09 tokens per second)\n",
            "llama_print_timings: prompt eval time =     280.05 ms /    83 tokens (    3.37 ms per token,   296.37 tokens per second)\n",
            "llama_print_timings:        eval time =    1581.67 ms /    15 runs   (  105.44 ms per token,     9.48 tokens per second)\n",
            "llama_print_timings:       total time =    2129.92 ms /    98 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: For that reason, on 17 December 1998 the Council adopted a comprehensive programme to supply agricultural produce to Russia.\n",
            "hyp1: The Council adopted a programme to supply agricultural produce to Russia in 1917.\n",
            "hyp2: The Council adopted a programme to supply agricultural produce to Russia in 1998.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =      12.41 ms /    16 runs   (    0.78 ms per token,  1289.18 tokens per second)\n",
            "llama_print_timings: prompt eval time =     313.80 ms /    93 tokens (    3.37 ms per token,   296.36 tokens per second)\n",
            "llama_print_timings:        eval time =    1759.77 ms /    15 runs   (  117.32 ms per token,     8.52 tokens per second)\n",
            "llama_print_timings:       total time =    2592.66 ms /   108 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Frontier regions where the inhabitants do not understand their neighbour's language cannot fully experience the single market at local level.\n",
            "hyp1: The single market can't be fully experienced in frontier regions where the inhabitants understand their neighbours' languages.\n",
            "hyp2: The single market can't be fully experienced in frontier regions where the inhabitants don't understand their neighbours' languages.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.71 ms /    16 runs   (    0.48 ms per token,  2076.30 tokens per second)\n",
            "llama_print_timings: prompt eval time =     282.42 ms /    87 tokens (    3.25 ms per token,   308.05 tokens per second)\n",
            "llama_print_timings:        eval time =    1615.91 ms /    15 runs   (  107.73 ms per token,     9.28 tokens per second)\n",
            "llama_print_timings:       total time =    2176.82 ms /   102 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: We welcome the adoption by the Council of Ministers of general guidelines on the issue of the death penalty in June 1998.\n",
            "hyp1: The Council of Ministers adopted general guidelines regarding the death penalty in late 1998.\n",
            "hyp2: The Council of Ministers adopted general guidelines regarding the death penalty in 1998.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.09 ms /    16 runs   (    0.51 ms per token,  1977.75 tokens per second)\n",
            "llama_print_timings: prompt eval time =     295.39 ms /    91 tokens (    3.25 ms per token,   308.07 tokens per second)\n",
            "llama_print_timings:        eval time =    1645.64 ms /    15 runs   (  109.71 ms per token,     9.12 tokens per second)\n",
            "llama_print_timings:       total time =    2233.87 ms /   106 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: In addition, television campaigns designed to dissuade would-be immigrants from making the crossing could be broadcast on television in northern Morocco.\n",
            "hyp1: Television campaigns designed to discourage would-be immigrants from crossing the border could be broadcasted on television.\n",
            "hyp2: Television campaigns designed to persuade would-be immigrants from crossing the border could be broadcasted on television.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.33 ms /    16 runs   (    0.52 ms per token,  1920.54 tokens per second)\n",
            "llama_print_timings: prompt eval time =     294.73 ms /   100 tokens (    2.95 ms per token,   339.30 tokens per second)\n",
            "llama_print_timings:        eval time =    1641.22 ms /    15 runs   (  109.41 ms per token,     9.14 tokens per second)\n",
            "llama_print_timings:       total time =    2263.95 ms /   115 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Thirdly, the Commission agrees with the preoccupation of the rapporteur as far as the famous RAL, reste à liquider , the backlog, is concerned.\n",
            "hyp1: The Commission agrees with the rapporteur's preoccupation with the famous RAL, reste liquider.\n",
            "hyp2: The Commission agrees with the rapporteur's preoccupation with the famous RAL, reste à liquider.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.06 ms /    16 runs   (    0.57 ms per token,  1766.39 tokens per second)\n",
            "llama_print_timings: prompt eval time =     292.69 ms /    76 tokens (    3.85 ms per token,   259.66 tokens per second)\n",
            "llama_print_timings:        eval time =    1768.70 ms /    15 runs   (  117.91 ms per token,     8.48 tokens per second)\n",
            "llama_print_timings:       total time =    2463.27 ms /    91 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: 1.Negotiations between the governments on the reform of agricultural policy are still in progress and will be resumed today.\n",
            "hyp1: The negotiations regarding the reform of agricultural policy have been finalized by the governments.\n",
            "hyp2: The reform of agricultural policy is still being negotiated by the governments.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.35 ms /    16 runs   (    0.46 ms per token,  2178.06 tokens per second)\n",
            "llama_print_timings: prompt eval time =     295.34 ms /    81 tokens (    3.65 ms per token,   274.26 tokens per second)\n",
            "llama_print_timings:        eval time =    1513.81 ms /    15 runs   (  100.92 ms per token,     9.91 tokens per second)\n",
            "llama_print_timings:       total time =    2067.47 ms /    96 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: It also recommends strengthening the initiative-taking and political impetus of the Commission, whose political accountability would also be improved in that way.\n",
            "hyp1: The Commission's political accountability would not be improved in this way.\n",
            "hyp2: The Commission's political accountability would be improved in this way.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.84 ms /    16 runs   (    0.49 ms per token,  2041.60 tokens per second)\n",
            "llama_print_timings: prompt eval time =     287.69 ms /    96 tokens (    3.00 ms per token,   333.69 tokens per second)\n",
            "llama_print_timings:        eval time =    1530.29 ms /    15 runs   (  102.02 ms per token,     9.80 tokens per second)\n",
            "llama_print_timings:       total time =    2130.10 ms /   111 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Another declaration to mention in this respect is Declaration No 39 annexed to the Treaty of Amsterdam on the quality of drafting.\n",
            "hyp1: Declaration No 39 was annexed to the Treaty of Amsterdam to discuss drafting quality.\n",
            "hyp2: Declaration No 39 wasn't annexed to the Treaty of Amsterdam to discuss drafting quality.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.72 ms /    16 runs   (    0.48 ms per token,  2072.81 tokens per second)\n",
            "llama_print_timings: prompt eval time =     282.93 ms /    87 tokens (    3.25 ms per token,   307.50 tokens per second)\n",
            "llama_print_timings:        eval time =    1517.97 ms /    15 runs   (  101.20 ms per token,     9.88 tokens per second)\n",
            "llama_print_timings:       total time =    2089.92 ms /   102 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: It is extremely important that the forthcoming European Council meeting in Berlin should cut the Gordian knot on Agenda 2000.\n",
            "hyp1: Agenda 2000 should be fully handled during the European Council meeting in Berlin.\n",
            "hyp2: Agenda 2000 should be cut at the European Council meeting in Berlin.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.19 ms /    16 runs   (    0.57 ms per token,  1741.97 tokens per second)\n",
            "llama_print_timings: prompt eval time =     329.56 ms /   110 tokens (    3.00 ms per token,   333.78 tokens per second)\n",
            "llama_print_timings:        eval time =    1565.09 ms /    15 runs   (  104.34 ms per token,     9.58 tokens per second)\n",
            "llama_print_timings:       total time =    2450.51 ms /   125 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: As a result of this investment, which I fully support, the suburban rail network will be increased in capacity in terms of over 60 %.\n",
            "hyp1: The investment will increase the suburban rail network's capacity by more than 60 percent.\n",
            "hyp2: The investment will increase the suburban rail network's capacity by more than 60 percentm which is an investment I support given the latest numbers provided by the Suburban Rail Network Committee.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.55 ms /    16 runs   (    0.47 ms per token,  2120.33 tokens per second)\n",
            "llama_print_timings: prompt eval time =     301.04 ms /    97 tokens (    3.10 ms per token,   322.22 tokens per second)\n",
            "llama_print_timings:        eval time =    1495.62 ms /    15 runs   (   99.71 ms per token,    10.03 tokens per second)\n",
            "llama_print_timings:       total time =    2106.04 ms /   112 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: That is precisely why we cannot give way to the unjustified US demand, even though and in fact just because they are threatening lunatic sanctions.\n",
            "hyp1: That's why we can't give in to the US demand even though they're threatening sanctions.\n",
            "hyp2: That's why they can't give in to the US demand even though we're threatening sanctions.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.00 ms /    16 runs   (    0.50 ms per token,  2000.00 tokens per second)\n",
            "llama_print_timings: prompt eval time =     269.60 ms /    76 tokens (    3.55 ms per token,   281.90 tokens per second)\n",
            "llama_print_timings:        eval time =    1556.51 ms /    15 runs   (  103.77 ms per token,     9.64 tokens per second)\n",
            "llama_print_timings:       total time =    2072.04 ms /    91 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: All aspects of the question need to be brought together; perhaps a thorough investigation of the whole practice of aquaculture is now called for.\n",
            "hyp1: A thorough investigation of all aspects of the question is needed.\n",
            "hyp2: A thorough investigation of all aspects of aquaculture may we warranted. \n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.09 ms /    16 runs   (    0.51 ms per token,  1978.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =     279.69 ms /    82 tokens (    3.41 ms per token,   293.18 tokens per second)\n",
            "llama_print_timings:        eval time =    1649.05 ms /    15 runs   (  109.94 ms per token,     9.10 tokens per second)\n",
            "llama_print_timings:       total time =    2215.15 ms /    97 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Mr President, the dossier on European cities of culture which has been open since October 1997 will be closed, I hope, tomorrow.\n",
            "hyp1: I hope that the European cities of culture will be closed tomorrow.\n",
            "hyp2: I hope that the dossier on European cities of culture will be closed tomorrow.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.82 ms /    15 runs   (    0.52 ms per token,  1917.42 tokens per second)\n",
            "llama_print_timings: prompt eval time =     311.18 ms /    84 tokens (    3.70 ms per token,   269.94 tokens per second)\n",
            "llama_print_timings:        eval time =    1490.21 ms /    14 runs   (  106.44 ms per token,     9.39 tokens per second)\n",
            "llama_print_timings:       total time =    2211.91 ms /    98 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Premiums are to be increased by 10 %, with a corresponding reduction in special subsidies allocated through producer organisations.\n",
            "hyp1: Premiums will decrease by 10 % and special subsidies will be increased.\n",
            "hyp2: Premiums will increase by 10 % and special subsidies will be reduced.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.52 ms /    16 runs   (    0.47 ms per token,  2127.09 tokens per second)\n",
            "llama_print_timings: prompt eval time =     282.71 ms /    84 tokens (    3.37 ms per token,   297.12 tokens per second)\n",
            "llama_print_timings:        eval time =    1517.92 ms /    15 runs   (  101.19 ms per token,     9.88 tokens per second)\n",
            "llama_print_timings:       total time =    2070.66 ms /    99 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: At the end of 1997, the Commission put forward a controversial proposal for the harmonisation of copyright law in the Community.\n",
            "hyp1: The proposal for copyright harmonization was put forward at the end of 1997.\n",
            "hyp2: The harmonisation proposal was put forward at the end of 1997.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.08 ms /    16 runs   (    0.50 ms per token,  1980.93 tokens per second)\n",
            "llama_print_timings: prompt eval time =     285.68 ms /    99 tokens (    2.89 ms per token,   346.55 tokens per second)\n",
            "llama_print_timings:        eval time =    1524.93 ms /    15 runs   (  101.66 ms per token,     9.84 tokens per second)\n",
            "llama_print_timings:       total time =    2122.28 ms /   114 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: I had proposed to you that the Euro-11 be recognised in the Treaty, but my understanding was that Mr Spiers did not agree.\n",
            "hyp1: I wanted the Euro-11 to be recognised in the Treaty but Mr. Spiers didn't agree with me.\n",
            "hyp2: I wanted the Euro-11 to be recognised in the Treaty and Mr. Spiers agreed with me.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.23 ms /    16 runs   (    0.51 ms per token,  1944.34 tokens per second)\n",
            "llama_print_timings: prompt eval time =     272.11 ms /    77 tokens (    3.53 ms per token,   282.97 tokens per second)\n",
            "llama_print_timings:        eval time =    1671.21 ms /    15 runs   (  111.41 ms per token,     8.98 tokens per second)\n",
            "llama_print_timings:       total time =    2209.40 ms /    92 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: As a result, we would like to be convinced that these cooperation agreements are not going to contribute to such extremely dangerous research.\n",
            "hyp1: She want to be sure that these agreements won't contribute to dangerous research.\n",
            "hyp2: We want to be sure that these agreements won't contribute to dangerous research.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.63 ms /    16 runs   (    0.48 ms per token,  2095.89 tokens per second)\n",
            "llama_print_timings: prompt eval time =     307.53 ms /    84 tokens (    3.66 ms per token,   273.15 tokens per second)\n",
            "llama_print_timings:        eval time =    1531.61 ms /    15 runs   (  102.11 ms per token,     9.79 tokens per second)\n",
            "llama_print_timings:       total time =    2117.22 ms /    99 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: An important factor here is that the pay and other working conditions of those employed in navigation should be in line with those of other occupations.\n",
            "hyp1: The pay and working conditions of those employed in navigation should be better than those of other jobs.\n",
            "hyp2: The pay and working conditions of those employed in navigation should match those of other jobs.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.89 ms /    16 runs   (    0.49 ms per token,  2027.88 tokens per second)\n",
            "llama_print_timings: prompt eval time =     292.70 ms /   106 tokens (    2.76 ms per token,   362.15 tokens per second)\n",
            "llama_print_timings:        eval time =    1588.10 ms /    15 runs   (  105.87 ms per token,     9.45 tokens per second)\n",
            "llama_print_timings:       total time =    2206.91 ms /   121 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Indeed, without the chairmanship skills of the Deputy Prime Minister, John Prescott, the Conference would have produced more hot air than it prevented!\n",
            "hyp1: The Conference wouldn't have produced as much hot air if it weren't for the chairmanship skills of the deputy prime minister.\n",
            "hyp2: The Conference would have made more hot air than it prevented if it weren't for the chairmanship skills of the deputy prime minister!\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.71 ms /    16 runs   (    0.48 ms per token,  2074.15 tokens per second)\n",
            "llama_print_timings: prompt eval time =     275.48 ms /    87 tokens (    3.17 ms per token,   315.81 tokens per second)\n",
            "llama_print_timings:        eval time =    1655.50 ms /    15 runs   (  110.37 ms per token,     9.06 tokens per second)\n",
            "llama_print_timings:       total time =    2207.42 ms /   102 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: I want to say very clearly that the Commission is no longer be prepared to accept ever-increasing tasks without receiving the means to execute them.\n",
            "hyp1: The Commission is not prepared to accept ever increasing tasks  while getting the means to execute them.\n",
            "hyp2: The Commission is not prepared to accept ever increasing tasks without getting the means to execute them.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.47 ms /    16 runs   (    0.59 ms per token,  1689.55 tokens per second)\n",
            "llama_print_timings: prompt eval time =     292.37 ms /   102 tokens (    2.87 ms per token,   348.87 tokens per second)\n",
            "llama_print_timings:        eval time =    1798.50 ms /    15 runs   (  119.90 ms per token,     8.34 tokens per second)\n",
            "llama_print_timings:       total time =    2432.27 ms /   117 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The enlargement of the EU compels us to pay greater attention to these questions, a point which is also emphasised in the committee's proposals.\n",
            "hyp1: The committee's proposals emphasize that the EU's enlargement makes us pay more attention to the questions.\n",
            "hyp2: The committee's proposals emphasize that the EEAS's enlargement makes us pay more attention to the questions.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.61 ms /    16 runs   (    0.54 ms per token,  1859.38 tokens per second)\n",
            "llama_print_timings: prompt eval time =     306.61 ms /    84 tokens (    3.65 ms per token,   273.96 tokens per second)\n",
            "llama_print_timings:        eval time =    1510.24 ms /    15 runs   (  100.68 ms per token,     9.93 tokens per second)\n",
            "llama_print_timings:       total time =    2083.39 ms /    99 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: I am referring to the current vote but in relation to a vote that will take place later, namely the vote on Amendment No 98.\n",
            "hyp1: The vote on Amendment No 98 will take place before the current vote.\n",
            "hyp2: The vote on Amendment No 98 will take place after the current vote.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.61 ms /    16 runs   (    0.48 ms per token,  2103.05 tokens per second)\n",
            "llama_print_timings: prompt eval time =     283.45 ms /    86 tokens (    3.30 ms per token,   303.40 tokens per second)\n",
            "llama_print_timings:        eval time =    1470.38 ms /    15 runs   (   98.03 ms per token,    10.20 tokens per second)\n",
            "llama_print_timings:       total time =    2022.94 ms /   101 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: When I became a Member of the European Parliament in 1994, I was struck by the lack of knowledge and the total absence of understanding of the overseas territories.\n",
            "hyp1: I was struck by the abundance of knowledge when I joined the European Parliament.\n",
            "hyp2: I was struck by the lack of knowledge when I joined the European Parliament.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.76 ms /    16 runs   (    0.49 ms per token,  2061.06 tokens per second)\n",
            "llama_print_timings: prompt eval time =     278.75 ms /    84 tokens (    3.32 ms per token,   301.34 tokens per second)\n",
            "llama_print_timings:        eval time =    1505.52 ms /    15 runs   (  100.37 ms per token,     9.96 tokens per second)\n",
            "llama_print_timings:       total time =    2051.10 ms /    99 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Mr President, let me express my thanks for the preceding remarks and extend a special welcome to my honourable colleague the Transport Commissioner.\n",
            "hyp1: Mr President, I would like to extend a warm welcome to the Transport Commissioner.\n",
            "hyp2: Madam President, I would like to extend a warm welcome to the Transport Commissioner.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       9.40 ms /    16 runs   (    0.59 ms per token,  1701.40 tokens per second)\n",
            "llama_print_timings: prompt eval time =     297.01 ms /   108 tokens (    2.75 ms per token,   363.63 tokens per second)\n",
            "llama_print_timings:        eval time =    1858.07 ms /    15 runs   (  123.87 ms per token,     8.07 tokens per second)\n",
            "llama_print_timings:       total time =    2510.38 ms /   123 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: We can replace 16 out of 626 Members of the European Parliament, but we can never again as voters decide to bring in a new law.\n",
            "hyp1: As voters we cannot decide on a new law, but we can replace 16 out of 626 Members of the European Parliament.\n",
            "hyp2: As voters decide on a new law, we can replace 16 out of 626 Members of the European Parliament.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.44 ms /    16 runs   (    0.46 ms per token,  2151.41 tokens per second)\n",
            "llama_print_timings: prompt eval time =     292.96 ms /    88 tokens (    3.33 ms per token,   300.38 tokens per second)\n",
            "llama_print_timings:        eval time =    1681.97 ms /    15 runs   (  112.13 ms per token,     8.92 tokens per second)\n",
            "llama_print_timings:       total time =    2245.92 ms /   103 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: The Council is making a mistake by leaving it entirely up to the United States to formulate proposals for a solution to the Middle East problem.\n",
            "hyp1: The United States should not be the only ones to come up with a solution to the Middle East problem\n",
            "hyp2: The United States needs to come up with a solution to the Middle East problem, not the Council.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        }
      ],
      "source": [
        "#replies_list = ['YES','NO']\n",
        "counter = 0\n",
        "\n",
        "# List to store the results.\n",
        "prediction_en = []\n",
        "\n",
        "for i in range(0,len(test_ds['test_detection_english'])):\n",
        "  src = test_ds['test_detection_english'][i]['source']\n",
        "  hyp1 = test_ds['test_detection_english'][i]['hyp1']\n",
        "  hyp2 = test_ds['test_detection_english'][i]['hyp2']\n",
        "  id = test_ds['test_detection_english'][i]['id']\n",
        "\n",
        "  current_sample = prompt_context+\"src: \"+src+\"\\nhyp1: \"+hyp1 + '\\nhyp2: '+hyp2+'\\nlabel: [/INST] \\n '\n",
        "  prompt = few_shot_samples_en+current_sample\n",
        "\n",
        "  #print(prompt)\n",
        "  #print(current_sample)\n",
        "\n",
        "  response = lcpp_llm(\n",
        "        prompt=prompt,\n",
        "        temperature= 0.2,\n",
        "        logprobs=1,\n",
        "        #max_tokens =1\n",
        "      )\n",
        "\n",
        "  #print(response)\n",
        "\n",
        "  answer = str(response[\"choices\"][0][\"text\"]).strip()\n",
        "  #print(answer)\n",
        "  answer = answer[:4]\n",
        "  #answer = answer.split()[0]\n",
        "  # Sometime output contains a '.' remove it!\n",
        "  #answer = answer.replace('.','')\n",
        "\n",
        "  # If the predicted word is not in emotion list just replace with neutral.\n",
        "  #if answer not in replies_list:\n",
        "\n",
        "  #current_sample += answer + \" \\n \"\n",
        "\n",
        "  print(\"GENERATED: \"+ current_sample+'\\n'+answer)\n",
        "\n",
        "  current_element = {\n",
        "        \"id\": id,\n",
        "        \"label\": answer,\n",
        "        \"explanation\": \"\"\n",
        "    }\n",
        "  prediction_en.append(current_element)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(prediction_en)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pqj9hh4JtGgm",
        "outputId": "11468b32-9582-4878-90b9-8298d42f4109"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'id': 0, 'label': 'hyp1', 'explanation': ''}, {'id': 1, 'label': 'hyp1', 'explanation': ''}, {'id': 2, 'label': 'hyp1', 'explanation': ''}, {'id': 3, 'label': 'hyp1', 'explanation': ''}, {'id': 4, 'label': 'hyp2', 'explanation': ''}, {'id': 5, 'label': 'hyp1', 'explanation': ''}, {'id': 6, 'label': 'hyp1', 'explanation': ''}, {'id': 7, 'label': 'hyp1', 'explanation': ''}, {'id': 8, 'label': 'hyp1', 'explanation': ''}, {'id': 9, 'label': 'hyp2', 'explanation': ''}, {'id': 10, 'label': 'hyp1', 'explanation': ''}, {'id': 11, 'label': 'hyp1', 'explanation': ''}, {'id': 12, 'label': 'hyp1', 'explanation': ''}, {'id': 13, 'label': 'hyp1', 'explanation': ''}, {'id': 14, 'label': 'hyp2', 'explanation': ''}, {'id': 15, 'label': 'hyp1', 'explanation': ''}, {'id': 16, 'label': 'hyp1', 'explanation': ''}, {'id': 17, 'label': 'hyp1', 'explanation': ''}, {'id': 18, 'label': 'hyp1', 'explanation': ''}, {'id': 19, 'label': 'hyp2', 'explanation': ''}, {'id': 20, 'label': 'hyp1', 'explanation': ''}, {'id': 21, 'label': 'hyp1', 'explanation': ''}, {'id': 22, 'label': 'hyp1', 'explanation': ''}, {'id': 23, 'label': 'hyp1', 'explanation': ''}, {'id': 24, 'label': 'hyp1', 'explanation': ''}, {'id': 25, 'label': 'hyp1', 'explanation': ''}, {'id': 26, 'label': 'hyp1', 'explanation': ''}, {'id': 27, 'label': 'hyp2', 'explanation': ''}, {'id': 28, 'label': 'hyp2', 'explanation': ''}, {'id': 29, 'label': 'hyp2', 'explanation': ''}, {'id': 30, 'label': 'hyp2', 'explanation': ''}, {'id': 31, 'label': 'hyp1', 'explanation': ''}, {'id': 32, 'label': 'hyp1', 'explanation': ''}, {'id': 33, 'label': 'hyp2', 'explanation': ''}, {'id': 34, 'label': 'hyp1', 'explanation': ''}, {'id': 35, 'label': 'hyp2', 'explanation': ''}, {'id': 36, 'label': 'hyp2', 'explanation': ''}, {'id': 37, 'label': 'hyp2', 'explanation': ''}, {'id': 38, 'label': 'hyp1', 'explanation': ''}, {'id': 39, 'label': 'hyp1', 'explanation': ''}, {'id': 40, 'label': 'hyp2', 'explanation': ''}, {'id': 41, 'label': 'hyp2', 'explanation': ''}, {'id': 42, 'label': 'hyp2', 'explanation': ''}, {'id': 43, 'label': 'hyp2', 'explanation': ''}, {'id': 44, 'label': 'hyp1', 'explanation': ''}, {'id': 45, 'label': 'hyp2', 'explanation': ''}, {'id': 46, 'label': 'hyp1', 'explanation': ''}, {'id': 47, 'label': 'hyp1', 'explanation': ''}, {'id': 48, 'label': 'hyp1', 'explanation': ''}, {'id': 49, 'label': 'hyp2', 'explanation': ''}, {'id': 50, 'label': 'hyp1', 'explanation': ''}, {'id': 51, 'label': 'hyp2', 'explanation': ''}, {'id': 52, 'label': 'hyp1', 'explanation': ''}, {'id': 53, 'label': 'hyp2', 'explanation': ''}, {'id': 54, 'label': 'hyp2', 'explanation': ''}, {'id': 55, 'label': 'hyp1', 'explanation': ''}, {'id': 56, 'label': 'hyp1', 'explanation': ''}, {'id': 57, 'label': 'hyp1', 'explanation': ''}, {'id': 58, 'label': 'hyp1', 'explanation': ''}, {'id': 59, 'label': 'hyp1', 'explanation': ''}, {'id': 60, 'label': 'hyp1', 'explanation': ''}, {'id': 61, 'label': 'hyp1', 'explanation': ''}, {'id': 62, 'label': 'hyp2', 'explanation': ''}, {'id': 63, 'label': 'hyp1', 'explanation': ''}, {'id': 64, 'label': 'hyp1', 'explanation': ''}, {'id': 65, 'label': 'hyp1', 'explanation': ''}, {'id': 66, 'label': 'hyp1', 'explanation': ''}, {'id': 67, 'label': 'hyp2', 'explanation': ''}, {'id': 68, 'label': 'hyp1', 'explanation': ''}, {'id': 69, 'label': 'hyp1', 'explanation': ''}, {'id': 70, 'label': 'hyp1', 'explanation': ''}, {'id': 71, 'label': 'hyp2', 'explanation': ''}, {'id': 72, 'label': 'hyp1', 'explanation': ''}, {'id': 73, 'label': 'hyp1', 'explanation': ''}, {'id': 74, 'label': 'hyp2', 'explanation': ''}, {'id': 75, 'label': 'hyp2', 'explanation': ''}, {'id': 76, 'label': 'hyp1', 'explanation': ''}, {'id': 77, 'label': 'hyp2', 'explanation': ''}, {'id': 78, 'label': 'hyp1', 'explanation': ''}, {'id': 79, 'label': 'hyp1', 'explanation': ''}, {'id': 80, 'label': 'hyp1', 'explanation': ''}, {'id': 81, 'label': 'hyp1', 'explanation': ''}, {'id': 82, 'label': 'hyp2', 'explanation': ''}, {'id': 83, 'label': 'hyp1', 'explanation': ''}, {'id': 84, 'label': 'hyp1', 'explanation': ''}, {'id': 85, 'label': 'hyp1', 'explanation': ''}, {'id': 86, 'label': 'hyp1', 'explanation': ''}, {'id': 87, 'label': 'hyp1', 'explanation': ''}, {'id': 88, 'label': 'hyp2', 'explanation': ''}, {'id': 89, 'label': 'hyp1', 'explanation': ''}, {'id': 90, 'label': 'hyp1', 'explanation': ''}, {'id': 91, 'label': 'hyp1', 'explanation': ''}, {'id': 92, 'label': 'hyp1', 'explanation': ''}, {'id': 93, 'label': 'hyp1', 'explanation': ''}, {'id': 94, 'label': 'hyp1', 'explanation': ''}, {'id': 95, 'label': 'hyp1', 'explanation': ''}, {'id': 96, 'label': 'hyp1', 'explanation': ''}, {'id': 97, 'label': 'hyp1', 'explanation': ''}, {'id': 98, 'label': 'hyp1', 'explanation': ''}, {'id': 99, 'label': 'hyp1', 'explanation': ''}, {'id': 100, 'label': 'hyp2', 'explanation': ''}, {'id': 101, 'label': 'hyp1', 'explanation': ''}, {'id': 102, 'label': 'hyp2', 'explanation': ''}, {'id': 103, 'label': 'hyp2', 'explanation': ''}, {'id': 104, 'label': 'hyp2', 'explanation': ''}, {'id': 105, 'label': 'hyp1', 'explanation': ''}, {'id': 106, 'label': 'hyp1', 'explanation': ''}, {'id': 107, 'label': 'hyp1', 'explanation': ''}, {'id': 108, 'label': 'hyp1', 'explanation': ''}, {'id': 109, 'label': 'hyp1', 'explanation': ''}, {'id': 110, 'label': 'hyp1', 'explanation': ''}, {'id': 111, 'label': 'hyp1', 'explanation': ''}, {'id': 112, 'label': 'hyp1', 'explanation': ''}, {'id': 113, 'label': 'hyp1', 'explanation': ''}, {'id': 114, 'label': 'hyp1', 'explanation': ''}, {'id': 115, 'label': 'hyp1', 'explanation': ''}, {'id': 116, 'label': 'hyp1', 'explanation': ''}, {'id': 117, 'label': 'hyp2', 'explanation': ''}, {'id': 118, 'label': 'hyp2', 'explanation': ''}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List to store the results.\n",
        "prediction_sv = []\n",
        "\n",
        "for i in range(0,len(test_ds['test_detection_swedish'])):\n",
        "  src = GoogleTranslator(source='sv', target='en').translate(test_ds['test_detection_swedish'][i]['source'])\n",
        "  hyp1 = GoogleTranslator(source='sv', target='en').translate(test_ds['test_detection_swedish'][i]['hyp1'])\n",
        "  hyp2 = GoogleTranslator(source='sv', target='en').translate(test_ds['test_detection_swedish'][i]['hyp2'])\n",
        "  id = test_ds['test_detection_swedish'][i]['id']\n",
        "\n",
        "  current_sample = prompt_context+\"src: \"+src+\"\\nhyp1: \"+hyp1 + '\\nhyp2: '+hyp2+'\\nlabel: [/INST] \\n '\n",
        "  prompt = few_shot_samples_sv+current_sample\n",
        "\n",
        "  #print(prompt)\n",
        "  #print(current_sample)\n",
        "\n",
        "  response = lcpp_llm(\n",
        "        prompt=prompt,\n",
        "        temperature= 0.2,\n",
        "        logprobs=1,\n",
        "        #max_tokens =1\n",
        "      )\n",
        "\n",
        "  #print(response)\n",
        "\n",
        "  answer = str(response[\"choices\"][0][\"text\"]).strip()\n",
        "  #print(answer)\n",
        "  answer = answer[:4]\n",
        "  #answer = answer.split()[0]\n",
        "  # Sometime output contains a '.' remove it!\n",
        "  #answer = answer.replace('.','')\n",
        "\n",
        "  # If the predicted word is not in emotion list just replace with neutral.\n",
        "  #if answer not in replies_list:\n",
        "\n",
        "  #current_sample += answer + \" \\n \"\n",
        "\n",
        "  print(\"GENERATED: \"+ current_sample+'\\n'+answer)\n",
        "\n",
        "  current_element = {\n",
        "        \"id\": id,\n",
        "        \"label\": answer,\n",
        "        \"explanation\": \"\"\n",
        "    }\n",
        "  prediction_sv.append(current_element)\n"
      ],
      "metadata": {
        "id": "4SfpJbGEa0R_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "425db1b3-8292-4078-d2a6-3b9b31d3332a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.08 ms /    16 runs   (    0.50 ms per token,  1980.69 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4018.19 ms /  2593 tokens (    1.55 ms per token,   645.32 tokens per second)\n",
            "llama_print_timings:        eval time =    1519.75 ms /    15 runs   (  101.32 ms per token,     9.87 tokens per second)\n",
            "llama_print_timings:       total time =   13216.20 ms /  2608 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Women will face higher car insurance premiums.\n",
            "hyp1: This means women can expect to pay higher prices for their vehicle supplement insurance.\n",
            "hyp2: Women will receive higher car insurance premiums.\n",
            "label: [/INST] \n",
            " \n",
            "hyp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.67 ms /    16 runs   (    0.48 ms per token,  2086.59 tokens per second)\n",
            "llama_print_timings: prompt eval time =     323.04 ms /   104 tokens (    3.11 ms per token,   321.94 tokens per second)\n",
            "llama_print_timings:        eval time =    1551.31 ms /    15 runs   (  103.42 ms per token,     9.67 tokens per second)\n",
            "llama_print_timings:       total time =    2173.91 ms /   119 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Operating income was $1.45 billion, up from last year's earnings of $1.38 billion.\n",
            "hyp1: Revenue from the operation was $1.45 billion, down from the previous year's result of $1.38 billion.\n",
            "hyp2: Revenue from the operation was $1.45 billion, up from the previous year's result of $1.38 billion.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       7.93 ms /    16 runs   (    0.50 ms per token,  2017.15 tokens per second)\n",
            "llama_print_timings: prompt eval time =     280.02 ms /    60 tokens (    4.67 ms per token,   214.27 tokens per second)\n",
            "llama_print_timings:        eval time =    1587.35 ms /    15 runs   (  105.82 ms per token,     9.45 tokens per second)\n",
            "llama_print_timings:       total time =    2056.53 ms /    75 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST] Which one of hyp1 and hyp2 is not supported by src?\n",
            "\n",
            "src: Mandela back in hospital in 'serious but stable' condition.\n",
            "hyp1: Mandela does not return to hospital in serious, but stable, health.\n",
            "hyp2: Mandela returns to hospital in serious but stable condition.\n",
            "label: [/INST] \n",
            " \n",
            "hyp1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1301.19 ms\n",
            "llama_print_timings:      sample time =       8.46 ms /    16 runs   (    0.53 ms per token,  1891.48 tokens per second)\n",
            "llama_print_timings: prompt eval time =     327.76 ms /   114 tokens (    2.88 ms per token,   347.81 tokens per second)\n",
            "llama_print_timings:        eval time =    1552.58 ms /    15 runs   (  103.51 ms per token,     9.66 tokens per second)\n",
            "llama_print_timings:       total time =    2235.44 ms /   129 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to create the CSV files containing the predictions."
      ],
      "metadata": {
        "id": "xIY-lNkCfb9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "def create_csv_prediction_file (data, filename):\n",
        "\n",
        "  # Data to be written to the CSV file\n",
        "  #data = [\n",
        "  #    {\"id\": 1, \"label\": \"A\", \"explanation\": \"Explanation for A\"},\n",
        "  #    {\"id\": 2, \"label\": \"B\", \"explanation\": \"Explanation for B\"},\n",
        "  #    {\"id\": 3, \"label\": \"C\", \"explanation\": \"Explanation for C\"},\n",
        "  #]\n",
        "\n",
        "  # Name of the CSV file\n",
        "  #csv_file = \"data.csv\"\n",
        "  csv_file = filename\n",
        "\n",
        "  # Field names\n",
        "  fields = [\"id\", \"label\", \"explanation\"]\n",
        "\n",
        "  # Writing to CSV file\n",
        "  with open(csv_file, mode='w', newline='') as file:\n",
        "      writer = csv.DictWriter(file, fieldnames=fields)\n",
        "\n",
        "      # Write the header\n",
        "      writer.writeheader()\n",
        "\n",
        "      # Write the data\n",
        "      for row in data:\n",
        "          writer.writerow(row)\n",
        "\n",
        "  print(\"CSV file created successfully.\")\n"
      ],
      "metadata": {
        "id": "np7tmKILfUtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_csv_prediction_file (prediction_en, 'eloquent2024_mc_mistral_en_prediction.csv')\n",
        "create_csv_prediction_file (prediction_sv, 'eloquent2024_mc_mistral_sv_prediction.csv')"
      ],
      "metadata": {
        "id": "pMSsoCgcfzG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('eloquent2024_mc_mistral_en_prediction.csv')\n",
        "files.download('eloquent2024_mc_mistral_sv_prediction.csv')"
      ],
      "metadata": {
        "id": "dia3xJw7TwSL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2647583556a2419d83b97a1d4e57555b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c466da3506e7425ca7f5874d08460a99",
              "IPY_MODEL_d3b2d3fd3ca24c3996a07f678229db32",
              "IPY_MODEL_dee9385e8bcd47f7af8e4b84b04e5a43"
            ],
            "layout": "IPY_MODEL_05bc6238f46b4cab82a21be889550a84"
          }
        },
        "c466da3506e7425ca7f5874d08460a99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1417532ecdd24e4f99754c4dad4af4d8",
            "placeholder": "​",
            "style": "IPY_MODEL_bdcc141c7bd544ee8420423779449ea3",
            "value": "mistral-7b-instruct-v0.2.Q6_K.gguf: 100%"
          }
        },
        "d3b2d3fd3ca24c3996a07f678229db32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fba8a0aed5654779ac9d8646ff25b1a7",
            "max": 5942065440,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_94dd097903c3479f94a51b6e938f7a3d",
            "value": 5942065440
          }
        },
        "dee9385e8bcd47f7af8e4b84b04e5a43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b88d2665c5694762a7d8f096ea0785d9",
            "placeholder": "​",
            "style": "IPY_MODEL_090c755b65054c7ca0f8d9bd059c21b1",
            "value": " 5.94G/5.94G [00:58&lt;00:00, 139MB/s]"
          }
        },
        "05bc6238f46b4cab82a21be889550a84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1417532ecdd24e4f99754c4dad4af4d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdcc141c7bd544ee8420423779449ea3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fba8a0aed5654779ac9d8646ff25b1a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94dd097903c3479f94a51b6e938f7a3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b88d2665c5694762a7d8f096ea0785d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "090c755b65054c7ca0f8d9bd059c21b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}